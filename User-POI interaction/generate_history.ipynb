{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a64a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab569128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserPOIInteractionGenerator:\n",
    "    def __init__(self, poi_tree_file: str, users_file: str):\n",
    "        \"\"\"\n",
    "        Initialize interaction generator\n",
    "        \n",
    "        Args:\n",
    "            poi_tree_file: Path to POI tree JSON file\n",
    "            users_file: Path to user preferences CSV\n",
    "        \"\"\"\n",
    "        # Load POI tree\n",
    "        with open(poi_tree_file, 'r', encoding='utf-8') as f:\n",
    "            self.poi_tree = json.load(f)\n",
    "        \n",
    "        # Load users\n",
    "        self.users_df = pd.read_csv(users_file)\n",
    "        \n",
    "        # Define Singapore bounding box for random location generation\n",
    "        self.singapore_bounds = {\n",
    "            'lat_min': 1.22,\n",
    "            'lat_max': 1.47,\n",
    "            'lon_min': 103.60,\n",
    "            'lon_max': 104.05\n",
    "        }\n",
    "        \n",
    "        # Define common areas in Singapore where users might be located\n",
    "        # These serve as \"hotspots\" for weighted random location generation\n",
    "        self.location_hotspots = [\n",
    "            {'name': 'Orchard', 'lat': 1.3048, 'lon': 103.8318, 'weight': 1.5},\n",
    "            {'name': 'Marina Bay', 'lat': 1.2838, 'lon': 103.8591, 'weight': 1.3},\n",
    "            {'name': 'Bugis', 'lat': 1.3009, 'lon': 103.8558, 'weight': 1.2},\n",
    "            {'name': 'Jurong East', 'lat': 1.3329, 'lon': 103.7436, 'weight': 1.0},\n",
    "            {'name': 'Tampines', 'lat': 1.3496, 'lon': 103.9568, 'weight': 1.0},\n",
    "            {'name': 'Woodlands', 'lat': 1.4382, 'lon': 103.7891, 'weight': 0.8},\n",
    "            {'name': 'Bishan', 'lat': 1.3526, 'lon': 103.8352, 'weight': 1.1},\n",
    "            {'name': 'Ang Mo Kio', 'lat': 1.3691, 'lon': 103.8454, 'weight': 1.0},\n",
    "            {'name': 'Bedok', 'lat': 1.3236, 'lon': 103.9273, 'weight': 1.0},\n",
    "            {'name': 'Clementi', 'lat': 1.3152, 'lon': 103.7649, 'weight': 1.0},\n",
    "            {'name': 'Toa Payoh', 'lat': 1.3343, 'lon': 103.8563, 'weight': 1.0},\n",
    "            {'name': 'Sengkang', 'lat': 1.3868, 'lon': 103.8914, 'weight': 0.9},\n",
    "            {'name': 'Punggol', 'lat': 1.4043, 'lon': 103.9021, 'weight': 0.8},\n",
    "            {'name': 'Serangoon', 'lat': 1.3554, 'lon': 103.8679, 'weight': 1.0},\n",
    "            {'name': 'Yishun', 'lat': 1.4304, 'lon': 103.8354, 'weight': 0.9},\n",
    "            {'name': 'Hougang', 'lat': 1.3612, 'lon': 103.8863, 'weight': 0.9},\n",
    "            {'name': 'Pasir Ris', 'lat': 1.3721, 'lon': 103.9474, 'weight': 0.8},\n",
    "            {'name': 'Clarke Quay', 'lat': 1.2906, 'lon': 103.8465, 'weight': 1.2},\n",
    "            {'name': 'Sentosa', 'lat': 1.2494, 'lon': 103.8303, 'weight': 0.7},\n",
    "            {'name': 'Changi', 'lat': 1.3644, 'lon': 103.9915, 'weight': 0.6},\n",
    "        ]\n",
    "        \n",
    "        # Interest to POI characteristic mapping\n",
    "        self.interest_mapping = {\n",
    "            'food': ['dining', 'restaurant', 'food', 'hawker', 'cafe', 'eatery', 'cuisine'],\n",
    "            'shopping': ['shopping', 'retail', 'fashion', 'mall', 'store', 'boutique'],\n",
    "            'movies': ['cinema', 'movie', 'film', 'theatre', 'entertainment'],\n",
    "            'cafes': ['cafe', 'coffee', 'tea', 'bakery', 'dessert'],\n",
    "            'cycling': ['cycling', 'bike', 'sports', 'outdoor', 'park'],\n",
    "            'photography': ['scenic', 'park', 'nature', 'attraction', 'view'],\n",
    "            'museums': ['museum', 'gallery', 'art', 'culture', 'heritage', 'exhibition'],\n",
    "            'books': ['book', 'library', 'store', 'reading'],\n",
    "            'nightlife': ['bar', 'pub', 'club', 'nightlife', 'lounge'],\n",
    "            'bars': ['bar', 'pub', 'lounge', 'drinks', 'alcohol'],\n",
    "            'concerts': ['music', 'entertainment', 'venue', 'performance'],\n",
    "            'family activities': ['family', 'kid', 'playground', 'park', 'entertainment'],\n",
    "            'playgrounds': ['playground', 'park', 'family', 'children'],\n",
    "            'malls': ['mall', 'shopping', 'retail'],\n",
    "            'sports': ['sports', 'gym', 'fitness', 'athletic'],\n",
    "            'gyms': ['gym', 'fitness', 'sports', 'workout'],\n",
    "            'healthy eating': ['healthy', 'salad', 'organic', 'wellness', 'fresh'],\n",
    "            'gaming': ['gaming', 'arcade', 'entertainment', 'game'],\n",
    "            'arcades': ['arcade', 'game', 'entertainment'],\n",
    "            'budget food': ['hawker', 'food court', 'budget', 'cheap', 'affordable'],\n",
    "            'tech stores': ['tech', 'electronics', 'gadget', 'computer', 'mobile'],\n",
    "            'coworking': ['coworking', 'cafe', 'workspace', 'work'],\n",
    "            'local food': ['hawker', 'local', 'traditional', 'food court'],\n",
    "            'parks': ['park', 'nature', 'outdoor', 'garden'],\n",
    "            'community events': ['community', 'event', 'recreation', 'centre']\n",
    "        }\n",
    "        \n",
    "        # Transportation mode to max distance mapping (km)\n",
    "        self.transport_distance = {\n",
    "            'MRT': 15.0,\n",
    "            'bus': 10.0,\n",
    "            'car': 25.0,\n",
    "            'walking': 2.0,\n",
    "            'bicycle': 5.0,\n",
    "            'ride-hailing': 20.0\n",
    "        }\n",
    "        \n",
    "        # Price sensitivity to price range mapping\n",
    "        self.price_ranges = {\n",
    "            'low': (0, 20),\n",
    "            'medium': (10, 40),\n",
    "            'high': (20, 100)\n",
    "        }\n",
    "    \n",
    "    def _generate_random_location(self) -> Tuple[float, float, str]:\n",
    "        \"\"\"\n",
    "        Generate a random location within Singapore.\n",
    "        Uses weighted hotspots to make locations more realistic.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (latitude, longitude, nearest_area_name)\n",
    "        \"\"\"\n",
    "        # 70% chance to be near a hotspot, 30% chance fully random\n",
    "        if random.random() < 0.7:\n",
    "            # Select a hotspot based on weights\n",
    "            weights = [h['weight'] for h in self.location_hotspots]\n",
    "            total_weight = sum(weights)\n",
    "            weights = [w / total_weight for w in weights]\n",
    "            \n",
    "            selected_hotspot = random.choices(self.location_hotspots, weights=weights)[0]\n",
    "            \n",
    "            # Add random offset (within ~2km radius using normal distribution)\n",
    "            lat_offset = random.gauss(0, 0.01)  # ~1km standard deviation\n",
    "            lon_offset = random.gauss(0, 0.01)\n",
    "            \n",
    "            lat = selected_hotspot['lat'] + lat_offset\n",
    "            lon = selected_hotspot['lon'] + lon_offset\n",
    "            \n",
    "            # Clamp to Singapore bounds\n",
    "            lat = max(self.singapore_bounds['lat_min'], \n",
    "                     min(self.singapore_bounds['lat_max'], lat))\n",
    "            lon = max(self.singapore_bounds['lon_min'], \n",
    "                     min(self.singapore_bounds['lon_max'], lon))\n",
    "            \n",
    "            return (lat, lon, selected_hotspot['name'])\n",
    "        else:\n",
    "            # Fully random within Singapore bounds\n",
    "            lat = random.uniform(self.singapore_bounds['lat_min'], \n",
    "                                self.singapore_bounds['lat_max'])\n",
    "            lon = random.uniform(self.singapore_bounds['lon_min'], \n",
    "                                self.singapore_bounds['lon_max'])\n",
    "            \n",
    "            # Find nearest hotspot for area name\n",
    "            min_dist = float('inf')\n",
    "            nearest_area = 'Unknown'\n",
    "            for hotspot in self.location_hotspots:\n",
    "                dist = self._haversine_distance(lat, lon, hotspot['lat'], hotspot['lon'])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    nearest_area = hotspot['name']\n",
    "            \n",
    "            return (lat, lon, nearest_area)\n",
    "    \n",
    "    def _haversine_distance(self, lat1: float, lon1: float, \n",
    "                           lat2: float, lon2: float) -> float:\n",
    "        \"\"\"Calculate distance between coordinates in km\"\"\"\n",
    "        R = 6371\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "        return R * c\n",
    "    \n",
    "    def _get_max_travel_distance(self, transport_modes: str) -> float:\n",
    "        \"\"\"Get maximum travel distance based on transportation modes\"\"\"\n",
    "        modes = [m.strip() for m in transport_modes.split(';')]\n",
    "        # Return max distance among all modes\n",
    "        distances = [self.transport_distance.get(mode, 10.0) for mode in modes]\n",
    "        return max(distances)\n",
    "    \n",
    "    def _match_interest_score(self, user_interests: str, poi_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate how well a POI matches user interests\n",
    "        Returns score from 0 to 1\n",
    "        \"\"\"\n",
    "        interests = [i.strip().lower() for i in user_interests.split(';')]\n",
    "        poi_text_lower = poi_text.lower()\n",
    "        \n",
    "        total_matches = 0\n",
    "        total_keywords = 0\n",
    "        \n",
    "        for interest in interests:\n",
    "            if interest in self.interest_mapping:\n",
    "                keywords = self.interest_mapping[interest]\n",
    "                total_keywords += len(keywords)\n",
    "                matches = sum(1 for keyword in keywords if keyword in poi_text_lower)\n",
    "                total_matches += matches\n",
    "        \n",
    "        if total_keywords == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return min(total_matches / (total_keywords * 0.3), 1.0)  # Normalize\n",
    "    \n",
    "    def _match_price_sensitivity(self, user_price_sens: str, poi_price: str) -> bool:\n",
    "        \"\"\"Check if POI price matches user's price sensitivity\"\"\"\n",
    "        if pd.isna(poi_price) or poi_price == '':\n",
    "            return True  # Assume affordable if no price info\n",
    "        \n",
    "        try:\n",
    "            # Parse price range (e.g., \"25.85 - 30.99\")\n",
    "            if '-' in str(poi_price):\n",
    "                prices = str(poi_price).split('-')\n",
    "                avg_price = (float(prices[0].strip()) + float(prices[1].strip())) / 2\n",
    "            else:\n",
    "                avg_price = float(poi_price)\n",
    "            \n",
    "            price_range = self.price_ranges.get(user_price_sens.lower(), (0, 100))\n",
    "            return price_range[0] <= avg_price <= price_range[1]\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def _get_candidate_pois(self, user: pd.Series, \n",
    "                           current_location: Tuple[float, float]) -> List[Tuple[str, Dict, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Get candidate POIs for a user based on their preferences and current location\n",
    "        \n",
    "        Args:\n",
    "            user: User data series\n",
    "            current_location: Tuple of (latitude, longitude) for user's current position\n",
    "        \n",
    "        Returns:\n",
    "            List of (poi_id, poi_data, score, distance, interest_score) tuples\n",
    "        \"\"\"\n",
    "        user_lat, user_lon = current_location\n",
    "        max_distance = self._get_max_travel_distance(user['transportation_modes'])\n",
    "        user_interests = user['interests']\n",
    "        user_price_sens = user['price_sensitivity']\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        # Iterate through Level 0 POIs (individual POIs)\n",
    "        for poi_id, poi_data in self.poi_tree['level_0'].items():\n",
    "            # Get POI coordinates\n",
    "            poi_spatial = poi_data['spatial']\n",
    "            if isinstance(poi_spatial, str):\n",
    "                poi_spatial = eval(poi_spatial)  # Convert string tuple to tuple\n",
    "            \n",
    "            poi_lat, poi_lon = poi_spatial\n",
    "            \n",
    "            # Step 1: Spatial Filtering - Calculate distance from current location\n",
    "            distance = self._haversine_distance(user_lat, user_lon, poi_lat, poi_lon)\n",
    "            \n",
    "            # Filter by distance (based on user's transportation modes)\n",
    "            if distance > max_distance:\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Interest Matching - Calculate interest match score\n",
    "            poi_text = poi_data.get('textual', '')\n",
    "            interest_score = self._match_interest_score(user_interests, poi_text)\n",
    "            \n",
    "            if interest_score < 0.1:  # Skip if very low interest match\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Price Filtering - Check price sensitivity\n",
    "            poi_price = poi_data['data'].get('price', '')\n",
    "            if not self._match_price_sensitivity(user_price_sens, poi_price):\n",
    "                continue\n",
    "            \n",
    "            # Step 4: Scoring - Calculate overall score\n",
    "            # Get popularity\n",
    "            try:\n",
    "                popularity = float(poi_data['data'].get('popularity', 3))\n",
    "            except:\n",
    "                popularity = 3.0\n",
    "            \n",
    "            # Scoring formula: interest_match * 3 + popularity * 0.5 - distance * 0.2\n",
    "            # This prioritizes interest match, then popularity, with distance penalty\n",
    "            score = (interest_score * 3.0) + (popularity * 0.5) - (distance * 0.2)\n",
    "            \n",
    "            candidates.append((poi_id, poi_data, score, distance, interest_score))\n",
    "        \n",
    "        # Sort by score (highest first)\n",
    "        candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def generate_interactions(self, \n",
    "                            min_interactions_per_user: int = 5,\n",
    "                            max_interactions_per_user: int = 20,\n",
    "                            days_back: int = 90) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate synthetic user-POI interactions with randomized current locations\n",
    "        \n",
    "        Args:\n",
    "            min_interactions_per_user: Minimum number of interactions per user\n",
    "            max_interactions_per_user: Maximum number of interactions per user\n",
    "            days_back: Generate interactions for past N days\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with interaction data including current_location info\n",
    "        \"\"\"\n",
    "        interactions = []\n",
    "        interaction_counter = 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Generating User-POI Interactions\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nPipeline: Random Location -> Spatial Filtering -> Interest Matching -> Price Filtering -> Scoring\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for idx, user in self.users_df.iterrows():\n",
    "            user_id = user['uuid'] if 'uuid' in user else user['uudi']  # Handle both spellings\n",
    "            user_name = user['name']\n",
    "            \n",
    "            print(f\"\\nProcessing user {idx+1}/{len(self.users_df)}: {user_name}\")\n",
    "            \n",
    "            # Generate multiple interaction sessions for this user\n",
    "            num_sessions = random.randint(\n",
    "                min_interactions_per_user,\n",
    "                max_interactions_per_user\n",
    "            )\n",
    "            \n",
    "            session_interactions = 0\n",
    "            attempts = 0\n",
    "            max_attempts = num_sessions * 3  # Prevent infinite loops\n",
    "            \n",
    "            while session_interactions < num_sessions and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                # Generate a random current location for this interaction session\n",
    "                current_lat, current_lon, area_name = self._generate_random_location()\n",
    "                \n",
    "                # Get candidate POIs based on current location\n",
    "                candidates = self._get_candidate_pois(user, (current_lat, current_lon))\n",
    "                \n",
    "                if not candidates:\n",
    "                    continue  # Try a different location\n",
    "                \n",
    "                # Select a POI using weighted random (higher score = higher probability)\n",
    "                scores = [c[2] for c in candidates]\n",
    "                # Shift scores to be positive and apply exponential weighting\n",
    "                min_score = min(scores)\n",
    "                weights = [math.exp(score - min_score) for score in scores]\n",
    "                weight_sum = sum(weights)\n",
    "                weights = [w / weight_sum for w in weights]\n",
    "                \n",
    "                selected_idx = random.choices(range(len(candidates)), weights=weights)[0]\n",
    "                poi_id, poi_data, score, distance, interest_score = candidates[selected_idx]\n",
    "                \n",
    "                # Generate visit timestamp\n",
    "                days_ago = random.randint(0, days_back)\n",
    "                hours = random.randint(8, 22)\n",
    "                minutes = random.randint(0, 59)\n",
    "                visit_time = datetime.now() - timedelta(days=days_ago, hours=hours, minutes=minutes)\n",
    "                \n",
    "                # Visit interaction\n",
    "                interactions.append({\n",
    "                    'interaction_id': f'int_{interaction_counter:06d}',\n",
    "                    'user_id': user_id,\n",
    "                    'user_name': user_name,\n",
    "                    'poi_id': poi_id,\n",
    "                    'poi_name': poi_data['name'],\n",
    "                    'interaction_type': 'visit',\n",
    "                    'value': 1,\n",
    "                    'timestamp': visit_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'current_lat': round(current_lat, 6),\n",
    "                    'current_lon': round(current_lon, 6),\n",
    "                    'current_area': area_name,\n",
    "                    'distance_km': round(distance, 2),\n",
    "                    'interest_match_score': round(interest_score, 2)\n",
    "                })\n",
    "                interaction_counter += 1\n",
    "                session_interactions += 1\n",
    "                \n",
    "                # Generate rating (80% chance)\n",
    "                if random.random() < 0.8:\n",
    "                    base_rating = 3.0\n",
    "                    interest_bonus = interest_score * 2.0\n",
    "                    distance_penalty = min(distance / 10.0, 1.0)\n",
    "                    \n",
    "                    rating = base_rating + interest_bonus - distance_penalty\n",
    "                    rating = max(1, min(5, int(round(rating))))\n",
    "                    \n",
    "                    if random.random() < 0.2:\n",
    "                        rating = max(1, rating - 1) if random.random() < 0.5 else min(5, rating + 1)\n",
    "                    \n",
    "                    rating_time = visit_time + timedelta(minutes=random.randint(5, 120))\n",
    "                    \n",
    "                    interactions.append({\n",
    "                        'interaction_id': f'int_{interaction_counter:06d}',\n",
    "                        'user_id': user_id,\n",
    "                        'user_name': user_name,\n",
    "                        'poi_id': poi_id,\n",
    "                        'poi_name': poi_data['name'],\n",
    "                        'interaction_type': 'rating',\n",
    "                        'value': rating,\n",
    "                        'timestamp': rating_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'current_lat': round(current_lat, 6),\n",
    "                        'current_lon': round(current_lon, 6),\n",
    "                        'current_area': area_name,\n",
    "                        'distance_km': round(distance, 2),\n",
    "                        'interest_match_score': round(interest_score, 2)\n",
    "                    })\n",
    "                    interaction_counter += 1\n",
    "                \n",
    "                # Generate search (30% chance, happens before visit)\n",
    "                if random.random() < 0.3:\n",
    "                    search_time = visit_time - timedelta(hours=random.randint(1, 48))\n",
    "                    \n",
    "                    interactions.append({\n",
    "                        'interaction_id': f'int_{interaction_counter:06d}',\n",
    "                        'user_id': user_id,\n",
    "                        'user_name': user_name,\n",
    "                        'poi_id': poi_id,\n",
    "                        'poi_name': poi_data['name'],\n",
    "                        'interaction_type': 'search',\n",
    "                        'value': 1,\n",
    "                        'timestamp': search_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'current_lat': round(current_lat, 6),\n",
    "                        'current_lon': round(current_lon, 6),\n",
    "                        'current_area': area_name,\n",
    "                        'distance_km': round(distance, 2),\n",
    "                        'interest_match_score': round(interest_score, 2)\n",
    "                    })\n",
    "                    interaction_counter += 1\n",
    "            \n",
    "            print(f\"  Generated {session_interactions} interaction sessions from {attempts} attempts\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Generated {len(interactions)} total interactions\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Convert to DataFrame and sort by timestamp\n",
    "        interactions_df = pd.DataFrame(interactions)\n",
    "        interactions_df = interactions_df.sort_values('timestamp')\n",
    "        \n",
    "        return interactions_df\n",
    "    \n",
    "    def generate_summary_stats(self, interactions_df: pd.DataFrame):\n",
    "        \"\"\"Print summary statistics of generated interactions\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"INTERACTION SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nTotal interactions: {len(interactions_df)}\")\n",
    "        print(f\"Total users: {interactions_df['user_id'].nunique()}\")\n",
    "        print(f\"Total POIs: {interactions_df['poi_id'].nunique()}\")\n",
    "        \n",
    "        print(\"\\nInteractions per user:\")\n",
    "        user_counts = interactions_df.groupby('user_id').size()\n",
    "        print(f\"  Mean: {user_counts.mean():.1f}\")\n",
    "        print(f\"  Median: {user_counts.median():.1f}\")\n",
    "        print(f\"  Min: {user_counts.min()}\")\n",
    "        print(f\"  Max: {user_counts.max()}\")\n",
    "        \n",
    "        print(\"\\nInteraction types:\")\n",
    "        type_counts = interactions_df['interaction_type'].value_counts()\n",
    "        for itype, count in type_counts.items():\n",
    "            print(f\"  {itype}: {count} ({count/len(interactions_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nRating distribution:\")\n",
    "        ratings = interactions_df[interactions_df['interaction_type'] == 'rating']['value']\n",
    "        if len(ratings) > 0:\n",
    "            print(f\"  Mean rating: {ratings.mean():.2f}\")\n",
    "            print(f\"  Rating counts:\")\n",
    "            for rating in sorted(ratings.unique()):\n",
    "                count = (ratings == rating).sum()\n",
    "                print(f\"    {int(rating)} stars: {count} ({count/len(ratings)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nDistance statistics (from current location):\")\n",
    "        print(f\"  Mean distance: {interactions_df['distance_km'].mean():.2f} km\")\n",
    "        print(f\"  Median distance: {interactions_df['distance_km'].median():.2f} km\")\n",
    "        print(f\"  Max distance: {interactions_df['distance_km'].max():.2f} km\")\n",
    "        \n",
    "        print(\"\\nCurrent location area distribution:\")\n",
    "        area_counts = interactions_df.groupby('current_area').size().sort_values(ascending=False).head(10)\n",
    "        for area, count in area_counts.items():\n",
    "            print(f\"  {area}: {count} interactions ({count/len(interactions_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nInterest match statistics:\")\n",
    "        print(f\"  Mean match score: {interactions_df['interest_match_score'].mean():.2f}\")\n",
    "        print(f\"  Median match score: {interactions_df['interest_match_score'].median():.2f}\")\n",
    "        \n",
    "        print(\"\\nTop 10 most visited POIs:\")\n",
    "        top_pois = interactions_df[interactions_df['interaction_type'] == 'visit'].groupby('poi_name').size().sort_values(ascending=False).head(10)\n",
    "        for poi, count in top_pois.items():\n",
    "            print(f\"  {poi}: {count} visits\")\n",
    "        \n",
    "        print(\"\\nMost active users:\")\n",
    "        top_users = interactions_df.groupby('user_name').size().sort_values(ascending=False).head(5)\n",
    "        for user, count in top_users.items():\n",
    "            print(f\"  {user}: {count} interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f011a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generating User-POI Interactions\n",
      "============================================================\n",
      "\n",
      "Pipeline: Random Location -> Spatial Filtering -> Interest Matching -> Price Filtering -> Scoring\n",
      "============================================================\n",
      "\n",
      "Processing user 1/21: Aiden\n",
      "  Generated 5 interaction sessions from 5 attempts\n",
      "\n",
      "Processing user 2/21: Chloe\n",
      "  Generated 14 interaction sessions from 14 attempts\n",
      "\n",
      "Processing user 3/21: Lucas\n",
      "  Generated 14 interaction sessions from 15 attempts\n",
      "\n",
      "Processing user 4/21: Ethan\n",
      "  Generated 8 interaction sessions from 8 attempts\n",
      "\n",
      "Processing user 5/21: Maya\n",
      "  Generated 14 interaction sessions from 14 attempts\n",
      "\n",
      "Processing user 6/21: Sophia\n",
      "  Generated 11 interaction sessions from 11 attempts\n",
      "\n",
      "Processing user 7/21: Maya\n",
      "  Generated 6 interaction sessions from 6 attempts\n",
      "\n",
      "Processing user 8/21: Kai\n",
      "  Generated 8 interaction sessions from 8 attempts\n",
      "\n",
      "Processing user 9/21: Isla\n",
      "  Generated 11 interaction sessions from 11 attempts\n",
      "\n",
      "Processing user 10/21: Noah\n",
      "  Generated 20 interaction sessions from 21 attempts\n",
      "\n",
      "Processing user 11/21: Zara\n",
      "  Generated 15 interaction sessions from 16 attempts\n",
      "\n",
      "Processing user 12/21: Liam\n",
      "  Generated 18 interaction sessions from 18 attempts\n",
      "\n",
      "Processing user 13/21: Kai\n",
      "  Generated 20 interaction sessions from 20 attempts\n",
      "\n",
      "Processing user 14/21: Brian\n",
      "  Generated 19 interaction sessions from 19 attempts\n",
      "\n",
      "Processing user 15/21: Sam\n",
      "  Generated 7 interaction sessions from 7 attempts\n",
      "\n",
      "Processing user 16/21: David\n",
      "  Generated 17 interaction sessions from 17 attempts\n",
      "\n",
      "Processing user 17/21: Noah\n",
      "  Generated 11 interaction sessions from 11 attempts\n",
      "\n",
      "Processing user 18/21: Freya\n",
      "  Generated 6 interaction sessions from 6 attempts\n",
      "\n",
      "Processing user 19/21: Elise\n",
      "  Generated 18 interaction sessions from 18 attempts\n",
      "\n",
      "Processing user 20/21: Jayden\n",
      "  Generated 13 interaction sessions from 13 attempts\n",
      "\n",
      "Processing user 21/21: Bobby\n",
      "  Generated 9 interaction sessions from 9 attempts\n",
      "\n",
      "============================================================\n",
      "Generated 567 total interactions\n",
      "============================================================\n",
      "\n",
      "Saved full interactions to: user_poi_interactions_full.csv\n",
      "Saved minimal interactions to: user_poi_interactions.csv\n",
      "\n",
      "============================================================\n",
      "INTERACTION SUMMARY STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total interactions: 567\n",
      "Total users: 21\n",
      "Total POIs: 235\n",
      "\n",
      "Interactions per user:\n",
      "  Mean: 27.0\n",
      "  Median: 28.0\n",
      "  Min: 10\n",
      "  Max: 45\n",
      "\n",
      "Interaction types:\n",
      "  visit: 264 (46.6%)\n",
      "  rating: 209 (36.9%)\n",
      "  search: 94 (16.6%)\n",
      "\n",
      "Rating distribution:\n",
      "  Mean rating: 3.09\n",
      "  Rating counts:\n",
      "    2 stars: 31 (14.8%)\n",
      "    3 stars: 130 (62.2%)\n",
      "    4 stars: 46 (22.0%)\n",
      "    5 stars: 2 (1.0%)\n",
      "\n",
      "Distance statistics (from current location):\n",
      "  Mean distance: 6.61 km\n",
      "  Median distance: 6.01 km\n",
      "  Max distance: 16.83 km\n",
      "\n",
      "Current location area distribution:\n",
      "  Jurong East: 63 interactions (11.1%)\n",
      "  Marina Bay: 40 interactions (7.1%)\n",
      "  Woodlands: 37 interactions (6.5%)\n",
      "  Orchard: 36 interactions (6.3%)\n",
      "  Clarke Quay: 36 interactions (6.3%)\n",
      "  Toa Payoh: 35 interactions (6.2%)\n",
      "  Yishun: 35 interactions (6.2%)\n",
      "  Bedok: 32 interactions (5.6%)\n",
      "  Clementi: 27 interactions (4.8%)\n",
      "  Punggol: 25 interactions (4.4%)\n",
      "\n",
      "Interest match statistics:\n",
      "  Mean match score: 0.38\n",
      "  Median match score: 0.37\n",
      "\n",
      "Top 10 most visited POIs:\n",
      "  Starbucks: 19 visits\n",
      "  UOB: 10 visits\n",
      "  7-Eleven: 8 visits\n",
      "  The Coffee Bean & Tea Leaf: 8 visits\n",
      "  Fitness factory: 3 visits\n",
      "  Brawns & Brains: 3 visits\n",
      "  McDonalds: 3 visits\n",
      "  Harvey Norman: 3 visits\n",
      "  Dominos: 3 visits\n",
      "  One Man Coffee: 3 visits\n",
      "\n",
      "Most active users:\n",
      "  Kai: 64 interactions\n",
      "  Noah: 64 interactions\n",
      "  Maya: 43 interactions\n",
      "  Liam: 41 interactions\n",
      "  David: 40 interactions\n",
      "\n",
      "============================================================\n",
      "SAMPLE INTERACTIONS (first 10)\n",
      "============================================================\n",
      "interaction_id                              user_id user_name                   poi_id         poi_name interaction_type  value           timestamp  current_lat  current_lon current_area  distance_km  interest_match_score\n",
      "    int_000343 bc6799a8-00d1-4ab5-80bf-eeffc36ba029       Kai poi_923_Surfers_Paradise Surfers Paradise           search      1 2025-10-16 07:52:43     1.384491   103.932040    Pasir Ris         2.37                  0.26\n",
      "    int_000341 bc6799a8-00d1-4ab5-80bf-eeffc36ba029       Kai poi_923_Surfers_Paradise Surfers Paradise            visit      1 2025-10-16 17:52:43     1.384491   103.932040    Pasir Ris         2.37                  0.26\n",
      "    int_000342 bc6799a8-00d1-4ab5-80bf-eeffc36ba029       Kai poi_923_Surfers_Paradise Surfers Paradise           rating      4 2025-10-16 19:02:43     1.384491   103.932040    Pasir Ris         2.37                  0.26\n",
      "    int_000204 84273d4e-1e2c-4baf-8f0d-7b4f2ef833d0      Noah          poi_4208_Barrys           Barrys           search      1 2025-10-16 23:31:42     1.279515   103.846743   Marina Bay         0.48                  0.77\n",
      "    int_000241 79dfa2c9-fcaf-4ac0-9cf6-acdc3165f1db      Zara   poi_3454_Super_Chicken    Super Chicken            visit      1 2025-10-17 02:22:42     1.289508   103.831986  Clarke Quay         2.88                  0.28\n",
      "    int_000242 79dfa2c9-fcaf-4ac0-9cf6-acdc3165f1db      Zara   poi_3454_Super_Chicken    Super Chicken           rating      3 2025-10-17 03:57:42     1.289508   103.831986  Clarke Quay         2.88                  0.28\n",
      "    int_000546 985b9ad3-79dc-4c6a-9159-27f4dda0e235    Jayden         poi_354_7-Eleven         7-Eleven           search      1 2025-10-17 13:22:45     1.435590   103.802011    Woodlands         7.74                  0.22\n",
      "    int_000544 985b9ad3-79dc-4c6a-9159-27f4dda0e235    Jayden         poi_354_7-Eleven         7-Eleven            visit      1 2025-10-17 17:22:45     1.435590   103.802011    Woodlands         7.74                  0.22\n",
      "    int_000545 985b9ad3-79dc-4c6a-9159-27f4dda0e235    Jayden         poi_354_7-Eleven         7-Eleven           rating      3 2025-10-17 18:31:45     1.435590   103.802011    Woodlands         7.74                  0.22\n",
      "    int_000356 bc6799a8-00d1-4ab5-80bf-eeffc36ba029       Kai   poi_1276_An_Aai_Affair    An Aai Affair           search      1 2025-10-17 21:28:43     1.306428   103.771002     Clementi         4.42                  0.51\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate interactions\n",
    "    generator = UserPOIInteractionGenerator(\n",
    "        poi_tree_file='poi_tree_with_uuids.json',\n",
    "        users_file='user_preferences.csv' \n",
    "    )\n",
    "    \n",
    "    interactions_df = generator.generate_interactions(\n",
    "        min_interactions_per_user=5,\n",
    "        max_interactions_per_user=20,\n",
    "        days_back=90\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    # Full version with metadata (includes current location)\n",
    "    interactions_df.to_csv('user_poi_interactions_full.csv', index=False)\n",
    "    print(\"\\nSaved full interactions to: user_poi_interactions_full.csv\")\n",
    "    \n",
    "    # Minimal version (just the essential columns for training)\n",
    "    interactions_minimal = interactions_df[[\n",
    "        'user_id', 'poi_id', 'interaction_type', 'value', 'timestamp',\n",
    "        'current_lat', 'current_lon', 'distance_km'\n",
    "    ]]\n",
    "    interactions_minimal.to_csv('user_poi_interactions.csv', index=False)\n",
    "    print(\"Saved minimal interactions to: user_poi_interactions.csv\")\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    generator.generate_summary_stats(interactions_df)\n",
    "    \n",
    "    # Show sample interactions\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE INTERACTIONS (first 10)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(interactions_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
