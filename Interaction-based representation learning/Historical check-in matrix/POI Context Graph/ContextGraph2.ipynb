{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f837bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28576db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_SEARCH_TIME_WINDOW = timedelta(minutes=30)\n",
    "CO_VISIT_TIME_WINDOW = timedelta(hours=3)\n",
    "CO_VISIT_MAX_DISTANCE = 10  # km\n",
    "PROXIMITY_THRESHOLD = 2  # km for geospatial edges\n",
    " \n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate great circle distance in km.\"\"\"\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    " \n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate bearing from point 1 to point 2 in degrees.\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    bearing = np.degrees(np.arctan2(x, y))\n",
    "    return (bearing + 360) % 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1cbcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total interactions: 567\n",
      "✓ Converting Key format to UUID\n",
      "Final POIs with coordinates: 235\n",
      "\n",
      "Graph initialized with 235 nodes\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "interactions_df = pd.read_csv('../../../Sources/Files/user_poi_interactions.csv')\n",
    "interactions_df['timestamp'] = pd.to_datetime(interactions_df['timestamp'])\n",
    " \n",
    "with open('../../../Sources/Files/poi_tree_with_uuids.json', 'r') as f:\n",
    "    poi_tree = json.load(f)\n",
    " \n",
    "print(f\"Total interactions: {len(interactions_df)}\")\n",
    " \n",
    "poi_key_to_uuid = {}\n",
    "uuid_to_poi_key = {}\n",
    "poi_name_to_uuid = {}\n",
    "poi_data_by_uuid = {}\n",
    " \n",
    "for level_key in poi_tree.keys():\n",
    "    if level_key.startswith('level_'):\n",
    "        for poi_key, poi_info in poi_tree[level_key].items():\n",
    "            uuid = poi_info.get('uuid')\n",
    "            if uuid and 'data' in poi_info:\n",
    "                poi_key_to_uuid[poi_key] = uuid\n",
    "                uuid_to_poi_key[uuid] = poi_key\n",
    "                if poi_info.get('name'):\n",
    "                    poi_name_to_uuid[poi_info['name']] = uuid\n",
    "                \n",
    "                poi_data_by_uuid[uuid] = {\n",
    "                    'lat': poi_info['data'].get('latitude'),\n",
    "                    'long': poi_info['data'].get('longitude'),\n",
    "                    'name': poi_info['data'].get('name'),\n",
    "                    'category': poi_info['data'].get('category'),\n",
    "                    'level': level_key,\n",
    "                    'key': poi_key\n",
    "                }\n",
    " \n",
    "interaction_poi_ids = set(interactions_df['poi_id'].unique())\n",
    "uuid_matches = len(interaction_poi_ids & set(uuid_to_poi_key.keys()))\n",
    "key_matches = len(interaction_poi_ids & set(poi_key_to_uuid.keys()))\n",
    " \n",
    "if uuid_matches >= key_matches:\n",
    "    print(\"✓ Using UUID format\")\n",
    "    id_mapping = {k: k for k in uuid_to_poi_key.keys()}\n",
    "else:\n",
    "    print(\"✓ Converting Key format to UUID\")\n",
    "    id_mapping = poi_key_to_uuid\n",
    "    interactions_df['poi_id'] = interactions_df['poi_id'].map(id_mapping)\n",
    "    interactions_df = interactions_df.dropna(subset=['poi_id'])\n",
    " \n",
    "# Ensure all POIs have coordinates\n",
    "interaction_poi_ids = set(interactions_df['poi_id'].unique())\n",
    "pois_with_coords = {}\n",
    "missing_coords = []\n",
    " \n",
    "for pid in interaction_poi_ids:\n",
    "    if pid in poi_data_by_uuid:\n",
    "        lat = poi_data_by_uuid[pid]['lat']\n",
    "        long = poi_data_by_uuid[pid]['long']\n",
    "        if lat is not None and long is not None:\n",
    "            pois_with_coords[pid] = poi_data_by_uuid[pid]\n",
    "        else:\n",
    "            missing_coords.append(pid)\n",
    " \n",
    "if missing_coords:\n",
    "    print(f\"⚠️  Warning: {len(missing_coords)} POIs missing coordinates. Removing from graph.\")\n",
    "    interactions_df = interactions_df[~interactions_df['poi_id'].isin(missing_coords)]\n",
    " \n",
    "if len(pois_with_coords) == 0:\n",
    "    raise ValueError(\"No POIs with valid coordinates!\")\n",
    " \n",
    "poi_coords = pois_with_coords\n",
    "print(f\"Final POIs with coordinates: {len(poi_coords)}\")\n",
    " \n",
    "# Initialize graph\n",
    "G = nx.MultiDiGraph()\n",
    "G.add_nodes_from(poi_coords.keys())\n",
    "print(f\"\\nGraph initialized with {G.number_of_nodes()} nodes\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a4a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cosearch_edges_fixed(interactions_df, G, time_window):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BUILDING CO-SEARCH EDGES (Fixed)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    searches = interactions_df[interactions_df['interaction_type'] == 'search'].copy()\n",
    "    print(f\"Searches: {len(searches)} ({searches['user_id'].nunique()} users)\")\n",
    "    \n",
    "    cosearch_edges = defaultdict(int)\n",
    "    \n",
    "    for user_id, user_searches in tqdm(searches.groupby('user_id'), desc=\"Co-searches\"):\n",
    "        user_searches = user_searches.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        if len(user_searches) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Sliding window: for each search, look at subsequent searches within window\n",
    "        for i in range(len(user_searches)):\n",
    "            poi_i = user_searches.iloc[i]['poi_id']\n",
    "            time_i = user_searches.iloc[i]['timestamp']\n",
    "            \n",
    "            # Look forward in time window\n",
    "            mask = (user_searches['timestamp'] > time_i) & \\\n",
    "\t\t\t\t(user_searches['timestamp'] <= time_i + time_window)\n",
    "            window_searches = user_searches[mask]\n",
    "            \n",
    "            for _, row_j in window_searches.iterrows():\n",
    "                poi_j = row_j['poi_id']\n",
    "                if poi_i != poi_j:\n",
    "                    # Sort to ensure undirected edge consistency\n",
    "                    edge_key = tuple(sorted([poi_i, poi_j]))\n",
    "                    cosearch_edges[edge_key] += 1\n",
    "    \n",
    "    print(f\"Unique co-search pairs: {len(cosearch_edges)}\")\n",
    "    \n",
    "    if len(cosearch_edges) == 0:\n",
    "        return G\n",
    "    \n",
    "    # Symmetric normalization (Jaccard-like)\n",
    "    # Calculate node degrees for normalization\n",
    "    node_counts = defaultdict(int)\n",
    "    for (poi_i, poi_j), count in cosearch_edges.items():\n",
    "        node_counts[poi_i] += count\n",
    "        node_counts[poi_j] += count\n",
    "    \n",
    "    # Add symmetric edges with normalized weights\n",
    "    for (poi_i, poi_j), count in cosearch_edges.items():\n",
    "        # Symmetric weight: count / (total searches of i + total searches of j - count)\n",
    "        union = node_counts[poi_i] + node_counts[poi_j] - count\n",
    "        weight = count / union if union > 0 else 0\n",
    "        \n",
    "        # Add edge features for GNN\n",
    "        # For co-search: [weight, normalized_count, type_encoding=0]\n",
    "        edge_attr = np.array([weight, count / max(cosearch_edges.values()), 0.0])\n",
    "        \n",
    "        # Add both directions explicitly with same weight\n",
    "        G.add_edge(poi_i, poi_j, edge_type='co-search', weight=weight, \n",
    "\t\t\t\traw_count=count, edge_attr=edge_attr)\n",
    "        G.add_edge(poi_j, poi_i, edge_type='co-search', weight=weight,\n",
    "\t\t\t\traw_count=count, edge_attr=edge_attr)\n",
    "    \n",
    "    print(f\"Added {len(cosearch_edges)} bidirectional co-search edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f388b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_covisit_edges_fixed(interactions_df, G, poi_coords, time_window, max_distance):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BUILDING CO-VISIT EDGES (Fixed)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    visits = interactions_df[interactions_df['interaction_type'] == 'visit'].copy()\n",
    "    print(f\"Visits: {len(visits)} ({visits['user_id'].nunique()} users)\")\n",
    "    \n",
    "    covisit_edges = defaultdict(lambda: {'count': 0, 'distances': [], 'time_gaps': []})\n",
    "    skipped = set()\n",
    "    \n",
    "    for user_id, user_visits in tqdm(visits.groupby('user_id'), desc=\"Co-visits\"):\n",
    "        user_visits = user_visits.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        if len(user_visits) < 2:\n",
    "            continue\n",
    "        \n",
    "        for i in range(len(user_visits) - 1):\n",
    "            poi_i = user_visits.iloc[i]['poi_id']\n",
    "            time_i = user_visits.iloc[i]['timestamp']\n",
    "            \n",
    "            if poi_i not in poi_coords:\n",
    "                skipped.add(poi_i)\n",
    "                continue\n",
    "            \n",
    "            coords_i = poi_coords[poi_i]\n",
    "            \n",
    "            for j in range(i + 1, len(user_visits)):\n",
    "                poi_j = user_visits.iloc[j]['poi_id']\n",
    "                time_j = user_visits.iloc[j]['timestamp']\n",
    "                \n",
    "                if time_j - time_i > time_window:\n",
    "                    break\n",
    "                \n",
    "                if poi_j not in poi_coords:\n",
    "                    skipped.add(poi_j)\n",
    "                    continue\n",
    "                \n",
    "                # Calculate distance\n",
    "                coords_j = poi_coords[poi_j]\n",
    "                distance = haversine(coords_i['lat'], coords_i['long'],\n",
    "\t\t\t\t\t\t\t\tcoords_j['lat'], coords_j['long'])\n",
    "                \n",
    "                if distance <= max_distance and poi_i != poi_j:\n",
    "                    edge_key = tuple(sorted([poi_i, poi_j]))\n",
    "                    covisit_edges[edge_key]['count'] += 1\n",
    "                    covisit_edges[edge_key]['distances'].append(distance)\n",
    "                    covisit_edges[edge_key]['time_gaps'].append((time_j - time_i).seconds / 3600)\n",
    "    \n",
    "    print(f\"Unique co-visit pairs: {len(covisit_edges)}\")\n",
    "    print(f\"Skipped POIs: {len(skipped)}\")\n",
    "    \n",
    "    if len(covisit_edges) == 0:\n",
    "        return G\n",
    "    \n",
    "    # Symmetric normalization\n",
    "    node_counts = defaultdict(int)\n",
    "    for (poi_i, poi_j), data in covisit_edges.items():\n",
    "        count = data['count']\n",
    "        node_counts[poi_i] += count\n",
    "        node_counts[poi_j] += count\n",
    "    \n",
    "    for (poi_i, poi_j), data in covisit_edges.items():\n",
    "        count = data['count']\n",
    "        union = node_counts[poi_i] + node_counts[poi_j] - count\n",
    "        weight = count / union if union > 0 else 0\n",
    "        \n",
    "        # Rich edge features\n",
    "        avg_dist = np.mean(data['distances'])\n",
    "        avg_time = np.mean(data['time_gaps'])\n",
    "        # Features: [weight, normalized_count, avg_distance/10, avg_time/3, type_encoding=1]\n",
    "        edge_attr = np.array([\n",
    "            weight,\n",
    "            count / max(v['count'] for v in covisit_edges.values()),\n",
    "            avg_dist / 10.0,  # Normalize to ~0-1\n",
    "            avg_time / 3.0,   # Normalize hours\n",
    "            1.0               # Type indicator for co-visit\n",
    "        ])\n",
    "        \n",
    "        # Add bidirectional\n",
    "        G.add_edge(poi_i, poi_j, edge_type='co-visit', weight=weight,\n",
    "\t\t\t\traw_count=count, avg_distance=avg_dist, avg_time_gap=avg_time,\n",
    "\t\t\t\tedge_attr=edge_attr)\n",
    "        G.add_edge(poi_j, poi_i, edge_type='co-visit', weight=weight,\n",
    "\t\t\t\traw_count=count, avg_distance=avg_dist, avg_time_gap=avg_time,\n",
    "\t\t\t\tedge_attr=edge_attr)\n",
    "    \n",
    "    print(f\"Added {len(covisit_edges)} bidirectional co-visit edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2406b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_geospatial_edges_optimized(G, poi_coords, threshold_km):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BUILDING GEOSPATIAL EDGES (Optimized with KDTree)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    poi_list = list(poi_coords.keys())\n",
    "    n_pois = len(poi_list)\n",
    "    \n",
    "    # Build coordinate array (lat, lon)\n",
    "    coords = np.array([[poi_coords[p]['lat'], poi_coords[p]['long']] for p in poi_list])\n",
    "    \n",
    "    # Convert km to approximate degrees for initial query (1 deg lat ~ 111 km)\n",
    "    threshold_deg = threshold_km / 111.0\n",
    "    \n",
    "    # Build KD-tree\n",
    "    tree = cKDTree(coords)\n",
    "    \n",
    "    # Query pairs within threshold (much faster than nested loops)\n",
    "    pairs = tree.query_pairs(r=threshold_deg, output_type='ndarray')\n",
    "    print(f\"Found {len(pairs)} candidate pairs within {threshold_km} km\")\n",
    "    \n",
    "    geospatial_edges = 0\n",
    "    for i, j in tqdm(pairs, desc=\"Processing geospatial edges\"):\n",
    "        poi_i = poi_list[i]\n",
    "        poi_j = poi_list[j]\n",
    "        \n",
    "        # Verify exact distance (more accurate than degree approximation)\n",
    "        lat_i, lon_i = coords[i]\n",
    "        lat_j, lon_j = coords[j]\n",
    "        dist = haversine(lat_i, lon_i, lat_j, lon_j)\n",
    "        \n",
    "        if dist <= threshold_km and poi_i != poi_j:\n",
    "            # Calculate bearing for directional feature\n",
    "            bearing = calculate_bearing(lat_i, lon_i, lat_j, lon_j)\n",
    "            \n",
    "            # Weight: inverse distance with exponential decay\n",
    "            weight = np.exp(-dist / threshold_km)  # Range: [e^-1, 1] ~ [0.368, 1]\n",
    "            \n",
    "            # Edge features for GNN\n",
    "            # [weight, normalized_distance, bearing/360, type_encoding=2]\n",
    "            edge_attr = np.array([\n",
    "                weight,\n",
    "                dist / threshold_km,  # Normalized 0-1\n",
    "                bearing / 360.0,      # Normalized 0-1\n",
    "                2.0                   # Type indicator for geospatial\n",
    "            ])\n",
    "            \n",
    "            # Add both directions (geospatial is symmetric but features differ by direction!)\n",
    "            # Note: bearing is directional, so i->j and j->i have different bearings\n",
    "            G.add_edge(poi_i, poi_j, edge_type='geospatial', weight=weight,\n",
    "                       distance_km=dist, bearing=bearing, edge_attr=edge_attr)\n",
    "            \n",
    "            # Reverse direction: bearing + 180\n",
    "            bearing_rev = (bearing + 180) % 360\n",
    "            edge_attr_rev = np.array([\n",
    "                weight,\n",
    "                dist / threshold_km,\n",
    "                bearing_rev / 360.0,\n",
    "                2.0\n",
    "            ])\n",
    "            G.add_edge(poi_j, poi_i, edge_type='geospatial', weight=weight,\n",
    "                       distance_km=dist, bearing=bearing_rev, edge_attr=edge_attr_rev)\n",
    "            \n",
    "            geospatial_edges += 1\n",
    "    \n",
    "    print(f\"Added {geospatial_edges} bidirectional geospatial edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18046d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_self_loops(G):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADDING SELF-LOOPS (Critical for GNN)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    self_loops = 0\n",
    "    for node in G.nodes():\n",
    "        # Check if self-loop exists\n",
    "        if not G.has_edge(node, node):\n",
    "            # Self-loop weight = 1.0, features = [1, 0, 0, 3] (type_encoding=3 for self)\n",
    "            edge_attr = np.array([1.0, 0.0, 0.0, 3.0])\n",
    "            G.add_edge(node, node, edge_type='self', weight=1.0, \n",
    "\t\t\t\t\traw_count=1, edge_attr=edge_attr)\n",
    "            self_loops += 1\n",
    "    \n",
    "    print(f\"Added {self_loops} self-loops\")\n",
    "    return G\n",
    "\n",
    "def normalize_edge_weights_by_type(G):\n",
    "    \"\"\"\n",
    "    Geospatial: ~0.5 avg, Behavioral: 1.0\n",
    "    Normalize each type to have mean 1.0 for balanced GNN aggregation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NORMALIZING EDGE WEIGHTS BY TYPE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    edge_types = defaultdict(list)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        edge_types[data['edge_type']].append(data['weight'])\n",
    "    \n",
    "    # Calculate normalization factors\n",
    "    type_stats = {}\n",
    "    for edge_type, weights in edge_types.items():\n",
    "        mean_w = np.mean(weights)\n",
    "        std_w = np.std(weights)\n",
    "        type_stats[edge_type] = {'mean': mean_w, 'std': std_w}\n",
    "        print(f\"  {edge_type}: mean={mean_w:.4f}, std={std_w:.4f}\")\n",
    "    \n",
    "    # Normalize weights in place\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        etype = data['edge_type']\n",
    "        if type_stats[etype]['mean'] > 0:\n",
    "            # Normalize to mean 1.0, then scale by original std if needed\n",
    "            normalized = data['weight'] / type_stats[etype]['mean']\n",
    "            data['weight_normalized'] = normalized\n",
    "        else:\n",
    "            data['weight_normalized'] = data['weight']\n",
    "    \n",
    "    return G, type_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c82c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING POI CONTEXT GRAPH CONSTRUCTION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "BUILDING CO-SEARCH EDGES (Fixed)\n",
      "======================================================================\n",
      "Searches: 94 (20 users)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Co-searches: 100%|██████████| 20/20 [00:00<00:00, 331.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique co-search pairs: 1\n",
      "Added 1 bidirectional co-search edges\n",
      "\n",
      "======================================================================\n",
      "BUILDING CO-VISIT EDGES (Fixed)\n",
      "======================================================================\n",
      "Visits: 264 (21 users)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Co-visits: 100%|██████████| 21/21 [00:00<00:00, 328.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique co-visit pairs: 4\n",
      "Skipped POIs: 0\n",
      "Added 4 bidirectional co-visit edges\n",
      "\n",
      "======================================================================\n",
      "BUILDING GEOSPATIAL EDGES (Optimized with KDTree)\n",
      "======================================================================\n",
      "Found 2809 candidate pairs within 2 km\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing geospatial edges: 100%|██████████| 2809/2809 [00:00<00:00, 46561.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2804 bidirectional geospatial edges\n",
      "\n",
      "======================================================================\n",
      "ADDING SELF-LOOPS (Critical for GNN)\n",
      "======================================================================\n",
      "Added 0 self-loops\n",
      "\n",
      "======================================================================\n",
      "NORMALIZING EDGE WEIGHTS BY TYPE\n",
      "======================================================================\n",
      "  geospatial: mean=0.5924, std=0.1651\n",
      "  self: mean=1.0000, std=0.0000\n",
      "  co-visit: mean=1.0000, std=0.0000\n",
      "  co-search: mean=1.0000, std=0.0000\n",
      "\n",
      "✓ No isolated nodes (all have self-loops)\n",
      "\n",
      "======================================================================\n",
      "GRAPH STATISTICS (FIXED)\n",
      "======================================================================\n",
      "Nodes: 235\n",
      "Edges: 11471\n",
      "\n",
      "Edges by type:\n",
      "  - geospatial: 11216\n",
      "  - self: 235\n",
      "  - co-visit: 16\n",
      "  - co-search: 4\n",
      "\n",
      "Degree statistics:\n",
      "  Average in-degree: 48.81\n",
      "  Average out-degree: 48.81\n",
      "  Min in-degree: 1 (should be >= 1 with self-loops)\n",
      "  Max in-degree: 135\n",
      "  ⚠️  Graph is not strongly connected (check for disconnected components)\n",
      "  Largest SCC: 183 nodes\n",
      "\n",
      "======================================================================\n",
      "SAVING GRAPH (with edge features for GNN)\n",
      "======================================================================\n",
      "✓ Saved to 'poi_context_graph.pkl'\n",
      "\n",
      "Exporting edge details to CSV...\n",
      "✓ Saved detailed edge list to 'poi_context_graph_edges.csv'\n",
      "\n",
      "Preparing PyTorch Geometric format...\n",
      "⚠️  Could not create PyG format: expected sequence of length 4 at dim 1 (got 5)\n",
      "\n",
      "======================================================================\n",
      "POI CONTEXT GRAPH CONSTRUCTION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING POI CONTEXT GRAPH CONSTRUCTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Build edges with fixed functions\n",
    "    G = build_cosearch_edges_fixed(interactions_df, G, CO_SEARCH_TIME_WINDOW)\n",
    "    G = build_covisit_edges_fixed(interactions_df, G, poi_coords, \n",
    "\t\t\t\t\t\t\t\tCO_VISIT_TIME_WINDOW, CO_VISIT_MAX_DISTANCE)\n",
    "    G = build_geospatial_edges_optimized(G, poi_coords, PROXIMITY_THRESHOLD)\n",
    "    \n",
    "    G = add_self_loops(G)\n",
    "    G, type_stats = normalize_edge_weights_by_type(G)\n",
    "    \n",
    "    # Check for isolated nodes (should be none after self-loops)\n",
    "    isolated = [n for n in G.nodes() if G.degree(n) == 0]\n",
    "    if isolated:\n",
    "        print(f\"\\n⚠️  WARNING: {len(isolated)} isolated nodes detected!\")\n",
    "    else:\n",
    "        print(\"\\n✓ No isolated nodes (all have self-loops)\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GRAPH STATISTICS (FIXED)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    edge_types = defaultdict(int)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        edge_types[data['edge_type']] += 1\n",
    "    \n",
    "    print(\"\\nEdges by type:\")\n",
    "    for edge_type, count in edge_types.items():\n",
    "        print(f\"  - {edge_type}: {count}\")\n",
    "    \n",
    "    # Degree analysis\n",
    "    in_degrees = [d for n, d in G.in_degree()]\n",
    "    out_degrees = [d for n, d in G.out_degree()]\n",
    "    \n",
    "    print(f\"\\nDegree statistics:\")\n",
    "    print(f\"  Average in-degree: {np.mean(in_degrees):.2f}\")\n",
    "    print(f\"  Average out-degree: {np.mean(out_degrees):.2f}\")\n",
    "    print(f\"  Min in-degree: {np.min(in_degrees)} (should be >= 1 with self-loops)\")\n",
    "    print(f\"  Max in-degree: {np.max(in_degrees)}\")\n",
    "    \n",
    "    # Check connectivity\n",
    "    if nx.is_strongly_connected(G):\n",
    "        print(\"  ✓ Graph is strongly connected\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Graph is not strongly connected (check for disconnected components)\")\n",
    "        # Find largest component\n",
    "        largest = max(nx.strongly_connected_components(G), key=len)\n",
    "        print(f\"  Largest SCC: {len(largest)} nodes\")\n",
    "    \n",
    "    # Save graph with edge attributes\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING GRAPH (with edge features for GNN)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open('../../../Sources/Embeddings v3/poi_context_graph.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'graph': G,\n",
    "            'edge_type_stats': type_stats,\n",
    "            'poi_coords': poi_coords,\n",
    "            'config': {\n",
    "                'co_search_window': str(CO_SEARCH_TIME_WINDOW),\n",
    "                'co_visit_window': str(CO_VISIT_TIME_WINDOW),\n",
    "                'co_visit_max_dist': CO_VISIT_MAX_DISTANCE,\n",
    "                'proximity_threshold': PROXIMITY_THRESHOLD\n",
    "            }\n",
    "        }, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"✓ Saved to 'poi_context_graph.pkl'\")\n",
    "    \n",
    "    # Export edge details with features\n",
    "    print(\"\\nExporting edge details to CSV...\")\n",
    "    edge_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        row = {\n",
    "            'source': u,\n",
    "            'target': v,\n",
    "            'edge_type': data['edge_type'],\n",
    "            'weight': data['weight'],\n",
    "            'weight_normalized': data.get('weight_normalized', data['weight']),\n",
    "            'raw_count': data.get('raw_count', 1),\n",
    "            'edge_attr': list(data['edge_attr']) if 'edge_attr' in data else None\n",
    "        }\n",
    "        # Add specific features based on type\n",
    "        if data['edge_type'] == 'geospatial':\n",
    "            row['distance_km'] = data.get('distance_km')\n",
    "            row['bearing'] = data.get('bearing')\n",
    "        elif data['edge_type'] == 'co-visit':\n",
    "            row['avg_distance'] = data.get('avg_distance')\n",
    "            row['avg_time_gap'] = data.get('avg_time_gap')\n",
    "        \n",
    "        edge_data.append(row)\n",
    "    \n",
    "    edges_df = pd.DataFrame(edge_data)\n",
    "    edges_df.to_csv('../../../Sources/Embeddings v3 csv/poi_context_graph_edges.csv', index=False)\n",
    "    print(\"✓ Saved detailed edge list to 'poi_context_graph_edges.csv'\")\n",
    "    \n",
    "    # Prepare PyTorch Geometric format (optional but recommended)\n",
    "    print(\"\\nPreparing PyTorch Geometric format...\")\n",
    "    try:\n",
    "        # Convert to PyG Data object\n",
    "        node_list = list(G.nodes())\n",
    "        node_idx = {node: i for i, node in enumerate(node_list)}\n",
    "        \n",
    "        # Edge index: [2, num_edges]\n",
    "        edge_index = []\n",
    "        edge_weight = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            edge_index.append([node_idx[u], node_idx[v]])\n",
    "            edge_weight.append(data.get('weight_normalized', data['weight']))\n",
    "            edge_attr.append(data['edge_attr'] if 'edge_attr' in data else [0,0,0,0])\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        \n",
    "        pyg_data = {\n",
    "            'num_nodes': len(node_list),\n",
    "            'edge_index': edge_index,\n",
    "            'edge_weight': edge_weight,\n",
    "            'edge_attr': edge_attr,\n",
    "            'node_mapping': node_idx\n",
    "        }\n",
    "        \n",
    "        torch.save(pyg_data, 'poi_graph_pyg_format.pt')\n",
    "        print(\"✓ Saved PyTorch Geometric format to 'poi_graph_pyg_format.pt'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not create PyG format: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"POI CONTEXT GRAPH CONSTRUCTION COMPLETE!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29008f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
