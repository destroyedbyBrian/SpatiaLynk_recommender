{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e3a6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bda28e",
   "metadata": {},
   "source": [
    "## Step 1: Load POI Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50469c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI Tree Mappings Built:\n",
      "  Level 0 (individual_poi): 4696 POIs\n",
      "  Level 1 (container_poi): 1355 POIs\n",
      "  Level 2 (street_poi): 44 POIs\n",
      "  Level 3 (district_poi): 5 POIs\n"
     ]
    }
   ],
   "source": [
    "class POITreeLoader:\n",
    "    def __init__(self, poi_tree_path):\n",
    "        with open(poi_tree_path, 'r') as f:\n",
    "            self.poi_tree = json.load(f)\n",
    "        \n",
    "        self.level_names = {\n",
    "            0: 'individual_poi',\n",
    "            1: 'container_poi', \n",
    "            2: 'street_poi',\n",
    "            3: 'district_poi'\n",
    "        }\n",
    "        \n",
    "        self.num_levels = 4\n",
    "        self._build_mappings()\n",
    "    \n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build POI ID mappings and parent-child relationships\"\"\"\n",
    "        self.poi_to_idx = {}\n",
    "        self.idx_to_poi = {}\n",
    "        self.parent_child = defaultdict(list)\n",
    "        self.poi_level_counts = {}\n",
    "        \n",
    "        for level in range(self.num_levels):\n",
    "            level_key = f'level_{level}'\n",
    "            pois = self.poi_tree[level_key]\n",
    "            \n",
    "            self.poi_to_idx[level] = {}\n",
    "            self.idx_to_poi[level] = {}\n",
    "            \n",
    "            for idx, (poi_id, poi_data) in enumerate(pois.items()):\n",
    "                self.poi_to_idx[level][poi_id] = idx\n",
    "                self.idx_to_poi[level][idx] = poi_id\n",
    "                \n",
    "                # Build parent-child relationships\n",
    "                if 'parent' in poi_data and poi_data['parent']:\n",
    "                    parent = poi_data['parent']\n",
    "                    self.parent_child[parent].append(poi_id)\n",
    "            \n",
    "            self.poi_level_counts[level] = len(pois)\n",
    "        \n",
    "        print(\"POI Tree Mappings Built:\")\n",
    "        for level in range(self.num_levels):\n",
    "            print(f\"  Level {level} ({self.level_names[level]}): {self.poi_level_counts[level]} POIs\")\n",
    "\n",
    "poi_loader = POITreeLoader(\"../Sources/Files/poi_tree_with_uuids.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e47841",
   "metadata": {},
   "source": [
    "## Step 2 : Load all Embeddings files for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01022117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelDataPreparation:\n",
    "    def __init__(self, poi_loader, user_embeddings, poi_embeddings, \n",
    "                interactions, metadata, target_poi_dim=20):\n",
    "        self.poi_loader = poi_loader\n",
    "        self.num_levels = poi_loader.num_levels\n",
    "        self.num_users = metadata['counts']['users']\n",
    "        self.target_poi_dim = target_poi_dim \n",
    "        \n",
    "        # Load embeddings\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.poi_embeddings = poi_embeddings\n",
    "        self.interactions = interactions\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        self._prepare_attribute_matrices()\n",
    "        self._prepare_interaction_matrices()\n",
    "    \n",
    "    # def _prepare_interaction_matrices(self):\n",
    "    #     \"\"\"\n",
    "    #     Parse interaction data from nested structure with 'edges' key\n",
    "    #     \"\"\"\n",
    "    #     print(\"\\nPreparing Interaction Matrices for L2...\")\n",
    "        \n",
    "    #     self.positive_samples = {}\n",
    "        \n",
    "    #     # Step 1: Extract positive samples for all levels\n",
    "    #     for level in range(self.num_levels):\n",
    "    #         level_key = f'level_{level}'\n",
    "            \n",
    "    #         pos_samples = self._extract_samples_from_level(\n",
    "    #             self.interactions['interactions'], \n",
    "    #             level_key, \n",
    "    #             'positive'\n",
    "    #         )\n",
    "    #         self.positive_samples[level] = pos_samples\n",
    "        \n",
    "    #     # Step 2: Generate negative samples for all levels at once\n",
    "    #     self._generate_negative_samples(num_negatives_per_positive=1)\n",
    "        \n",
    "    #     # Step 3: Print summary\n",
    "    #     for level in range(self.num_levels):\n",
    "    #         print(f\"  Level {level}: {self.positive_samples[level].shape[0]} positive, \"\n",
    "    #             f\"{self.negative_samples[level].shape[0]} negative samples\")\n",
    "            \n",
    "    # def _extract_samples_from_level(self, data_dict, level_key, sample_type):\n",
    "    #     \"\"\"\n",
    "    #     Extract user-POI pairs from the nested structure\n",
    "        \n",
    "    #     Expected structure:\n",
    "    #     data_dict[level_key] = {\n",
    "    #         'edges': {\n",
    "    #             'user_indices': array([...]),\n",
    "    #             'poi_indices': array([...]),\n",
    "    #             ...\n",
    "    #         },\n",
    "    #         'matrices': {...},\n",
    "    #         'user_to_pois': {...}\n",
    "    #     }\n",
    "        \n",
    "    #     âœ… Returns NumPy array (not TensorFlow tensor)\n",
    "    #     \"\"\"\n",
    "    #     if level_key not in data_dict:\n",
    "    #         print(f\"  Warning: {level_key} not found in {sample_type} samples\")\n",
    "    #         return np.array([], dtype=np.int32).reshape(0, 2)\n",
    "        \n",
    "    #     level_data = data_dict[level_key]\n",
    "        \n",
    "    #     # Check if it's the nested structure with 'edges'\n",
    "    #     if isinstance(level_data, dict) and 'edges' in level_data:\n",
    "    #         edges = level_data['edges']\n",
    "            \n",
    "    #         # Extract indices from edges\n",
    "    #         if 'user_indices' in edges and 'poi_indices' in edges:\n",
    "    #             user_indices = edges['user_indices']\n",
    "    #             poi_indices = edges['poi_indices']\n",
    "                \n",
    "    #             # Convert to numpy if needed\n",
    "    #             if not isinstance(user_indices, np.ndarray):\n",
    "    #                 user_indices = np.array(user_indices, dtype=np.int32)\n",
    "    #             if not isinstance(poi_indices, np.ndarray):\n",
    "    #                 poi_indices = np.array(poi_indices, dtype=np.int32)\n",
    "                \n",
    "    #             # Ensure they have the same length\n",
    "    #             if len(user_indices) != len(poi_indices):\n",
    "    #                 print(f\"  Warning: Mismatched lengths for level {level_key}: \"\n",
    "    #                     f\"users={len(user_indices)}, pois={len(poi_indices)}\")\n",
    "    #                 min_len = min(len(user_indices), len(poi_indices))\n",
    "    #                 user_indices = user_indices[:min_len]\n",
    "    #                 poi_indices = poi_indices[:min_len]\n",
    "                \n",
    "    #             # Stack into [N, 2] array\n",
    "    #             samples = np.column_stack([user_indices, poi_indices])\n",
    "    #             return samples  # âœ… Return NumPy array\n",
    "    #         else:\n",
    "    #             print(f\"  Warning: 'user_indices' or 'poi_indices' not found in {level_key} edges\")\n",
    "    #             return np.array([], dtype=np.int32).reshape(0, 2)\n",
    "        \n",
    "    #     # If it's directly a dict, list, or array\n",
    "    #     elif isinstance(level_data, (list, np.ndarray)):\n",
    "    #         try:\n",
    "    #             data_array = np.array(level_data, dtype=np.int32)\n",
    "    #             if len(data_array.shape) == 2 and data_array.shape[1] == 2:\n",
    "    #                 return data_array\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"  Warning: Could not parse {level_key} as array: {e}\")\n",
    "        \n",
    "    #     print(f\"  Warning: Unknown structure for {level_key} in {sample_type} samples\")\n",
    "    #     return np.array([], dtype=np.int32).reshape(0, 2)\n",
    "    \n",
    "    # def _generate_negative_samples(self, num_negatives_per_positive=1):\n",
    "    #     \"\"\"\n",
    "    #     Generate multiple negative samples per positive sample\n",
    "    #     âœ… Works on all levels at once\n",
    "    #     \"\"\"\n",
    "    #     self.negative_samples = {}\n",
    "        \n",
    "    #     for level in range(self.num_levels):\n",
    "    #         pos_samples = self.positive_samples[level]\n",
    "            \n",
    "    #         if pos_samples.shape[0] == 0:\n",
    "    #             self.negative_samples[level] = np.array([], dtype=np.int32).reshape(0, 2)\n",
    "    #             continue\n",
    "            \n",
    "    #         neg_samples = []\n",
    "    #         num_pois = self.poi_loader.poi_level_counts[level]\n",
    "            \n",
    "    #         # âœ… FIX: Ensure pos_samples is NumPy array (not TensorFlow tensor)\n",
    "    #         if isinstance(pos_samples, tf.Tensor):\n",
    "    #             pos_samples = pos_samples.numpy()\n",
    "            \n",
    "    #         # Build user -> positive POIs mapping\n",
    "    #         user_positive_pois = {}\n",
    "    #         for user_idx, poi_idx in pos_samples:\n",
    "    #             user_idx = int(user_idx)  # âœ… Convert to Python int\n",
    "    #             poi_idx = int(poi_idx)\n",
    "                \n",
    "    #             if user_idx not in user_positive_pois:\n",
    "    #                 user_positive_pois[user_idx] = set()\n",
    "    #             user_positive_pois[user_idx].add(poi_idx)\n",
    "            \n",
    "    #         # Generate negative samples\n",
    "    #         for i in range(pos_samples.shape[0]):\n",
    "    #             user_idx = int(pos_samples[i, 0])\n",
    "    #             pos_poi_set = user_positive_pois.get(user_idx, set())\n",
    "                \n",
    "    #             # Sample N negative POIs for this user\n",
    "    #             for _ in range(num_negatives_per_positive):\n",
    "    #                 neg_poi_idx = np.random.randint(0, num_pois)\n",
    "                    \n",
    "    #                 # Ensure it's not a positive POI\n",
    "    #                 max_attempts = 100\n",
    "    #                 attempts = 0\n",
    "    #                 while neg_poi_idx in pos_poi_set and attempts < max_attempts:\n",
    "    #                     neg_poi_idx = np.random.randint(0, num_pois)\n",
    "    #                     attempts += 1\n",
    "                    \n",
    "    #                 neg_samples.append([user_idx, neg_poi_idx])\n",
    "            \n",
    "    #         self.negative_samples[level] = np.array(neg_samples, dtype=np.int32)\n",
    "            \n",
    "    #         print(f\"  Level {level}: Generated {len(neg_samples)} negative samples \"\n",
    "    #             f\"({num_negatives_per_positive} per positive)\")\n",
    "            \n",
    "    def _prepare_attribute_matrices(self):\n",
    "        \"\"\"Prepare and normalize all attribute matrices\"\"\"\n",
    "        print(\"\\nPreparing Attribute Matrices for L1...\")\n",
    "        \n",
    "        # Build raw matrices\n",
    "        self.X_raw = self._build_user_attribute_matrix()\n",
    "        self.Y_raw = {}\n",
    "        for level in range(self.num_levels):\n",
    "            self.Y_raw[level] = self._build_poi_attribute_matrix(level)\n",
    "            \n",
    "            # Verify dimensions\n",
    "            if self.Y_raw[level].shape[1] != self.target_poi_dim:\n",
    "                print(f\"  WARNING: Level {level} has {self.Y_raw[level].shape[1]} features, \"\n",
    "                    f\"expected {self.target_poi_dim}\")\n",
    "        \n",
    "        # Standardize user features\n",
    "        X_mean = np.mean(self.X_raw, axis=0, keepdims=True)\n",
    "        X_std = np.std(self.X_raw, axis=0, keepdims=True) + 1e-8\n",
    "        self.X = (self.X_raw - X_mean) / X_std\n",
    "        self.X = tf.constant(self.X, dtype=tf.float32)\n",
    "        \n",
    "        print(f\"  User attribute matrix X:\")\n",
    "        print(f\"    Shape: {self.X.shape}\")\n",
    "        print(f\"    Normalized: mean={self.X.numpy().mean():.4f}, std={self.X.numpy().std():.4f}\")\n",
    "        \n",
    "        # Standardize POI features per level\n",
    "        self.Y = {}\n",
    "        for level in range(self.num_levels):\n",
    "            Y_raw = self.Y_raw[level]\n",
    "            \n",
    "            # Check if all zeros\n",
    "            if Y_raw.std() < 1e-6:\n",
    "                print(f\"  WARNING: Level {level} has near-zero variance! Adding small noise.\")\n",
    "                Y_raw = Y_raw + np.random.randn(*Y_raw.shape).astype(np.float32) * 0.01\n",
    "            \n",
    "            Y_mean = np.mean(Y_raw, axis=0, keepdims=True)\n",
    "            Y_std = np.std(Y_raw, axis=0, keepdims=True) + 1e-8\n",
    "            Y_normalized = (Y_raw - Y_mean) / Y_std\n",
    "            \n",
    "            self.Y[level] = tf.constant(Y_normalized, dtype=tf.float32)\n",
    "            \n",
    "            print(f\"  POI attribute matrix Y^{level}:\")\n",
    "            print(f\"    Shape: {self.Y[level].shape}\")\n",
    "            print(f\"    Normalized: mean={Y_normalized.mean():.4f}, std={Y_normalized.std():.4f}\")\n",
    "\n",
    "    def _build_user_attribute_matrix(self):\n",
    "        \"\"\"Build user attribute matrix\"\"\"\n",
    "        user_ids = self.metadata['user_ids']\n",
    "        features_list = []\n",
    "        \n",
    "        for user_id in user_ids:\n",
    "            if user_id in self.user_embeddings['user_embeddings']:\n",
    "                user_data = self.user_embeddings['user_embeddings'][user_id]\n",
    "                features = self._extract_user_features(user_data)\n",
    "            else:\n",
    "                features = np.zeros(self._get_user_feature_dim())\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        X = np.array(features_list, dtype=np.float32)\n",
    "        return X\n",
    "    \n",
    "    def _get_user_feature_dim(self):\n",
    "        \"\"\"Get dimension of user features\"\"\"\n",
    "        if self.user_embeddings['user_embeddings']:\n",
    "            sample_user_id = list(self.user_embeddings['user_embeddings'].keys())[0]\n",
    "            sample_data = self.user_embeddings['user_embeddings'][sample_user_id]\n",
    "            sample_features = self._extract_user_features(sample_data)\n",
    "            return len(sample_features)\n",
    "        return 10\n",
    "    \n",
    "    def _extract_user_features(self, user_data):\n",
    "        \"\"\"Extract feature vector from user data\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        if isinstance(user_data, (np.ndarray, list)):\n",
    "            return np.array(user_data, dtype=np.float32).flatten()\n",
    "        \n",
    "        if isinstance(user_data, dict):\n",
    "            for key in sorted(user_data.keys()):\n",
    "                value = user_data[key]\n",
    "                if isinstance(value, (int, float, np.number)):\n",
    "                    features.append(float(value))\n",
    "                elif isinstance(value, (list, np.ndarray)):\n",
    "                    arr = np.array(value).flatten()\n",
    "                    features.extend(arr.tolist())\n",
    "        \n",
    "        if not features:\n",
    "            features = np.zeros(10)\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def _parse_price(self, price_str):\n",
    "        \"\"\"Parse price string\"\"\"\n",
    "        try:\n",
    "            price_str = str(price_str).replace('$', '').strip()\n",
    "            parts = price_str.split(' - ')\n",
    "            if len(parts) == 2:\n",
    "                avg_price = (float(parts[0]) + float(parts[1])) / 2.0\n",
    "                return avg_price / 50.0\n",
    "            elif len(parts) == 1:\n",
    "                return float(parts[0]) / 50.0\n",
    "        except:\n",
    "            pass\n",
    "        return 0.0\n",
    "    \n",
    "    def _build_poi_attribute_matrix(self, level):\n",
    "        \"\"\"Build Y^l with consistent dimensions across all levels\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        pois = self.poi_loader.poi_tree[level_key]\n",
    "        \n",
    "        poi_features = []\n",
    "        for poi_id in sorted(pois.keys()):\n",
    "            poi_data = pois[poi_id]['data']\n",
    "            features = self._extract_poi_features_by_level(poi_data, level)\n",
    "            \n",
    "            # Pad or truncate to target dimension\n",
    "            if len(features) < self.target_poi_dim:\n",
    "                # Pad with zeros\n",
    "                padding = np.zeros(self.target_poi_dim - len(features), dtype=np.float32)\n",
    "                features = np.concatenate([features, padding])\n",
    "            elif len(features) > self.target_poi_dim:\n",
    "                # Truncate\n",
    "                features = features[:self.target_poi_dim]\n",
    "            \n",
    "            poi_features.append(features)\n",
    "        \n",
    "        Y_l = np.array(poi_features, dtype=np.float32)\n",
    "        return Y_l\n",
    "    \n",
    "    def _extract_poi_features_by_level(self, poi_data, level):\n",
    "        \"\"\"Extract features specific to each level\"\"\"\n",
    "        \n",
    "        if level in [0, 1]:  # individual_poi, container_poi\n",
    "            return self._extract_detailed_poi_features(poi_data)\n",
    "        elif level == 2:  # street_poi\n",
    "            return self._extract_street_features(poi_data)\n",
    "        elif level == 3:  # district_poi\n",
    "            return self._extract_district_features(poi_data)\n",
    "        \n",
    "    def _extract_detailed_poi_features(self, poi_data):\n",
    "        \"\"\"Features for individual POIs and containers\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Normalized coordinates\n",
    "        if 'latitude' in poi_data and 'longitude' in poi_data:\n",
    "            lat_norm = (float(poi_data['latitude']) - 1.2) / 0.3\n",
    "            lon_norm = (float(poi_data['longitude']) - 103.6) / 0.4\n",
    "            features.extend([np.clip(lat_norm, 0, 1), np.clip(lon_norm, 0, 1)])\n",
    "        else:\n",
    "            features.extend([0.5, 0.5])\n",
    "        \n",
    "        # Category\n",
    "        categories = ['supermarket', 'restaurant', 'cafe', 'shop', 'entertainment', \n",
    "                    'hotel', 'mall', 'school', 'hospital', 'atm', 'shopping_mall', \n",
    "                    'convenience_store', 'other']\n",
    "        category = poi_data.get('category', 'other')\n",
    "        category_encoding = [1.0 if cat == category else 0.0 for cat in categories]\n",
    "        features.extend(category_encoding)\n",
    "        \n",
    "        # Price (normalized)\n",
    "        price_val = self._parse_price(poi_data.get('price', '0'))\n",
    "        features.append(price_val)\n",
    "        \n",
    "        # Popularity (normalized)\n",
    "        popularity = float(poi_data.get('popularity', 0)) / 5.0\n",
    "        features.append(popularity)\n",
    "        \n",
    "        # Characteristics\n",
    "        char_str = str(poi_data.get('characteristic', '')).lower()\n",
    "        characteristics = ['budget', 'premium', 'family', 'essentials', 'luxury']\n",
    "        char_encoding = [1.0 if char in char_str else 0.0 for char in characteristics]\n",
    "        features.extend(char_encoding)\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def _extract_street_features(self, poi_data):\n",
    "        \"\"\"Features for streets (Level 2) - same dimension as detailed features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Coordinates (2 features)\n",
    "        if 'latitude' in poi_data and 'longitude' in poi_data:\n",
    "            lat_norm = (float(poi_data['latitude']) - 1.2) / 0.3\n",
    "            lon_norm = (float(poi_data['longitude']) - 103.6) / 0.4\n",
    "            features.extend([np.clip(lat_norm, 0, 1), np.clip(lon_norm, 0, 1)])\n",
    "        else:\n",
    "            features.extend([0.5, 0.5])\n",
    "        \n",
    "        # 2. District/Area one-hot (8 features - to match category dimension)\n",
    "        district = poi_data.get('district', 'OTHER')\n",
    "        districts = [\n",
    "            \"Orchard Road\",\n",
    "            \"Scotts Road\",\n",
    "            \"Bras Basah Road\",\n",
    "            \"Bugis Street\",\n",
    "            \"Victoria Street\",\n",
    "            \"North Bridge Road\",\n",
    "            \"Beach Road\",\n",
    "            \"Arab Street\",\n",
    "            \"Haji Lane\",\n",
    "            \"Chinatown Point Road\",\n",
    "            \"South Bridge Road\",\n",
    "            \"Eu Tong Sen Street\",\n",
    "            \"New Bridge Road\",\n",
    "            \"Tanjong Pagar Road\",\n",
    "            \"Anson Road\",\n",
    "            \"Shenton Way\",\n",
    "            \"Raffles Place\",\n",
    "            \"Collyer Quay\",\n",
    "            \"Marina Boulevard\",\n",
    "            \"Marina Bay Sands Drive\",\n",
    "            \"Cecil Street\",\n",
    "            \"Robinson Road\",\n",
    "            \"Telok Ayer Street\",\n",
    "            \"Amoy Street\",\n",
    "            \"Keong Saik Road\",\n",
    "            \"Bukit Timah Road\",\n",
    "            \"Holland Road\",\n",
    "            \"Clementi Road\",\n",
    "            \"Upper Thomson Road\",\n",
    "            \"Serangoon Road\",\n",
    "            \"Balestier Road\",\n",
    "            \"Thomson Road\",\n",
    "            \"Geylang Road\",\n",
    "            \"Paya Lebar Road\",\n",
    "            \"East Coast Road\",\n",
    "            \"Tampines Avenue 1\",\n",
    "            \"Pasir Ris Drive 1\",\n",
    "            \"Ang Mo Kio Avenue 3\",\n",
    "            \"Yishun Ring Road\",\n",
    "            \"Woodlands Avenue 6\",\n",
    "            \"Jurong West Street 41\",\n",
    "            \"Boon Lay Way\",\n",
    "            \"Choa Chu Kang Avenue 4\",\n",
    "            \"Bukit Batok Road\",\n",
    "            \"Sengkang East Way\",\n",
    "            \"Punggol Central\"\n",
    "        ]\n",
    "\n",
    "        district_encoding = [1.0 if d == district else 0.0 for d in districts]\n",
    "        features.extend(district_encoding)\n",
    "        \n",
    "        # 3. Average price level (1 feature)\n",
    "        features.append(float(poi_data.get('avg_price', 0)))\n",
    "        \n",
    "        # 4. Average popularity (1 feature)\n",
    "        features.append(float(poi_data.get('avg_popularity', 0)) / 5.0)\n",
    "        \n",
    "        # 5. Street characteristics (5 features - to match characteristics dimension)\n",
    "        street_type = str(poi_data.get('type', '')).lower()\n",
    "        street_types = ['commercial', 'residential', 'mixed', 'tourist', 'other']\n",
    "        type_encoding = [1.0 if st in street_type else 0.0 for st in street_types]\n",
    "        features.extend(type_encoding)\n",
    "        \n",
    "        # Total: 2 + 8 + 1 + 1 + 5 = 17 features\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def _extract_district_features(self, poi_data):\n",
    "        \"\"\"Features for districts (Level 3) - same dimension as detailed features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. District center coordinates (2 features)\n",
    "        if 'latitude' in poi_data and 'longitude' in poi_data:\n",
    "            lat_norm = (float(poi_data['latitude']) - 1.2) / 0.3\n",
    "            lon_norm = (float(poi_data['longitude']) - 103.6) / 0.4\n",
    "            features.extend([np.clip(lat_norm, 0, 1), np.clip(lon_norm, 0, 1)])\n",
    "        else:\n",
    "            features.extend([0.5, 0.5])\n",
    "        \n",
    "        # 2. District name one-hot (8 features)\n",
    "        district_name = poi_data.get('name', 'OTHER')\n",
    "        districts = [\n",
    "            \"ANG MO KIO\",\n",
    "            \"BEDOK\",\n",
    "            \"BISHAN\",\n",
    "            \"BOON LAY\",\n",
    "            \"BUKIT BATOK\",\n",
    "            \"BUKIT MERAH\",\n",
    "            \"BUKIT PANJANG\",\n",
    "            \"BUKIT TIMAH\",\n",
    "            \"CENTRAL WATER CATCHMENT\",\n",
    "            \"CHANGI\",\n",
    "            \"CHANGI BAY\",\n",
    "            \"CHOA CHU KANG\",\n",
    "            \"CLEMENTI\",\n",
    "            \"DOWNTOWN CORE\",\n",
    "            \"GEYLANG\",\n",
    "            \"HOUGANG\",\n",
    "            \"JURONG EAST\",\n",
    "            \"JURONG WEST\",\n",
    "            \"KALLANG\",\n",
    "            \"LIM CHU KANG\",\n",
    "            \"MANDAI\",\n",
    "            \"MARINA EAST\",\n",
    "            \"MARINA SOUTH\",\n",
    "            \"MARINA BAY\",\n",
    "            \"NOVENA\",\n",
    "            \"ORCHARD\",\n",
    "            \"OUTRAM\",\n",
    "            \"PASIR RIS\",\n",
    "            \"PAYA LEBAR\",\n",
    "            \"PIONEER\",\n",
    "            \"PUNGGOL\",\n",
    "            \"QUEENSTOWN\",\n",
    "            \"RIVER VALLEY\",\n",
    "            \"ROCHOR\",\n",
    "            \"SELETAR\",\n",
    "            \"SEMBAWANG\",\n",
    "            \"SENGKANG\",\n",
    "            \"SERANGOON\",\n",
    "            \"SIMPANG\",\n",
    "            \"SOUTHERN ISLANDS\",\n",
    "            \"STRAITS VIEW\",\n",
    "            \"SUNGEI KADUT\",\n",
    "            \"TAMPINES\",\n",
    "            \"TANGLIN\",\n",
    "            \"TENGAH\",\n",
    "            \"TOA PAYOH\",\n",
    "            \"TUAS\",\n",
    "            \"WESTERN ISLANDS\",\n",
    "            \"WESTERN WATER CATCHMENT\",\n",
    "            \"WOODLANDS\",\n",
    "            \"YISHUN\"\n",
    "        ]\n",
    "\n",
    "        district_encoding = [1.0 if d == district_name else 0.0 for d in districts]\n",
    "        features.extend(district_encoding)\n",
    "        \n",
    "        # 3. District density (1 feature)\n",
    "        density = float(poi_data.get('num_pois', 0)) / 1000.0  # Normalize\n",
    "        features.append(density)\n",
    "        \n",
    "        # 4. District average popularity (1 feature)\n",
    "        features.append(float(poi_data.get('avg_popularity', 0)) / 5.0)\n",
    "        \n",
    "        # 5. Region one-hot (5 features)\n",
    "        region = poi_data.get('region', 'OTHER')\n",
    "        regions = ['CENTRAL', 'NORTH', 'SOUTH', 'EAST', 'WEST']\n",
    "        region_encoding = [1.0 if r == region else 0.0 for r in regions]\n",
    "        features.extend(region_encoding)\n",
    "        \n",
    "        # Total: 2 + 8 + 1 + 1 + 5 = 17 features\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e574c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointLossComputation:\n",
    "    def __init__(self, data_prep, embedding_dim=32):\n",
    "        self.data_prep = data_prep\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_levels = data_prep.num_levels\n",
    "        self.num_users = data_prep.num_users\n",
    "\n",
    "        self.L1_scale = None\n",
    "        self.L2_scale = None\n",
    "        self.calibration_epochs = 10\n",
    "        self.epoch_counter = 0\n",
    "        \n",
    "        # Initialize learnable parameters\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize with level-specific transformation matrices\"\"\"\n",
    "        \n",
    "        # User latent matrix\n",
    "        user_stddev = np.sqrt(2.0 / (self.num_users + self.embedding_dim))\n",
    "        self.U_u = tf.Variable(\n",
    "            tf.random.normal([self.num_users, self.embedding_dim], stddev=user_stddev),\n",
    "            name='U_u',\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # POI latent matrices for each level\n",
    "        self.U_p = {}\n",
    "        for level in range(self.num_levels):\n",
    "            num_pois = self.data_prep.poi_loader.poi_level_counts[level]\n",
    "            poi_stddev = np.sqrt(2.0 / (num_pois + self.embedding_dim))\n",
    "            \n",
    "            self.U_p[level] = tf.Variable(\n",
    "                tf.random.normal([num_pois, self.embedding_dim], stddev=poi_stddev),\n",
    "                name=f'U_p_level_{level}',\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "        \n",
    "        # User transformation matrix\n",
    "        user_feature_dim = self.data_prep.X.shape[1]\n",
    "        v_u_stddev = np.sqrt(2.0 / (user_feature_dim + self.embedding_dim))\n",
    "        \n",
    "        self.V_u = tf.Variable(\n",
    "            tf.random.normal([user_feature_dim, self.embedding_dim], stddev=v_u_stddev),\n",
    "            name='V_u',\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # ðŸ”´ FIX: Level-specific transformation matrices (2D, not 1D!)\n",
    "        self.V_p = {}\n",
    "        for level in range(self.num_levels):\n",
    "            poi_feature_dim = self.data_prep.Y[level].shape[1]  # Should be 20\n",
    "            v_p_stddev = np.sqrt(2.0 / (poi_feature_dim + self.embedding_dim))\n",
    "            \n",
    "            # âœ… Correct: [poi_feature_dim, embedding_dim]\n",
    "            self.V_p[level] = tf.Variable(\n",
    "                tf.random.normal([poi_feature_dim, self.embedding_dim], stddev=v_p_stddev),\n",
    "                name=f'V_p_level_{level}',\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            \n",
    "            print(f\"  V_p^{level}: shape {self.V_p[level].shape} (feature_dim={poi_feature_dim}, embed_dim={self.embedding_dim})\")\n",
    "        \n",
    "        print(f\"\\nInitialized Parameters:\")\n",
    "        print(f\"  U_u: {self.U_u.shape}\")\n",
    "        for level in range(self.num_levels):\n",
    "            print(f\"  U_p^{level}: {self.U_p[level].shape}\")\n",
    "        print(f\"  V_u: {self.V_u.shape}\")\n",
    "        for level in range(self.num_levels):\n",
    "            print(f\"  V_p^{level}: {self.V_p[level].shape}\")\n",
    "\n",
    "    def compute_L1_loss(self):\n",
    "        \"\"\"Compute L1 loss using MEAN squared error instead of SUM\"\"\"\n",
    "        # User reconstruction\n",
    "        X_reconstructed = tf.matmul(self.U_u, self.V_u, transpose_b=True)\n",
    "        # CHANGE: reduce_sum -> reduce_mean\n",
    "        L1_user = tf.reduce_mean(tf.square(X_reconstructed - self.data_prep.X)) \n",
    "        \n",
    "        L1_poi = tf.constant(0.0, dtype=tf.float32)\n",
    "        poi_losses = {}\n",
    "        \n",
    "        for level in range(self.num_levels):\n",
    "            Y_l = self.data_prep.Y[level]\n",
    "            Y_reconstructed = tf.matmul(self.U_p[level], self.V_p[level], transpose_b=True)\n",
    "            \n",
    "            # CHANGE: reduce_sum -> reduce_mean\n",
    "            level_loss = tf.reduce_mean(tf.square(Y_reconstructed - Y_l))\n",
    "            L1_poi += level_loss\n",
    "            poi_losses[level] = level_loss\n",
    "        \n",
    "        L1_total = L1_user + L1_poi\n",
    "        return L1_total, L1_user, L1_poi, poi_losses\n",
    "    \n",
    "    def compute_L2_loss(self):\n",
    "        \"\"\"\n",
    "        Compute L2 (BPR) loss for ranking\n",
    "        L2 = -Î£_l Î£_i Î£_j log(Ïƒ(u_i^T v_{p+}^l - u_i^T v_{p-}^l))\n",
    "        \n",
    "        âœ… FIX: Use the SAME user for positive and negative samples\n",
    "        \"\"\"\n",
    "        L2_total = tf.constant(0.0, dtype=tf.float32)\n",
    "        level_losses = {}\n",
    "        \n",
    "        for level in range(self.num_levels):\n",
    "            pos_samples = tf.constant(self.data_prep.positive_samples[level], dtype=tf.int32)\n",
    "            neg_samples = tf.constant(self.data_prep.negative_samples[level], dtype=tf.int32)\n",
    "            \n",
    "            if pos_samples.shape[0] == 0:\n",
    "                level_losses[level] = tf.constant(0.0, dtype=tf.float32)\n",
    "                continue\n",
    "            \n",
    "            # âœ… FIX: Both samples should have same user index\n",
    "            # pos_samples: [user_idx, pos_poi_idx]\n",
    "            # neg_samples: [user_idx, neg_poi_idx]  <- SAME user_idx\n",
    "            \n",
    "            user_indices = pos_samples[:, 0]  # Get user indices from positive samples\n",
    "            pos_poi_indices = pos_samples[:, 1]\n",
    "            neg_poi_indices = neg_samples[:, 1]\n",
    "            \n",
    "            # Verify shapes match\n",
    "            if pos_poi_indices.shape[0] != neg_poi_indices.shape[0]:\n",
    "                raise ValueError(\n",
    "                    f\"Level {level}: Positive and negative samples must have same length. \"\n",
    "                    f\"Got pos={pos_poi_indices.shape[0]}, neg={neg_poi_indices.shape[0]}\"\n",
    "                )\n",
    "            \n",
    "            # Get embeddings\n",
    "            u = tf.gather(self.U_u, user_indices)  # Same user for both\n",
    "            v_pos = tf.gather(self.U_p[level], pos_poi_indices)\n",
    "            v_neg = tf.gather(self.U_p[level], neg_poi_indices)\n",
    "            \n",
    "            # Compute scores\n",
    "            scores_pos = tf.reduce_sum(u * v_pos, axis=1)  # [batch_size]\n",
    "            scores_neg = tf.reduce_sum(u * v_neg, axis=1)  # [batch_size]\n",
    "            \n",
    "            # Now shapes match!\n",
    "            diff = scores_pos - scores_neg  # [batch_size]\n",
    "            loss = -tf.reduce_sum(tf.math.log(tf.sigmoid(diff) + 1e-10))\n",
    "            \n",
    "            L2_total += loss\n",
    "            level_losses[level] = loss\n",
    "        \n",
    "        return L2_total, level_losses\n",
    "    \n",
    "    def compute_regularization(self):\n",
    "        \"\"\"Compute regularization term\"\"\"\n",
    "        reg = tf.reduce_sum(tf.square(self.U_u))\n",
    "        reg += tf.reduce_sum(tf.square(self.V_u))\n",
    "        \n",
    "        for level in range(self.num_levels):\n",
    "            reg += tf.reduce_sum(tf.square(self.U_p[level]))\n",
    "            reg += tf.reduce_sum(tf.square(self.V_p[level]))  # âœ… [level] not missing\n",
    "        \n",
    "        return reg\n",
    "    \n",
    "    def _compute_L2_for_level(self, level):\n",
    "        \"\"\"Compute BPR loss for a specific level\"\"\"\n",
    "        pos_samples = self.data_prep.positive_samples[level]\n",
    "        neg_samples = self.data_prep.negative_samples[level]\n",
    "        \n",
    "        # Check if we have samples\n",
    "        if pos_samples.shape[0] == 0 or neg_samples.shape[0] == 0:\n",
    "            return tf.constant(0.0, dtype=tf.float32)\n",
    "        \n",
    "        # Ensure we have the same number of positive and negative samples\n",
    "        min_samples = min(pos_samples.shape[0], neg_samples.shape[0])\n",
    "        pos_samples = pos_samples[:min_samples]\n",
    "        neg_samples = neg_samples[:min_samples]\n",
    "        \n",
    "        # Extract user and POI indices\n",
    "        pos_user_indices = pos_samples[:, 0]\n",
    "        pos_poi_indices = pos_samples[:, 1]\n",
    "        neg_user_indices = neg_samples[:, 0]\n",
    "        neg_poi_indices = neg_samples[:, 1]\n",
    "        \n",
    "        # Get embeddings (vectorized)\n",
    "        u_pos = tf.gather(self.U_u, pos_user_indices)  # [n_samples, embedding_dim]\n",
    "        v_pos = tf.gather(self.U_p[level], pos_poi_indices)  # [n_samples, embedding_dim]\n",
    "        u_neg = tf.gather(self.U_u, neg_user_indices)  # [n_samples, embedding_dim]\n",
    "        v_neg = tf.gather(self.U_p[level], neg_poi_indices)  # [n_samples, embedding_dim]\n",
    "        \n",
    "        # Compute scores (element-wise dot product)\n",
    "        scores_pos = tf.reduce_sum(u_pos * v_pos, axis=1)  # [n_samples]\n",
    "        scores_neg = tf.reduce_sum(u_neg * v_neg, axis=1)  # [n_samples]\n",
    "        \n",
    "        # BPR loss: -ln(Ïƒ(score_pos - score_neg))\n",
    "        diff = scores_pos - scores_neg\n",
    "        loss = -tf.reduce_sum(tf.math.log(tf.sigmoid(diff) + 1e-10))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_regularization(self):\n",
    "        reg = tf.reduce_sum(tf.square(self.U_u))\n",
    "        reg += tf.reduce_sum(tf.square(self.V_u))\n",
    "        \n",
    "        for level in [0, 1]:\n",
    "            reg += tf.reduce_sum(tf.square(self.U_p[level]))\n",
    "            reg += tf.reduce_sum(tf.square(self.V_p[level]))\n",
    "        \n",
    "        # âœ… FIX: Divide by total number of parameters\n",
    "        total_params = (\n",
    "            tf.size(self.U_u, out_type=tf.float32) +\n",
    "            tf.size(self.V_u, out_type=tf.float32)\n",
    "        )\n",
    "        \n",
    "        for level in [0, 1]:\n",
    "            total_params += tf.size(self.U_p[level], out_type=tf.float32)\n",
    "            total_params += tf.size(self.V_p[level], out_type=tf.float32)\n",
    "        \n",
    "        # Normalize by total parameters\n",
    "        reg = reg / total_params\n",
    "        \n",
    "        return reg\n",
    "        \n",
    "    def compute_total_loss(self, lambda1, lambda2, reg_weight):\n",
    "        L1_total, L1_user, L1_poi, L1_poi_levels = self.compute_L1_loss()\n",
    "        L2_total, L2_levels = self.compute_L2_loss()\n",
    "        L_reg = self.compute_regularization()\n",
    "        \n",
    "        # âœ… CALIBRATE: Compute scales from first 10 epochs\n",
    "        if self.epoch_counter < self.calibration_epochs:\n",
    "            if self.L1_scale is None:\n",
    "                self.L1_scale = float(L1_total.numpy())\n",
    "                self.L2_scale = float(L2_total.numpy())\n",
    "            else:\n",
    "                # Running average\n",
    "                alpha = 0.9\n",
    "                self.L1_scale = alpha * self.L1_scale + (1 - alpha) * float(L1_total.numpy())\n",
    "                self.L2_scale = alpha * self.L2_scale + (1 - alpha) * float(L2_total.numpy())\n",
    "            \n",
    "            self.epoch_counter += 1\n",
    "        \n",
    "        # âœ… NORMALIZE: Scale losses to comparable ranges\n",
    "        L1_normalized = L1_total / max(self.L1_scale, 1e-6)\n",
    "        L2_normalized = L2_total / max(self.L2_scale, 1e-6)\n",
    "        \n",
    "        # âœ… WEIGHT: Now lambda1 and lambda2 have real meaning\n",
    "        total_loss = (lambda1 * L1_normalized) + \\\n",
    "                     (lambda2 * L2_normalized) + \\\n",
    "                     (reg_weight * L_reg)\n",
    "        \n",
    "        # Return un-normalized values for logging\n",
    "        loss_components = {\n",
    "            'total_loss': total_loss,\n",
    "            'L1_total': L1_total,\n",
    "            'L1_user': L1_user,\n",
    "            'L1_poi': L1_poi,\n",
    "            'L1_poi_levels': L1_poi_levels,\n",
    "            'L2_total': L2_total,\n",
    "            'L2_levels': L2_levels,\n",
    "            'regularization': L_reg\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_components\n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        \"\"\"Return list of trainable variables\"\"\"\n",
    "        trainable_vars = [self.U_u, self.V_u]\n",
    "        \n",
    "        for level in range(self.num_levels):\n",
    "            trainable_vars.append(self.U_p[level])\n",
    "            trainable_vars.append(self.V_p[level]) \n",
    "        \n",
    "        return trainable_vars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dac722a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_3472\\3378191801.py:156: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  poi_embeddings = pickle.load(f)\n",
      "C:\\Windows\\Temp\\ipykernel_3472\\3378191801.py:159: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  interactions = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing Attribute Matrices for L1...\n",
      "  User attribute matrix X:\n",
      "    Shape: (21, 71)\n",
      "    Normalized: mean=-0.0000, std=0.9929\n",
      "  POI attribute matrix Y^0:\n",
      "    Shape: (4696, 20)\n",
      "    Normalized: mean=0.0000, std=0.7416\n",
      "  POI attribute matrix Y^1:\n",
      "    Shape: (1355, 20)\n",
      "    Normalized: mean=0.0000, std=0.6708\n",
      "  POI attribute matrix Y^2:\n",
      "    Shape: (44, 20)\n",
      "    Normalized: mean=0.0000, std=0.0000\n",
      "  POI attribute matrix Y^3:\n",
      "    Shape: (5, 20)\n",
      "    Normalized: mean=0.0000, std=0.0000\n",
      "\n",
      "Preparing Interaction Matrices for L2...\n",
      "  Level 0: Generated 257 negative samples (1 per positive)\n",
      "  Level 1: Generated 231 negative samples (1 per positive)\n",
      "  Level 2: Generated 180 negative samples (1 per positive)\n",
      "  Level 3: Generated 71 negative samples (1 per positive)\n",
      "  Level 0: 257 positive, 257 negative samples\n",
      "  Level 1: 231 positive, 231 negative samples\n",
      "  Level 2: 180 positive, 180 negative samples\n",
      "  Level 3: 71 positive, 71 negative samples\n",
      "  V_p^0: shape (20, 16) (feature_dim=20, embed_dim=16)\n",
      "  V_p^1: shape (20, 16) (feature_dim=20, embed_dim=16)\n",
      "  V_p^2: shape (20, 16) (feature_dim=20, embed_dim=16)\n",
      "  V_p^3: shape (20, 16) (feature_dim=20, embed_dim=16)\n",
      "\n",
      "Initialized Parameters:\n",
      "  U_u: (21, 16)\n",
      "  U_p^0: (4696, 16)\n",
      "  U_p^1: (1355, 16)\n",
      "  U_p^2: (44, 16)\n",
      "  U_p^3: (5, 16)\n",
      "  V_u: (71, 16)\n",
      "  V_p^0: (20, 16)\n",
      "  V_p^1: (20, 16)\n",
      "  V_p^2: (20, 16)\n",
      "  V_p^3: (20, 16)\n",
      "\n",
      "================================================================================\n",
      "DETAILED INTERACTION STRUCTURE INSPECTION\n",
      "================================================================================\n",
      "\n",
      "1. Checking interactions['level_0']:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['edges', 'matrices', 'user_to_pois', 'poi_to_users', 'splits', 'stats', 'n_users', 'n_pois']\n",
      "\n",
      "2. Checking edges:\n",
      "   Type: <class 'dict'>\n",
      "   Keys: ['user_ids', 'poi_ids', 'user_indices', 'poi_indices', 'weights', 'timestamps']\n",
      "\n",
      "3. user_indices:\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   Shape/Length: (257,)\n",
      "   First 5 values: [12  9 10 19 12]\n",
      "\n",
      "4. poi_indices:\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   Shape/Length: (257,)\n",
      "   First 5 values: [4614 3443 2671 2777  301]\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STARTING JOINT OPTIMIZATION OVER 4 POI LEVELS\n",
      "================================================================================\n",
      "Hyperparameters:\n",
      "  Î»1 (Attribute loss weight): 1.0\n",
      "  Î»2 (Interaction loss weight): 1.0\n",
      "  Regularization weight: 0.001\n",
      "  Learning rate: 0.0010000000474974513\n",
      "  Epochs: 500\n",
      "\n",
      "\n",
      "Epoch    0:\n",
      "  Total Loss:       2.0000\n",
      "  â”œâ”€ L1 (Attribute):           2.1243\n",
      "  â”‚   â”œâ”€ User:                1.0142\n",
      "  â”‚   â””â”€ POI (total):         1.1101\n",
      "  â”‚       â”œâ”€ Level 0 (individual_poi ):       0.5503\n",
      "  â”‚       â”œâ”€ Level 1 (container_poi  ):       0.4516\n",
      "  â”‚       â”œâ”€ Level 2 (street_poi     ):       0.0255\n",
      "  â”‚       â”œâ”€ Level 3 (district_poi   ):       0.0827\n",
      "  â”œâ”€ L2 (Interaction):       514.9905\n",
      "  â”‚   â”œâ”€ Level 0 (individual_poi ):     177.9804\n",
      "  â”‚   â”œâ”€ Level 1 (container_poi  ):     160.1872\n",
      "  â”‚   â”œâ”€ Level 2 (street_poi     ):     124.4309\n",
      "  â”‚   â”œâ”€ Level 3 (district_poi   ):      52.3920\n",
      "  â””â”€ Regularization:           0.0015\n",
      "\n",
      "Epoch   10:\n",
      "  Total Loss:       1.9644\n",
      "  â”œâ”€ L1 (Attribute):           2.0777\n",
      "  â”‚   â”œâ”€ User:                0.9983\n",
      "  â”‚   â””â”€ POI (total):         1.0794\n",
      "  â”‚       â”œâ”€ Level 0 (individual_poi ):       0.5435\n",
      "  â”‚       â”œâ”€ Level 1 (container_poi  ):       0.4459\n",
      "  â”‚       â”œâ”€ Level 2 (street_poi     ):       0.0227\n",
      "  â”‚       â”œâ”€ Level 3 (district_poi   ):       0.0673\n",
      "  â”œâ”€ L2 (Interaction):       498.3573\n",
      "  â”‚   â”œâ”€ Level 0 (individual_poi ):     170.6173\n",
      "  â”‚   â”œâ”€ Level 1 (container_poi  ):     154.4419\n",
      "  â”‚   â”œâ”€ Level 2 (street_poi     ):     122.1380\n",
      "  â”‚   â”œâ”€ Level 3 (district_poi   ):      51.1601\n",
      "  â””â”€ Regularization:           0.0015\n",
      "\n",
      "Epoch   20:\n",
      "  Total Loss:       1.9109\n",
      "  â”œâ”€ L1 (Attribute):           2.0332\n",
      "  â”‚   â”œâ”€ User:                0.9831\n",
      "  â”‚   â””â”€ POI (total):         1.0501\n",
      "  â”‚       â”œâ”€ Level 0 (individual_poi ):       0.5354\n",
      "  â”‚       â”œâ”€ Level 1 (container_poi  ):       0.4396\n",
      "  â”‚       â”œâ”€ Level 2 (street_poi     ):       0.0204\n",
      "  â”‚       â”œâ”€ Level 3 (district_poi   ):       0.0547\n",
      "  â”œâ”€ L2 (Interaction):       481.8575\n",
      "  â”‚   â”œâ”€ Level 0 (individual_poi ):     163.2884\n",
      "  â”‚   â”œâ”€ Level 1 (container_poi  ):     148.7153\n",
      "  â”‚   â”œâ”€ Level 2 (street_poi     ):     119.8767\n",
      "  â”‚   â”œâ”€ Level 3 (district_poi   ):      49.9771\n",
      "  â””â”€ Regularization:           0.0017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 213\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   First 5 values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00medges[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoi_indices\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    220\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[0;32m    223\u001b[0m trainer\u001b[38;5;241m.\u001b[39mplot_training_history()\n",
      "Cell \u001b[1;32mIn[56], line 39\u001b[0m, in \u001b[0;36mMultiLevelTrainer.train\u001b[1;34m(self, num_epochs, lambda1, lambda2, reg_weight, verbose, log_interval)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, trainable_vars))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Log history\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[56], line 53\u001b[0m, in \u001b[0;36mMultiLevelTrainer._log_history\u001b[1;34m(self, loss_components)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Log training history - HANDLE DICTIONARIES PROPERLY\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# âœ… Convert tensor scalars to Python floats\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[43mloss_components\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1_user\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1_user\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\brian\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MultiLevelTrainer:\n",
    "    def __init__(self, loss_computer, learning_rate=0.001, clipnorm=1.0):\n",
    "        self.loss_computer = loss_computer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def train(self, num_epochs=100, lambda1=0.5, lambda2=0.5, reg_weight=0.01,\n",
    "            verbose=True, log_interval=10):\n",
    "        \"\"\"\n",
    "        Train the model with joint optimization across all POI levels\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"STARTING JOINT OPTIMIZATION OVER {self.loss_computer.num_levels} POI LEVELS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Hyperparameters:\")\n",
    "        print(f\"  Î»1 (Attribute loss weight): {lambda1}\")\n",
    "        print(f\"  Î»2 (Interaction loss weight): {lambda2}\")\n",
    "        print(f\"  Regularization weight: {reg_weight}\")\n",
    "        print(f\"  Learning rate: {self.optimizer.learning_rate.numpy()}\")\n",
    "        print(f\"  Epochs: {num_epochs}\\n\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss, loss_components = self.loss_computer.compute_total_loss(\n",
    "                    lambda1, lambda2, reg_weight\n",
    "                )\n",
    "            \n",
    "            # Compute gradients\n",
    "            trainable_vars = self.loss_computer.get_trainable_variables()\n",
    "            gradients = tape.gradient(total_loss, trainable_vars)\n",
    "\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n",
    "            \n",
    "            # Apply gradients\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "            # Log history\n",
    "            self._log_history(loss_components)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % log_interval == 0 or epoch == num_epochs - 1):\n",
    "                self._print_progress(epoch, loss_components)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "    def _log_history(self, loss_components):\n",
    "        \"\"\"Log training history - HANDLE DICTIONARIES PROPERLY\"\"\"\n",
    "        \n",
    "        # âœ… Convert tensor scalars to Python floats\n",
    "        self.history['total_loss'].append(float(loss_components['total_loss'].numpy()))\n",
    "        self.history['L1_total'].append(float(loss_components['L1_total'].numpy()))\n",
    "        self.history['L1_user'].append(float(loss_components['L1_user'].numpy()))\n",
    "        self.history['L1_poi'].append(float(loss_components['L1_poi'].numpy()))\n",
    "        self.history['L2_total'].append(float(loss_components['L2_total'].numpy()))\n",
    "        self.history['regularization'].append(float(loss_components['regularization'].numpy()))\n",
    "        \n",
    "        # âœ… Initialize nested dictionaries if needed\n",
    "        if 'L1_poi_levels' not in self.history:\n",
    "            self.history['L1_poi_levels'] = {\n",
    "                level: [] for level in range(self.loss_computer.num_levels)\n",
    "            }\n",
    "        \n",
    "        if 'L2_levels' not in self.history:\n",
    "            self.history['L2_levels'] = {\n",
    "                level: [] for level in range(self.loss_computer.num_levels)\n",
    "            }\n",
    "        \n",
    "        # âœ… Extract values from dictionaries\n",
    "        for level, loss_tensor in loss_components['L1_poi_levels'].items():\n",
    "            self.history['L1_poi_levels'][level].append(float(loss_tensor.numpy()))\n",
    "        \n",
    "        for level, loss_tensor in loss_components['L2_levels'].items():\n",
    "            self.history['L2_levels'][level].append(float(loss_tensor.numpy()))\n",
    "    \n",
    "    def _print_progress(self, epoch, loss_components):\n",
    "        \"\"\"Print training progress\"\"\"\n",
    "        print(f\"\\nEpoch {epoch:4d}:\")\n",
    "        print(f\"  Total Loss: {loss_components['total_loss'].numpy():12.4f}\")  # âœ… Changed 'total' to 'total_loss'\n",
    "        print(f\"  â”œâ”€ L1 (Attribute):     {loss_components['L1_total'].numpy():12.4f}\")\n",
    "        print(f\"  â”‚   â”œâ”€ User:          {loss_components['L1_user'].numpy():12.4f}\")\n",
    "        print(f\"  â”‚   â””â”€ POI (total):   {loss_components['L1_poi'].numpy():12.4f}\")\n",
    "        \n",
    "        # Print L1 POI losses per level\n",
    "        for level in range(self.loss_computer.num_levels):\n",
    "            level_name = self.loss_computer.data_prep.poi_loader.level_names[level]\n",
    "            if level in loss_components['L1_poi_levels']:\n",
    "                loss_val = loss_components['L1_poi_levels'][level].numpy()\n",
    "                print(f\"  â”‚       â”œâ”€ Level {level} ({level_name:15s}): {loss_val:12.4f}\")\n",
    "        \n",
    "        print(f\"  â”œâ”€ L2 (Interaction):   {loss_components['L2_total'].numpy():12.4f}\")\n",
    "        \n",
    "        # Print L2 losses per level\n",
    "        for level in range(self.loss_computer.num_levels):\n",
    "            level_name = self.loss_computer.data_prep.poi_loader.level_names[level]\n",
    "            if level in loss_components['L2_levels']:\n",
    "                loss_val = loss_components['L2_levels'][level].numpy()\n",
    "                print(f\"  â”‚   â”œâ”€ Level {level} ({level_name:15s}): {loss_val:12.4f}\")\n",
    "        \n",
    "        print(f\"  â””â”€ Regularization:     {loss_components['regularization'].numpy():12.4f}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Total loss\n",
    "        axes[0, 0].plot(self.history['total_loss'])\n",
    "        axes[0, 0].set_title('Total Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # L1 components\n",
    "        axes[0, 1].plot(self.history['L1_total'], label='L1 Total')\n",
    "        axes[0, 1].plot(self.history['L1_user'], label='L1 User', alpha=0.7)\n",
    "        axes[0, 1].plot(self.history['L1_poi'], label='L1 POI', alpha=0.7)\n",
    "        axes[0, 1].set_title('L1 Loss Components')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # L2 per level - âœ… FIXED: Access nested dictionary correctly\n",
    "        axes[1, 0].plot(self.history['L2_total'], label='L2 Total', linewidth=2)\n",
    "        for level in range(self.loss_computer.num_levels):\n",
    "            level_name = self.loss_computer.data_prep.poi_loader.level_names[level]\n",
    "            # âœ… Changed from self.history[f'L2_level_{level}'] to self.history['L2_levels'][level]\n",
    "            if level in self.history['L2_levels']:\n",
    "                axes[1, 0].plot(self.history['L2_levels'][level], \n",
    "                            label=f'Level {level} ({level_name})', alpha=0.7)\n",
    "        axes[1, 0].set_title('L2 Loss Per Level')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Regularization\n",
    "        axes[1, 1].plot(self.history['regularization'])\n",
    "        axes[1, 1].set_title('Regularization Loss')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "with open('../Sources/Embeddings/user_embeddings.pkl', 'rb') as f:\n",
    "    user_embeddings = pickle.load(f)\n",
    "\n",
    "with open('../Sources/Embeddings/poi_embeddings.pkl', 'rb') as f:\n",
    "    poi_embeddings = pickle.load(f)\n",
    "\n",
    "with open('../Sources/Embeddings/interactions.pkl', 'rb') as f:\n",
    "    interactions = pickle.load(f)\n",
    "\n",
    "with open('../Sources/Embeddings/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "data_prep = MultiLevelDataPreparation(\n",
    "    poi_loader,\n",
    "    user_embeddings,\n",
    "    poi_embeddings,\n",
    "    interactions,\n",
    "    metadata\n",
    ")\n",
    "\n",
    "# Initialize loss computation\n",
    "loss_computer = JointLossComputation(data_prep, embedding_dim=16)\n",
    "\n",
    "# loss_computer.validate_tensors()\n",
    "\n",
    "# Initialize trainer and run training\n",
    "trainer = MultiLevelTrainer(loss_computer, learning_rate=0.001)\n",
    "\n",
    "# Debug inspection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED INTERACTION STRUCTURE INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "level_key = 'level_0'\n",
    "\n",
    "print(f\"\\n1. Checking interactions['{level_key}']:\")\n",
    "if level_key in interactions['interactions']:\n",
    "    level_data = interactions['interactions'][level_key]\n",
    "    print(f\"   Type: {type(level_data)}\")\n",
    "    print(f\"   Keys: {list(level_data.keys()) if isinstance(level_data, dict) else 'Not a dict'}\")\n",
    "    \n",
    "    if 'edges' in level_data:\n",
    "        edges = level_data['edges']\n",
    "        print(f\"\\n2. Checking edges:\")\n",
    "        print(f\"   Type: {type(edges)}\")\n",
    "        print(f\"   Keys: {list(edges.keys())}\")\n",
    "        \n",
    "        if 'user_indices' in edges:\n",
    "            print(f\"\\n3. user_indices:\")\n",
    "            print(f\"   Type: {type(edges['user_indices'])}\")\n",
    "            print(f\"   Shape/Length: {edges['user_indices'].shape if hasattr(edges['user_indices'], 'shape') else len(edges['user_indices'])}\")\n",
    "            print(f\"   First 5 values: {edges['user_indices'][:5]}\")\n",
    "        \n",
    "        if 'poi_indices' in edges:\n",
    "            print(f\"\\n4. poi_indices:\")\n",
    "            print(f\"   Type: {type(edges['poi_indices'])}\")\n",
    "            print(f\"   Shape/Length: {edges['poi_indices'].shape if hasattr(edges['poi_indices'], 'shape') else len(edges['poi_indices'])}\")\n",
    "            print(f\"   First 5 values: {edges['poi_indices'][:5]}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train(\n",
    "    num_epochs=500,\n",
    "    lambda1=1.0,\n",
    "    lambda2=1.0,\n",
    "    reg_weight=0.001,\n",
    "    verbose=True,\n",
    "    log_interval=10\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b50f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
