{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebbe312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "  Users: 21, attributes: 71\n",
      "  Level 0: 4696 POIs, 442 attributes\n",
      "  Level 1: 1355 POIs, 342 attributes\n",
      "  Level 2: 44 POIs, 250 attributes\n",
      "  Level 3: 5 POIs, 210 attributes\n",
      "\n",
      "Building R^l from CSV...\n",
      "  CSV rows: 567, Valid rows: 567\n",
      "  Unique users in CSV: 21\n",
      "  Users mapped: 21/21\n",
      "  Level 0: No interactions\n",
      "  Level 1: No interactions\n",
      "  Level 2: No interactions\n",
      "  Level 3: No interactions\n",
      "  S^0: shape torch.Size([21, 4696])\n",
      "  S^1: shape torch.Size([21, 1355])\n",
      "  S^2: shape torch.Size([21, 44])\n",
      "  S^3: shape torch.Size([21, 5])\n",
      "\n",
      "Training: k=64, α=0.3, β=0.2, γ=1.0\n",
      "Device: cpu\n",
      "Epoch   1 | Total: 298813.16 | L1: 293260.59 | L2: 0.0000 | L3: 27762.76\n",
      "Epoch  20 | Total: 271485.09 | L1: 266209.50 | L2: 0.0000 | L3: 26378.03\n",
      "Epoch  40 | Total: 209999.73 | L1: 205365.41 | L2: 0.0000 | L3: 23171.62\n",
      "Epoch  60 | Total: 132633.59 | L1: 128838.66 | L2: 0.0000 | L3: 18974.64\n",
      "Epoch  80 | Total: 72768.44 | L1: 69875.20 | L2: 0.0000 | L3: 14466.23\n",
      "Epoch 100 | Total: 38359.95 | L1: 36212.89 | L2: 0.0000 | L3: 10735.32\n",
      "Epoch 120 | Total: 19503.27 | L1: 17927.78 | L2: 0.0000 | L3: 7877.44\n",
      "Epoch 140 | Total: 8161.54 | L1: 7093.53 | L2: 0.0000 | L3: 5340.04\n",
      "Epoch 160 | Total: 2897.03 | L1: 2243.02 | L2: 0.0000 | L3: 3270.05\n",
      "Epoch 180 | Total: 1051.16 | L1: 690.18 | L2: 0.0000 | L3: 1804.91\n",
      "Epoch 200 | Total: 611.07 | L1: 420.92 | L2: 0.0000 | L3: 950.73\n",
      "\n",
      "Saved to ../Sources/Embeddings v3/joint_optimized_final.pkl\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "L1 (Attribute): 420.92\n",
      "L2 (BPR):       0.0000\n",
      "L3 (S-align):   950.73\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class JointMultiLevelPOIModel(nn.Module):\n",
    "    def __init__(self, n_users, n_pois_per_level, user_attr_dim, poi_attr_dims, \n",
    "                 latent_dim=64, use_s_matrix=True):\n",
    "        super().__init__()\n",
    "        self.use_s_matrix = use_s_matrix\n",
    "        self.n_levels = len(n_pois_per_level)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.U_u = nn.Parameter(torch.randn(n_users, latent_dim) * 0.01)\n",
    "        self.U_p_levels = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(n_pois, latent_dim) * 0.01)\n",
    "            for n_pois in n_pois_per_level\n",
    "        ])\n",
    "        self.V_u = nn.Parameter(torch.randn(user_attr_dim, latent_dim) * 0.01)\n",
    "        self.V_p_levels = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(dims, latent_dim) * 0.01)\n",
    "            for dims in poi_attr_dims\n",
    "        ])\n",
    "        \n",
    "    def compute_L1(self, X, Y_list, reg_lambda=0.01):\n",
    "        \"\"\"Attribute reconstruction\"\"\"\n",
    "        X_pred = self.U_u @ self.V_u.t()\n",
    "        loss_user = torch.norm(X_pred - X, p='fro') ** 2\n",
    "        \n",
    "        loss_poi = torch.tensor(0.0, device=X.device)\n",
    "        for l, Y in enumerate(Y_list):\n",
    "            if Y is not None:\n",
    "                Y_pred = self.U_p_levels[l] @ self.V_p_levels[l].t()\n",
    "                loss_poi = loss_poi + torch.norm(Y_pred - Y, p='fro') ** 2\n",
    "        \n",
    "        reg_loss = reg_lambda * (torch.norm(self.V_u) ** 2 + \n",
    "                                  sum(torch.norm(V) ** 2 for V in self.V_p_levels))\n",
    "        \n",
    "        return loss_user + loss_poi + reg_loss\n",
    "    \n",
    "    def compute_L2(self, R_list, num_negatives=5):\n",
    "        \"\"\"BPR ranking - CRITICAL FIX: Always return tensor\"\"\"\n",
    "        if R_list is None:\n",
    "            return torch.tensor(0.0, device=self.U_u.device)\n",
    "            \n",
    "        total_loss = torch.tensor(0.0, device=self.U_u.device)\n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for l, R in enumerate(R_list):\n",
    "            if R is None or R.nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            rows, cols = R.nonzero()\n",
    "            if len(rows) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Sample subset if too many interactions (for efficiency)\n",
    "            if len(rows) > 5000:\n",
    "                idx = np.random.choice(len(rows), 5000, replace=False)\n",
    "                rows, cols = rows[idx], cols[idx]\n",
    "            \n",
    "            users = torch.from_numpy(rows).long().to(self.U_u.device)\n",
    "            pos_pois = torch.from_numpy(cols).long().to(self.U_u.device)\n",
    "            \n",
    "            u_factors = self.U_u[users]\n",
    "            p_pos = self.U_p_levels[l][pos_pois]\n",
    "            scores_pos = torch.sum(u_factors * p_pos, dim=1)\n",
    "            \n",
    "            n_pos = len(users)\n",
    "            n_pois_l = self.U_p_levels[l].shape[0]\n",
    "            \n",
    "            for _ in range(num_negatives):\n",
    "                neg_pois = torch.randint(0, n_pois_l, (n_pos,), device=self.U_u.device)\n",
    "                # Ensure negatives are actually negative (not in R)\n",
    "                p_neg = self.U_p_levels[l][neg_pois]\n",
    "                scores_neg = torch.sum(u_factors * p_neg, dim=1)\n",
    "                \n",
    "                diff = scores_pos - scores_neg\n",
    "                total_loss = total_loss + torch.nn.functional.softplus(-diff).mean()\n",
    "                valid_pairs += 1\n",
    "        \n",
    "        if valid_pairs == 0:\n",
    "            return torch.tensor(0.0, device=self.U_u.device)\n",
    "            \n",
    "        return total_loss / valid_pairs\n",
    "    \n",
    "    def compute_L3(self, S_list, reg_lambda=0.01):\n",
    "        \"\"\"Feature alignment - CRITICAL FIX: Always return tensor\"\"\"\n",
    "        if not self.use_s_matrix or S_list is None:\n",
    "            return torch.tensor(0.0, device=self.U_u.device)\n",
    "        \n",
    "        loss = torch.tensor(0.0, device=self.U_u.device)\n",
    "        count = 0\n",
    "        \n",
    "        for l, S in enumerate(S_list):\n",
    "            if S is not None:\n",
    "                O_pred = self.U_u @ self.U_p_levels[l].t()\n",
    "                # Ensure S is on same device\n",
    "                if S.device != O_pred.device:\n",
    "                    S = S.to(O_pred.device)\n",
    "                loss = loss + torch.norm(O_pred - S, p='fro') ** 2\n",
    "                count += 1\n",
    "        \n",
    "        if count == 0:\n",
    "            return torch.tensor(0.0, device=self.U_u.device)\n",
    "            \n",
    "        reg_loss = reg_lambda * (torch.norm(self.U_u) ** 2 + \n",
    "                                  sum(torch.norm(U) ** 2 for U in self.U_p_levels))\n",
    "        \n",
    "        return loss / count + reg_loss\n",
    "    \n",
    "    def forward(self, X, Y_list, R_list=None, S_list=None, alpha=0.5, beta=0.1, gamma=1.0):\n",
    "        L1 = self.compute_L1(X, Y_list)\n",
    "        L2 = self.compute_L2(R_list)\n",
    "        L3 = self.compute_L3(S_list)\n",
    "        \n",
    "        # CRITICAL: Ensure all are tensors\n",
    "        device = X.device\n",
    "        if not isinstance(L1, torch.Tensor):\n",
    "            L1 = torch.tensor(float(L1), device=device)\n",
    "        if not isinstance(L2, torch.Tensor):\n",
    "            L2 = torch.tensor(float(L2), device=device)\n",
    "        if not isinstance(L3, torch.Tensor):\n",
    "            L3 = torch.tensor(float(L3), device=device)\n",
    "        \n",
    "        total = gamma * L1 + alpha * L2 + beta * L3\n",
    "        return total, L1, L2, L3\n",
    "\n",
    "\n",
    "def load_joint_data(paths):\n",
    "    \"\"\"Load all data with CRITICAL FIXES for S_matrix and R_matrix\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # 1. Load user embeddings\n",
    "    with open(paths['user_emb'], 'rb') as f:\n",
    "        user_data = pickle.load(f)\n",
    "        X_A = user_data['X_A']\n",
    "        X_T = user_data['X_T']\n",
    "        X = np.concatenate([X_A, X_T], axis=1)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        n_users = X.shape[0]\n",
    "        user_ids = user_data.get('user_ids', [f\"user_{i}\" for i in range(n_users)])\n",
    "        print(f\"  Users: {n_users}, attributes: {X.shape[1]}\")\n",
    "    \n",
    "    # 2. Load POI embeddings\n",
    "    with open(paths['poi_emb'], 'rb') as f:\n",
    "        poi_data = pickle.load(f)\n",
    "    \n",
    "    Y_list = []\n",
    "    n_pois_list = []\n",
    "    poi_attr_dims = []\n",
    "    \n",
    "    for level in range(4):\n",
    "        level_key = f'level_{level}'\n",
    "        level_emb = poi_data['poi_embeddings'][level_key]\n",
    "        \n",
    "        Y_A = level_emb['Y_A']\n",
    "        Y_T = level_emb['Y_T']\n",
    "        \n",
    "        # Check for A_lp in different possible locations\n",
    "        A_lp = None\n",
    "        if 'A_lp' in poi_data:\n",
    "            if level_key in poi_data['A_lp']:\n",
    "                A_lp = poi_data['A_lp'][level_key]\n",
    "            elif isinstance(poi_data['A_lp'], dict) and level in poi_data['A_lp']:\n",
    "                A_lp = poi_data['A_lp'][level]\n",
    "        \n",
    "        if A_lp is None:\n",
    "            # Use zeros with appropriate dimension\n",
    "            target_dims = {0: 221, 1: 171, 2: 125, 3: 105}\n",
    "            A_lp = np.zeros((Y_A.shape[0], target_dims[level]))\n",
    "            \n",
    "        Y = np.concatenate([Y_A, Y_T, A_lp], axis=1)\n",
    "        Y_list.append(torch.from_numpy(Y).float())\n",
    "        n_pois_list.append(Y.shape[0])\n",
    "        poi_attr_dims.append(Y.shape[1])\n",
    "        print(f\"  Level {level}: {Y.shape[0]} POIs, {Y.shape[1]} attributes\")\n",
    "    \n",
    "    # 3. Build R^l from CSV - CRITICAL SECTION\n",
    "    print(\"\\nBuilding R^l from CSV...\")\n",
    "    df = pd.read_csv(paths['csv'])\n",
    "    \n",
    "    with open(paths['poi_tree'], 'r') as f:\n",
    "        poi_tree = json.load(f)\n",
    "    \n",
    "    # Build poi_key -> uuid mapping\n",
    "    poi_key_to_uuid = {}\n",
    "    uuid_to_level = {}\n",
    "    for level_key, level_data in poi_tree.items():\n",
    "        if level_key.startswith('level_'):\n",
    "            l = int(level_key.split('_')[1])\n",
    "            for pkey, pinfo in level_data.items():\n",
    "                if 'uuid' in pinfo:\n",
    "                    poi_key_to_uuid[pkey] = pinfo['uuid']\n",
    "                    uuid_to_level[pinfo['uuid']] = l\n",
    "    \n",
    "    # Map user IDs\n",
    "    user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
    "    df['user_idx'] = df['user_id'].map(user_to_idx)\n",
    "    \n",
    "    # Map POI IDs\n",
    "    df['poi_uuid'] = df['poi_id'].map(lambda x: poi_key_to_uuid.get(x))\n",
    "    df['level'] = df['poi_uuid'].map(uuid_to_level)\n",
    "    \n",
    "    # Filter valid\n",
    "    valid_df = df[df['user_idx'].notna() & df['poi_uuid'].notna()].copy()\n",
    "    valid_df['user_idx'] = valid_df['user_idx'].astype(int)\n",
    "    \n",
    "    print(f\"  CSV rows: {len(df)}, Valid rows: {len(valid_df)}\")\n",
    "    print(f\"  Unique users in CSV: {df['user_id'].nunique()}\")\n",
    "    print(f\"  Users mapped: {valid_df['user_idx'].nunique()}/{n_users}\")\n",
    "    \n",
    "    # Build R matrices\n",
    "    R_list = []\n",
    "    for level in range(4):\n",
    "        level_df = valid_df[valid_df['level'] == level]\n",
    "        level_key = f'level_{level}'\n",
    "        poi_ids = poi_data['poi_embeddings'][level_key]['poi_ids']\n",
    "        poi_to_idx = {pid: i for i, pid in enumerate(poi_ids)}\n",
    "        \n",
    "        # Filter to POIs in this level\n",
    "        level_df = level_df[level_df['poi_uuid'].isin(poi_to_idx)]\n",
    "        \n",
    "        if len(level_df) == 0:\n",
    "            R_list.append(None)\n",
    "            print(f\"  Level {level}: No interactions\")\n",
    "            continue\n",
    "        \n",
    "        # Aggregate interactions\n",
    "        agg = level_df.groupby(['user_idx', 'poi_uuid']).size().reset_index(name='count')\n",
    "        agg['poi_idx'] = agg['poi_uuid'].map(poi_to_idx)\n",
    "        \n",
    "        rows = agg['user_idx'].values.astype(int)\n",
    "        cols = agg['poi_idx'].values.astype(int)\n",
    "        data = agg['count'].values.astype(float)\n",
    "        \n",
    "        R = csr_matrix((data, (rows, cols)), shape=(n_users, len(poi_ids)))\n",
    "        R_list.append(R)\n",
    "        print(f\"  Level {level}: R{R.shape}, {R.nnz} interactions\")\n",
    "    \n",
    "    # 4. Load S_matrices - CRITICAL FIX FOR TYPE HANDLING\n",
    "    S_list = None\n",
    "    if 's_matrix' in paths:\n",
    "        try:\n",
    "            with open(paths['s_matrix'], 'rb') as f:\n",
    "                S_data = pickle.load(f)\n",
    "            \n",
    "            S_list = []\n",
    "            for level in range(4):\n",
    "                level_key = f'level_{level}'\n",
    "                S_raw = S_data.get('S_matrices', {}).get(level_key)\n",
    "                \n",
    "                if S_raw is not None:\n",
    "                    # Handle both numpy and torch tensors\n",
    "                    if isinstance(S_raw, torch.Tensor):\n",
    "                        S_tensor = S_raw.float()\n",
    "                    elif isinstance(S_raw, np.ndarray):\n",
    "                        S_tensor = torch.from_numpy(S_raw).float()\n",
    "                    else:\n",
    "                        print(f\"  Level {level}: Unknown type {type(S_raw)}\")\n",
    "                        S_tensor = None\n",
    "                    \n",
    "                    if S_tensor is not None:\n",
    "                        S_list.append(S_tensor)\n",
    "                        print(f\"  S^{level}: shape {S_tensor.shape}\")\n",
    "                else:\n",
    "                    S_list.append(None)\n",
    "            \n",
    "            if not any(s is not None for s in S_list):\n",
    "                S_list = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not load S matrices: {e}\")\n",
    "            S_list = None\n",
    "    \n",
    "    data_dict = {\n",
    "        'X': X, 'Y_list': Y_list, 'R_list': R_list, 'S_list': S_list,\n",
    "        'n_users': n_users, 'n_pois_list': n_pois_list,\n",
    "        'poi_attr_dims': poi_attr_dims, 'user_ids': user_ids\n",
    "    }\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def run_joint_optimization(paths, latent_dim=64, alpha=0.3, beta=0.2, \n",
    "                          gamma=1.0, lr=0.005, epochs=150, device='cpu'):\n",
    "    \"\"\"Run joint optimization\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    data = load_joint_data(paths)\n",
    "    \n",
    "    # Check if we have any training signal\n",
    "    has_R = any(r is not None and r.nnz > 0 for r in data['R_list']) if data['R_list'] else False\n",
    "    has_S = data.get('S_list') is not None\n",
    "    \n",
    "    if not has_R and not has_S:\n",
    "        print(\"\\nWARNING: No interaction data (R^l) or feature matrices (S^l) found!\")\n",
    "        print(\"Training will only reconstruct attributes (L1 only)\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = JointMultiLevelPOIModel(\n",
    "        n_users=data['n_users'],\n",
    "        n_pois_per_level=data['n_pois_list'],\n",
    "        user_attr_dim=data['X'].shape[1],\n",
    "        poi_attr_dims=data['poi_attr_dims'],\n",
    "        latent_dim=latent_dim,\n",
    "        use_s_matrix=has_S\n",
    "    ).to(device)\n",
    "    \n",
    "    # Move data to device\n",
    "    X = data['X'].to(device)\n",
    "    Y_list = [y.to(device) for y in data['Y_list']]\n",
    "    R_list = data['R_list']  # Keep as scipy sparse\n",
    "    S_list = None\n",
    "    if has_S:\n",
    "        S_list = [s.to(device) if s is not None else None for s in data['S_list']]\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'total': [], 'L1': [], 'L2': [], 'L3': []}\n",
    "    print(f\"\\nTraining: k={latent_dim}, α={alpha}, β={beta}, γ={gamma}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss, L1, L2, L3 = model(\n",
    "            X, Y_list, R_list, S_list,\n",
    "            alpha=alpha, beta=beta, gamma=gamma\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record\n",
    "        history['total'].append(total_loss.item())\n",
    "        history['L1'].append(L1.item())\n",
    "        history['L2'].append(L2.item())\n",
    "        history['L3'].append(L3.item())\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | \"\n",
    "                  f\"Total: {total_loss.item():.2f} | \"\n",
    "                  f\"L1: {L1.item():.2f} | \"\n",
    "                  f\"L2: {L2.item():.4f} | \"\n",
    "                  f\"L3: {L3.item():.2f}\")\n",
    "    \n",
    "    # Extract results\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        results = {\n",
    "            'U_u': model.U_u.cpu().numpy(),\n",
    "            'U_p_levels': [up.cpu().numpy() for up in model.U_p_levels],\n",
    "            'V_u': model.V_u.cpu().numpy(),\n",
    "            'V_p_levels': [vp.cpu().numpy() for vp in model.V_p_levels],\n",
    "            'history': history,\n",
    "            'config': {'latent_dim': latent_dim, 'alpha': alpha, \n",
    "                      'beta': beta, 'gamma': gamma}\n",
    "        }\n",
    "    \n",
    "    if 'output' in paths:\n",
    "        with open(paths['output'], 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"\\nSaved to {paths['output']}\")\n",
    "    \n",
    "    return model, results, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = {\n",
    "        'csv': '../Sources/Files/user_poi_interactions.csv',\n",
    "        'poi_tree': '../Sources/Files/poi_tree_with_uuids.json',\n",
    "        'user_emb': '../Sources/Embeddings v3/user_embeddings.pkl',\n",
    "        'poi_emb': '../Sources/Embeddings v3/poi_embeddings.pkl',\n",
    "        's_matrix': '../Sources/Embeddings v3/S_matrices_feature.pkl',\n",
    "        'output': '../Sources/Embeddings v3/joint_optimized_final.pkl'\n",
    "    }\n",
    "    \n",
    "    config = {\n",
    "        'latent_dim': 64,\n",
    "        'alpha': 0.3,      # L2 weight\n",
    "        'beta': 0.2,       # L3 weight\n",
    "        'gamma': 1.0,      # L1 weight\n",
    "        'lr': 0.005,\n",
    "        'epochs': 200,\n",
    "        'device': 'cpu'\n",
    "    }\n",
    "    \n",
    "    model, results, history = run_joint_optimization(paths, **config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"L1 (Attribute): {history['L1'][-1]:.2f}\")\n",
    "    print(f\"L2 (BPR):       {history['L2'][-1]:.4f}\")\n",
    "    print(f\"L3 (S-align):   {history['L3'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ecaea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
