{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90576fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c332736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeBasedRepresentationLearning:\n",
    "    def __init__(self, \n",
    "                 users_file: str,\n",
    "                 interactions_file: str,\n",
    "                 poi_tree_file: str):\n",
    "        \"\"\"\n",
    "        Initialize representation learning\n",
    "        \n",
    "        Args:\n",
    "            users_file: Path to user preferences CSV\n",
    "            interactions_file: Path to user-POI interactions CSV\n",
    "            poi_tree_file: Path to POI tree JSON\n",
    "        \"\"\"\n",
    "        self.users_df = pd.read_csv(users_file)\n",
    "        self.interactions_df = pd.read_csv(interactions_file)\n",
    "        \n",
    "        with open(poi_tree_file, 'r') as f:\n",
    "            self.poi_tree = json.load(f)\n",
    "        \n",
    "        self.user_embeddings = {}\n",
    "        self.poi_embeddings = {}\n",
    "        \n",
    "    # ========================================================================\n",
    "    # USER REPRESENTATION LEARNING\n",
    "    # ========================================================================\n",
    "    \n",
    "    def build_X_A(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build direct user attribute matrix X_A\n",
    "        \n",
    "        Features from user profiles:\n",
    "        - age_group (one-hot)\n",
    "        - area_of_residence (one-hot or lat/lon)\n",
    "        - interests (multi-hot)\n",
    "        - transportation_mode (multi-hot)\n",
    "        - price_sensitivity (one-hot)\n",
    "        \n",
    "        Returns:\n",
    "            X_A: (num_users, num_features) matrix\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Building X_A: Direct User Attribute Matrix\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        features_list = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # 1. Age group (one-hot encoding)\n",
    "        age_encoder = LabelEncoder()\n",
    "        age_encoded = age_encoder.fit_transform(self.users_df['age_group'])\n",
    "        age_onehot = np.eye(len(age_encoder.classes_))[age_encoded]\n",
    "        features_list.append(age_onehot)\n",
    "        feature_names.extend([f'age_{cls}' for cls in age_encoder.classes_])\n",
    "        print(f\"Added age_group features: {len(age_encoder.classes_)} dimensions\")\n",
    "        \n",
    "        # 2. Area of residence (one-hot encoding)\n",
    "        area_encoder = LabelEncoder()\n",
    "        area_encoded = area_encoder.fit_transform(self.users_df['area_of_residence'])\n",
    "        area_onehot = np.eye(len(area_encoder.classes_))[area_encoded]\n",
    "        features_list.append(area_onehot)\n",
    "        feature_names.extend([f'area_{cls}' for cls in area_encoder.classes_])\n",
    "        print(f\"Added area_of_residence features: {len(area_encoder.classes_)} dimensions\")\n",
    "        \n",
    "        # 3. Interests (multi-hot encoding)\n",
    "        interests_list = [\n",
    "            [interest.strip() for interest in row.split(';')] \n",
    "            for row in self.users_df['interests']\n",
    "        ]\n",
    "        mlb_interests = MultiLabelBinarizer()\n",
    "        interests_onehot = mlb_interests.fit_transform(interests_list)\n",
    "        features_list.append(interests_onehot)\n",
    "        feature_names.extend([f'interest_{cls}' for cls in mlb_interests.classes_])\n",
    "        print(f\"Added interests features: {len(mlb_interests.classes_)} dimensions\")\n",
    "        \n",
    "        # 4. Transportation modes (multi-hot encoding)\n",
    "        transport_list = [\n",
    "            [mode.strip() for mode in row.split(';')] \n",
    "            for row in self.users_df['transportation_modes']\n",
    "        ]\n",
    "        mlb_transport = MultiLabelBinarizer()\n",
    "        transport_onehot = mlb_transport.fit_transform(transport_list)\n",
    "        features_list.append(transport_onehot)\n",
    "        feature_names.extend([f'transport_{cls}' for cls in mlb_transport.classes_])\n",
    "        print(f\"Added transportation_modes features: {len(mlb_transport.classes_)} dimensions\")\n",
    "        \n",
    "        # 5. Price sensitivity (one-hot encoding)\n",
    "        price_encoder = LabelEncoder()\n",
    "        price_encoded = price_encoder.fit_transform(self.users_df['price_sensitivity'])\n",
    "        price_onehot = np.eye(len(price_encoder.classes_))[price_encoded]\n",
    "        features_list.append(price_onehot)\n",
    "        feature_names.extend([f'price_{cls}' for cls in price_encoder.classes_])\n",
    "        print(f\"Added price_sensitivity features: {len(price_encoder.classes_)} dimensions\")\n",
    "        \n",
    "        # Concatenate all features\n",
    "        X_A = np.hstack(features_list)\n",
    "        \n",
    "        print(f\"\\nX_A shape: {X_A.shape}\")\n",
    "        print(f\"Total features: {len(feature_names)}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        self.X_A = X_A\n",
    "        self.X_A_feature_names = feature_names\n",
    "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(self.users_df['uudi'])}\n",
    "        \n",
    "        return X_A\n",
    "    \n",
    "    def build_X_T(self, embedding_dim: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build inverse user attribute matrix X_T\n",
    "        \n",
    "        Learned from user-POI interaction patterns using matrix factorization.\n",
    "        \n",
    "        Intuition: Users who visit similar POIs should have similar X_T vectors.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimensionality of learned embeddings\n",
    "        \n",
    "        Returns:\n",
    "            X_T: (num_users, embedding_dim) matrix\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Building X_T: Inverse User Attribute Matrix\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Build user-POI interaction matrix\n",
    "        # Rows = users, Columns = POIs, Values = interaction strength\n",
    "        \n",
    "        # Get unique users and POIs\n",
    "        unique_users = self.users_df['uudi'].tolist()\n",
    "        \n",
    "        # Get all level 0 POI IDs from tree\n",
    "        all_poi_ids = list(self.poi_tree['level_0'].keys())\n",
    "        \n",
    "        user_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "        poi_to_idx = {pid: idx for idx, pid in enumerate(all_poi_ids)}\n",
    "        \n",
    "        # Create sparse interaction matrix\n",
    "        n_users = len(unique_users)\n",
    "        n_pois = len(all_poi_ids)\n",
    "        \n",
    "        print(f\"Building interaction matrix: {n_users} users × {n_pois} POIs\")\n",
    "        \n",
    "        # Aggregate interactions (visits + weighted ratings)\n",
    "        user_poi_scores = {}\n",
    "        \n",
    "        for _, row in self.interactions_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            poi_id = row['poi_id']\n",
    "            \n",
    "            if user_id not in user_to_idx or poi_id not in poi_to_idx:\n",
    "                continue\n",
    "            \n",
    "            key = (user_id, poi_id)\n",
    "            \n",
    "            if row['interaction_type'] == 'visit':\n",
    "                user_poi_scores[key] = user_poi_scores.get(key, 0) + 1.0\n",
    "            elif row['interaction_type'] == 'rating':\n",
    "                # Normalize rating to 0-1 scale\n",
    "                user_poi_scores[key] = user_poi_scores.get(key, 0) + (row['value'] / 5.0)\n",
    "            elif row['interaction_type'] == 'search':\n",
    "                user_poi_scores[key] = user_poi_scores.get(key, 0) + 0.3\n",
    "        \n",
    "        # Build sparse matrix\n",
    "        row_indices = []\n",
    "        col_indices = []\n",
    "        values = []\n",
    "        \n",
    "        for (user_id, poi_id), score in user_poi_scores.items():\n",
    "            row_indices.append(user_to_idx[user_id])\n",
    "            col_indices.append(poi_to_idx[poi_id])\n",
    "            values.append(score)\n",
    "        \n",
    "        interaction_matrix = csr_matrix(\n",
    "            (values, (row_indices, col_indices)),\n",
    "            shape=(n_users, n_pois)\n",
    "        )\n",
    "        \n",
    "        print(f\"Interaction matrix density: {interaction_matrix.nnz / (n_users * n_pois) * 100:.2f}%\")\n",
    "        \n",
    "        # Matrix factorization to learn latent user features\n",
    "        # Using NMF (Non-negative Matrix Factorization)\n",
    "        print(f\"Performing matrix factorization (embedding_dim={embedding_dim})...\")\n",
    "        \n",
    "        nmf = NMF(n_components=embedding_dim, init='random', random_state=42, max_iter=200)\n",
    "        X_T = nmf.fit_transform(interaction_matrix)\n",
    "        \n",
    "        # Normalize\n",
    "        scaler = StandardScaler()\n",
    "        X_T = scaler.fit_transform(X_T)\n",
    "        \n",
    "        print(f\"X_T shape: {X_T.shape}\")\n",
    "        print(f\"Reconstruction error: {nmf.reconstruction_err_:.4f}\")\n",
    "        \n",
    "        self.X_T = X_T\n",
    "        self.X_T_model = nmf\n",
    "        self.poi_to_idx = poi_to_idx\n",
    "        \n",
    "        return X_T\n",
    "    \n",
    "    def build_user_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build complete user embeddings: X = [X_A | X_T]\n",
    "        \n",
    "        Returns:\n",
    "            X: (num_users, dim_X_A + dim_X_T) matrix\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Building Complete User Embeddings\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        X_A = self.build_X_A()\n",
    "        X_T = self.build_X_T(embedding_dim=32)\n",
    "        \n",
    "        # Concatenate\n",
    "        X = np.hstack([X_A, X_T])\n",
    "        \n",
    "        print(f\"\\nFinal user embedding shape: {X.shape}\")\n",
    "        print(f\"  X_A dimensions: {X_A.shape[1]}\")\n",
    "        print(f\"  X_T dimensions: {X_T.shape[1]}\")\n",
    "        print(f\"  Total dimensions: {X.shape[1]}\")\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        # Store user embeddings in dictionary\n",
    "        for idx, user_id in enumerate(self.users_df['uudi']):\n",
    "            self.user_embeddings[user_id] = X[idx]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    # ========================================================================\n",
    "    # POI REPRESENTATION LEARNING (Multi-level)\n",
    "    # ========================================================================\n",
    "    \n",
    "    def build_Y_A_level(self, level: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build direct POI attribute matrix Y_A^l for level l\n",
    "        \n",
    "        Features from POI attributes:\n",
    "        - category (one-hot)\n",
    "        - spatial (lat, lon, normalized)\n",
    "        - price (normalized)\n",
    "        - popularity (normalized)\n",
    "        - characteristic (text embedding - TF-IDF or multi-hot)\n",
    "        - region (one-hot)\n",
    "        - district (one-hot)\n",
    "        \n",
    "        Args:\n",
    "            level: Tree level (0=finest, 3=coarsest)\n",
    "        \n",
    "        Returns:\n",
    "            Y_A^l: (num_pois_at_level, num_features) matrix\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Building Y_A^{level}: Direct POI Attribute Matrix (Level {level})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        level_key = f'level_{level}'\n",
    "        pois_at_level = self.poi_tree[level_key]\n",
    "        \n",
    "        poi_ids = list(pois_at_level.keys())\n",
    "        n_pois = len(poi_ids)\n",
    "        \n",
    "        print(f\"Number of POIs at level {level}: {n_pois}\")\n",
    "        \n",
    "        features_list = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # 1. Spatial features (lat, lon)\n",
    "        spatial_features = []\n",
    "        for poi_id in poi_ids:\n",
    "            poi_data = pois_at_level[poi_id]\n",
    "            spatial = poi_data['spatial']\n",
    "            if isinstance(spatial, str):\n",
    "                spatial = eval(spatial)\n",
    "            spatial_features.append(spatial)\n",
    "        \n",
    "        spatial_features = np.array(spatial_features)\n",
    "        # Normalize\n",
    "        spatial_scaler = StandardScaler()\n",
    "        spatial_normalized = spatial_scaler.fit_transform(spatial_features)\n",
    "        features_list.append(spatial_normalized)\n",
    "        feature_names.extend(['lat_norm', 'lon_norm'])\n",
    "        print(f\"Added spatial features: 2 dimensions\")\n",
    "        \n",
    "        # For level 0 (individual POIs), add more detailed features\n",
    "        if level == 0:\n",
    "            # 2. Category (one-hot)\n",
    "            categories = [pois_at_level[pid]['data']['category'] for pid in poi_ids]\n",
    "            category_encoder = LabelEncoder()\n",
    "            category_encoded = category_encoder.fit_transform(categories)\n",
    "            category_onehot = np.eye(len(category_encoder.classes_))[category_encoded]\n",
    "            features_list.append(category_onehot)\n",
    "            feature_names.extend([f'cat_{cls}' for cls in category_encoder.classes_])\n",
    "            print(f\"Added category features: {len(category_encoder.classes_)} dimensions\")\n",
    "            \n",
    "            # 3. Price (normalized)\n",
    "            prices = []\n",
    "            for poi_id in poi_ids:\n",
    "                price_str = pois_at_level[poi_id]['data']['price']\n",
    "                try:\n",
    "                    if '-' in str(price_str):\n",
    "                        price_vals = str(price_str).split('-')\n",
    "                        avg_price = (float(price_vals[0].strip()) + float(price_vals[1].strip())) / 2\n",
    "                    else:\n",
    "                        avg_price = float(price_str)\n",
    "                except:\n",
    "                    avg_price = 25.0  # Default\n",
    "                prices.append(avg_price)\n",
    "            \n",
    "            prices = np.array(prices).reshape(-1, 1)\n",
    "            price_scaler = StandardScaler()\n",
    "            prices_normalized = price_scaler.fit_transform(prices)\n",
    "            features_list.append(prices_normalized)\n",
    "            feature_names.append('price_norm')\n",
    "            print(f\"Added price feature: 1 dimension\")\n",
    "            \n",
    "            # 4. Popularity (normalized)\n",
    "            popularities = []\n",
    "            for poi_id in poi_ids:\n",
    "                try:\n",
    "                    pop = float(pois_at_level[poi_id]['data']['popularity'])\n",
    "                except:\n",
    "                    pop = 3.0\n",
    "                popularities.append(pop)\n",
    "            \n",
    "            popularities = np.array(popularities).reshape(-1, 1)\n",
    "            pop_scaler = StandardScaler()\n",
    "            popularities_normalized = pop_scaler.fit_transform(popularities)\n",
    "            features_list.append(popularities_normalized)\n",
    "            feature_names.append('popularity_norm')\n",
    "            print(f\"Added popularity feature: 1 dimension\")\n",
    "            \n",
    "            # 5. Characteristics (multi-hot from hashtags)\n",
    "            characteristics_list = []\n",
    "            for poi_id in poi_ids:\n",
    "                char_str = str(pois_at_level[poi_id]['data']['characteristic'])\n",
    "                # Extract hashtags\n",
    "                tags = [tag.strip().replace('#', '') for tag in char_str.split(',')]\n",
    "                characteristics_list.append(tags)\n",
    "            \n",
    "            mlb_chars = MultiLabelBinarizer()\n",
    "            chars_onehot = mlb_chars.fit_transform(characteristics_list)\n",
    "            features_list.append(chars_onehot)\n",
    "            feature_names.extend([f'char_{cls}' for cls in mlb_chars.classes_])\n",
    "            print(f\"Added characteristic features: {len(mlb_chars.classes_)} dimensions\")\n",
    "            \n",
    "            # 6. Region (one-hot)\n",
    "            regions = [pois_at_level[pid]['data']['region'] for pid in poi_ids]\n",
    "            region_encoder = LabelEncoder()\n",
    "            region_encoded = region_encoder.fit_transform(regions)\n",
    "            region_onehot = np.eye(len(region_encoder.classes_))[region_encoded]\n",
    "            features_list.append(region_onehot)\n",
    "            feature_names.extend([f'region_{cls}' for cls in region_encoder.classes_])\n",
    "            print(f\"Added region features: {len(region_encoder.classes_)} dimensions\")\n",
    "        \n",
    "        # For higher levels, use aggregated text features\n",
    "        else:\n",
    "            # Use textual representation (simpler for higher levels)\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            \n",
    "            texts = [pois_at_level[pid]['textual'] for pid in poi_ids]\n",
    "            tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "            text_features = tfidf.fit_transform(texts).toarray()\n",
    "            features_list.append(text_features)\n",
    "            feature_names.extend([f'text_{i}' for i in range(text_features.shape[1])])\n",
    "            print(f\"Added text features: {text_features.shape[1]} dimensions\")\n",
    "        \n",
    "        # Concatenate all features\n",
    "        Y_A_l = np.hstack(features_list)\n",
    "        \n",
    "        print(f\"\\nY_A^{level} shape: {Y_A_l.shape}\")\n",
    "        print(f\"Total features: {len(feature_names)}\")\n",
    "        \n",
    "        return Y_A_l, poi_ids, feature_names\n",
    "    \n",
    "    def build_Y_T_level(self, level: int, embedding_dim: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build inverse POI attribute matrix Y_T^l for level l\n",
    "        \n",
    "        Learned from user-POI interaction patterns.\n",
    "        \n",
    "        Intuition: POIs visited by similar users should have similar Y_T vectors.\n",
    "        \n",
    "        Args:\n",
    "            level: Tree level\n",
    "            embedding_dim: Dimensionality of learned embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Y_T^l: (num_pois_at_level, embedding_dim) matrix\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Building Y_T^{level}: Inverse POI Attribute Matrix (Level {level})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        level_key = f'level_{level}'\n",
    "        pois_at_level = self.poi_tree[level_key]\n",
    "        poi_ids = list(pois_at_level.keys())\n",
    "        \n",
    "        # Build POI-User interaction matrix (transpose of user-POI)\n",
    "        n_pois = len(poi_ids)\n",
    "        n_users = len(self.users_df)\n",
    "        \n",
    "        print(f\"Building interaction matrix: {n_pois} POIs × {n_users} users\")\n",
    "        \n",
    "        poi_to_idx_local = {pid: idx for idx, pid in enumerate(poi_ids)}\n",
    "        user_to_idx = {uid: idx for idx, uid in enumerate(self.users_df['uudi'])}\n",
    "        \n",
    "        # Aggregate interactions\n",
    "        poi_user_scores = {}\n",
    "        \n",
    "        for _, row in self.interactions_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            poi_id = row['poi_id']\n",
    "            \n",
    "            # For higher levels, map fine-grained POI to coarser level\n",
    "            if level > 0:\n",
    "                poi_id = self._get_parent_at_level(poi_id, target_level=level)\n",
    "            \n",
    "            if poi_id not in poi_to_idx_local or user_id not in user_to_idx:\n",
    "                continue\n",
    "            \n",
    "            key = (poi_id, user_id)\n",
    "            \n",
    "            if row['interaction_type'] == 'visit':\n",
    "                poi_user_scores[key] = poi_user_scores.get(key, 0) + 1.0\n",
    "            elif row['interaction_type'] == 'rating':\n",
    "                poi_user_scores[key] = poi_user_scores.get(key, 0) + (row['value'] / 5.0)\n",
    "            elif row['interaction_type'] == 'search':\n",
    "                poi_user_scores[key] = poi_user_scores.get(key, 0) + 0.3\n",
    "        \n",
    "        # Build sparse matrix\n",
    "        row_indices = []\n",
    "        col_indices = []\n",
    "        values = []\n",
    "        \n",
    "        for (poi_id, user_id), score in poi_user_scores.items():\n",
    "            row_indices.append(poi_to_idx_local[poi_id])\n",
    "            col_indices.append(user_to_idx[user_id])\n",
    "            values.append(score)\n",
    "        \n",
    "        interaction_matrix = csr_matrix(\n",
    "            (values, (row_indices, col_indices)),\n",
    "            shape=(n_pois, n_users)\n",
    "        )\n",
    "        \n",
    "        print(f\"Interaction matrix density: {interaction_matrix.nnz / (n_pois * n_users) * 100:.2f}%\")\n",
    "        \n",
    "        # Matrix factorization\n",
    "        print(f\"Performing matrix factorization (embedding_dim={embedding_dim})...\")\n",
    "        \n",
    "        nmf = NMF(n_components=embedding_dim, init='random', random_state=42, max_iter=200)\n",
    "        Y_T_l = nmf.fit_transform(interaction_matrix)\n",
    "        \n",
    "        # Normalize\n",
    "        scaler = StandardScaler()\n",
    "        Y_T_l = scaler.fit_transform(Y_T_l)\n",
    "        \n",
    "        print(f\"Y_T^{level} shape: {Y_T_l.shape}\")\n",
    "        print(f\"Reconstruction error: {nmf.reconstruction_err_:.4f}\")\n",
    "        \n",
    "        return Y_T_l\n",
    "    \n",
    "    def _get_parent_at_level(self, poi_id: str, target_level: int) -> str:\n",
    "        \"\"\"\n",
    "        Get parent node of poi_id at target_level\n",
    "        \"\"\"\n",
    "        # Start from level 0\n",
    "        current_level = 0\n",
    "        current_id = poi_id\n",
    "        \n",
    "        while current_level < target_level:\n",
    "            level_key = f'level_{current_level}'\n",
    "            if current_id in self.poi_tree[level_key]:\n",
    "                parent = self.poi_tree[level_key][current_id].get('parent')\n",
    "                if parent:\n",
    "                    current_id = parent\n",
    "                    current_level += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return current_id\n",
    "    \n",
    "    def build_poi_embeddings(self, levels: List[int] = [0, 1, 2, 3]):\n",
    "        \"\"\"\n",
    "        Build complete POI embeddings for all specified levels\n",
    "        \n",
    "        Y^l = [Y_A^l | Y_T^l] for each level l\n",
    "        \n",
    "        Args:\n",
    "            levels: List of tree levels to build embeddings for\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Building Complete POI Embeddings (All Levels)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for level in levels:\n",
    "            print(f\"\\n--- Processing Level {level} ---\")\n",
    "            \n",
    "            Y_A_l, poi_ids, feature_names = self.build_Y_A_level(level)\n",
    "            Y_T_l = self.build_Y_T_level(level, embedding_dim=32)\n",
    "            \n",
    "            # Concatenate\n",
    "            Y_l = np.hstack([Y_A_l, Y_T_l])\n",
    "            \n",
    "            print(f\"\\nFinal POI embedding for level {level}:\")\n",
    "            print(f\"  Y_A^{level} dimensions: {Y_A_l.shape[1]}\")\n",
    "            print(f\"  Y_T^{level} dimensions: {Y_T_l.shape[1]}\")\n",
    "            print(f\"  Total dimensions: {Y_l.shape[1]}\")\n",
    "            \n",
    "            # Store\n",
    "            self.poi_embeddings[f'level_{level}'] = {\n",
    "                'embeddings': Y_l,\n",
    "                'poi_ids': poi_ids,\n",
    "                'Y_A': Y_A_l,\n",
    "                'Y_T': Y_T_l,\n",
    "                'feature_names': feature_names\n",
    "            }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE & LOAD\n",
    "    # ========================================================================\n",
    "    \n",
    "    def save_embeddings(self, output_file: str = 'embeddings.pkl'):\n",
    "        \"\"\"Save all embeddings to file\"\"\"\n",
    "        data = {\n",
    "            'user_embeddings': self.user_embeddings,\n",
    "            'poi_embeddings': self.poi_embeddings,\n",
    "            'user_id_to_idx': self.user_id_to_idx,\n",
    "            'X': self.X,\n",
    "            'X_A': self.X_A,\n",
    "            'X_T': self.X_T,\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        print(f\"\\nEmbeddings saved to {output_file}\")\n",
    "    \n",
    "    def load_embeddings(self, input_file: str = 'embeddings.pkl'):\n",
    "        \"\"\"Load embeddings from file\"\"\"\n",
    "        with open(input_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.user_embeddings = data['user_embeddings']\n",
    "        self.poi_embeddings = data['poi_embeddings']\n",
    "        self.user_id_to_idx = data['user_id_to_idx']\n",
    "        self.X = data['X']\n",
    "        self.X_A = data['X_A']\n",
    "        self.X_T = data['X_T']\n",
    "        \n",
    "        print(f\"\\nEmbeddings loaded from {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdaa9d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building Complete User Embeddings\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Building X_A: Direct User Attribute Matrix\n",
      "============================================================\n",
      "Added age_group features: 5 dimensions\n",
      "Added area_of_residence features: 11 dimensions\n",
      "Added interests features: 25 dimensions\n",
      "Added transportation_modes features: 6 dimensions\n",
      "Added price_sensitivity features: 3 dimensions\n",
      "\n",
      "X_A shape: (21, 50)\n",
      "Total features: 50\n",
      "\n",
      "============================================================\n",
      "Building X_T: Inverse User Attribute Matrix\n",
      "============================================================\n",
      "Building interaction matrix: 21 users × 4696 POIs\n",
      "Interaction matrix density: 0.25%\n",
      "Performing matrix factorization (embedding_dim=32)...\n",
      "X_T shape: (21, 32)\n",
      "Reconstruction error: 0.0047\n",
      "\n",
      "Final user embedding shape: (21, 82)\n",
      "  X_A dimensions: 50\n",
      "  X_T dimensions: 32\n",
      "  Total dimensions: 82\n",
      "\n",
      "============================================================\n",
      "Building Complete POI Embeddings (All Levels)\n",
      "============================================================\n",
      "\n",
      "--- Processing Level 0 ---\n",
      "\n",
      "============================================================\n",
      "Building Y_A^0: Direct POI Attribute Matrix (Level 0)\n",
      "============================================================\n",
      "Number of POIs at level 0: 4696\n",
      "Added spatial features: 2 dimensions\n",
      "Added category features: 23 dimensions\n",
      "Added price feature: 1 dimension\n",
      "Added popularity feature: 1 dimension\n",
      "Added characteristic features: 22 dimensions\n",
      "Added region features: 5 dimensions\n",
      "\n",
      "Y_A^0 shape: (4696, 54)\n",
      "Total features: 54\n",
      "\n",
      "============================================================\n",
      "Building Y_T^0: Inverse POI Attribute Matrix (Level 0)\n",
      "============================================================\n",
      "Building interaction matrix: 4696 POIs × 21 users\n",
      "Interaction matrix density: 0.25%\n",
      "Performing matrix factorization (embedding_dim=32)...\n",
      "Y_T^0 shape: (4696, 32)\n",
      "Reconstruction error: 0.0214\n",
      "\n",
      "Final POI embedding for level 0:\n",
      "  Y_A^0 dimensions: 54\n",
      "  Y_T^0 dimensions: 32\n",
      "  Total dimensions: 86\n",
      "\n",
      "--- Processing Level 1 ---\n",
      "\n",
      "============================================================\n",
      "Building Y_A^1: Direct POI Attribute Matrix (Level 1)\n",
      "============================================================\n",
      "Number of POIs at level 1: 1355\n",
      "Added spatial features: 2 dimensions\n",
      "Added text features: 50 dimensions\n",
      "\n",
      "Y_A^1 shape: (1355, 52)\n",
      "Total features: 52\n",
      "\n",
      "============================================================\n",
      "Building Y_T^1: Inverse POI Attribute Matrix (Level 1)\n",
      "============================================================\n",
      "Building interaction matrix: 1355 POIs × 21 users\n",
      "Interaction matrix density: 0.78%\n",
      "Performing matrix factorization (embedding_dim=32)...\n",
      "Y_T^1 shape: (1355, 32)\n",
      "Reconstruction error: 0.0415\n",
      "\n",
      "Final POI embedding for level 1:\n",
      "  Y_A^1 dimensions: 52\n",
      "  Y_T^1 dimensions: 32\n",
      "  Total dimensions: 84\n",
      "\n",
      "--- Processing Level 2 ---\n",
      "\n",
      "============================================================\n",
      "Building Y_A^2: Direct POI Attribute Matrix (Level 2)\n",
      "============================================================\n",
      "Number of POIs at level 2: 44\n",
      "Added spatial features: 2 dimensions\n",
      "Added text features: 50 dimensions\n",
      "\n",
      "Y_A^2 shape: (44, 52)\n",
      "Total features: 52\n",
      "\n",
      "============================================================\n",
      "Building Y_T^2: Inverse POI Attribute Matrix (Level 2)\n",
      "============================================================\n",
      "Building interaction matrix: 44 POIs × 21 users\n",
      "Interaction matrix density: 17.10%\n",
      "Performing matrix factorization (embedding_dim=32)...\n",
      "Y_T^2 shape: (44, 32)\n",
      "Reconstruction error: 0.7489\n",
      "\n",
      "Final POI embedding for level 2:\n",
      "  Y_A^2 dimensions: 52\n",
      "  Y_T^2 dimensions: 32\n",
      "  Total dimensions: 84\n",
      "\n",
      "--- Processing Level 3 ---\n",
      "\n",
      "============================================================\n",
      "Building Y_A^3: Direct POI Attribute Matrix (Level 3)\n",
      "============================================================\n",
      "Number of POIs at level 3: 5\n",
      "Added spatial features: 2 dimensions\n",
      "Added text features: 50 dimensions\n",
      "\n",
      "Y_A^3 shape: (5, 52)\n",
      "Total features: 52\n",
      "\n",
      "============================================================\n",
      "Building Y_T^3: Inverse POI Attribute Matrix (Level 3)\n",
      "============================================================\n",
      "Building interaction matrix: 5 POIs × 21 users\n",
      "Interaction matrix density: 61.90%\n",
      "Performing matrix factorization (embedding_dim=32)...\n",
      "Y_T^3 shape: (5, 32)\n",
      "Reconstruction error: 0.0281\n",
      "\n",
      "Final POI embedding for level 3:\n",
      "  Y_A^3 dimensions: 52\n",
      "  Y_T^3 dimensions: 32\n",
      "  Total dimensions: 84\n",
      "\n",
      "Embeddings saved to embeddings.pkl\n",
      "\n",
      "User 966592ed-5bfd-4113-9c4d-d93cd3637b40 embedding shape: (82,)\n",
      "Level 0 POI embeddings shape: (4696, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brian\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    learner = AttributeBasedRepresentationLearning(\n",
    "        users_file='user_preferences.csv',\n",
    "        interactions_file='user_poi_interactions.csv',\n",
    "        poi_tree_file='poi_tree_with_uuids.json'\n",
    "    )\n",
    "    \n",
    "    # Build user embeddings\n",
    "    X = learner.build_user_embeddings()\n",
    "    \n",
    "    # Build POI embeddings for all levels\n",
    "    learner.build_poi_embeddings(levels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Save embeddings\n",
    "    learner.save_embeddings('embeddings.pkl')\n",
    "    \n",
    "    # Example: Get embedding for a specific user\n",
    "    user_id = learner.users_df.iloc[0]['uudi']\n",
    "    user_embedding = learner.user_embeddings[user_id]\n",
    "    print(f\"\\nUser {user_id} embedding shape: {user_embedding.shape}\")\n",
    "    \n",
    "    # Example: Get embeddings for level 0 POIs\n",
    "    level_0_embeddings = learner.poi_embeddings['level_0']['embeddings']\n",
    "    print(f\"Level 0 POI embeddings shape: {level_0_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7589c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
