{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7ecda2bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterator, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Keep embedding dimensions consistent across the pipeline\n",
        "FINAL_EMBEDDING_DIM = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0c46083e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: 모듈 파일 분리하지 않고, 아래 셀들에서 정의된 코드만 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "27630917",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Event schema\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InteractionEvent:\n",
        "    user_id: str\n",
        "    poi_id: str\n",
        "    interaction_type: str  # e.g., visit | rating | search | click | view\n",
        "    value: float = 1.0\n",
        "    timestamp: Optional[str] = None  # keep as string\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Interaction weighting\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InteractionWeights:\n",
        "    view: float = 0.2\n",
        "    click: float = 0.6\n",
        "    visit: float = 1.0\n",
        "    like: float = 1.2\n",
        "    search: float = 0.3\n",
        "    rating: float = 1.0  # scaled by value/5\n",
        "    bookmark: float = 0.7\n",
        "    wishlist: float = 0.8\n",
        "    share: float = 0.9\n",
        "    review: float = 1.1\n",
        "    purchase: float = 1.3\n",
        "    revisit: float = 1.1\n",
        "    dwell: float = 0.9  # scaled by value (seconds)\n",
        "    other: float = 0.5\n",
        "\n",
        "\n",
        "def weight_for(interaction_type: str, value: float, w: InteractionWeights) -> float:\n",
        "    t = (interaction_type or \"\").lower().strip()\n",
        "    if t == \"view\":\n",
        "        return w.view\n",
        "    if t == \"click\":\n",
        "        return w.click\n",
        "    if t == \"visit\":\n",
        "        return w.visit\n",
        "    if t == \"like\":\n",
        "        return w.like\n",
        "    if t == \"search\":\n",
        "        return w.search\n",
        "    if t == \"rating\":\n",
        "        v = float(value)\n",
        "        v = max(1.0, min(5.0, v))\n",
        "        return (v / 5.0) * w.rating\n",
        "    if t == \"bookmark\":\n",
        "        return w.bookmark\n",
        "    if t == \"wishlist\":\n",
        "        return w.wishlist\n",
        "    if t == \"share\":\n",
        "        return w.share\n",
        "    if t == \"review\":\n",
        "        return w.review\n",
        "    if t == \"purchase\":\n",
        "        return w.purchase\n",
        "    if t == \"revisit\":\n",
        "        return w.revisit\n",
        "    if t == \"dwell\":\n",
        "        v = max(0.0, float(value))\n",
        "        v = min(v, 3600.0)\n",
        "        return (v / 60.0) * w.dwell\n",
        "    return w.other\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ad651739",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Timestamp parsing (lightweight)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "_TS_FORMATS = (\n",
        "    \"%m/%d/%Y %H:%M\",\n",
        "    \"%m/%d/%Y %H:%M:%S\",\n",
        "    \"%Y-%m-%d %H:%M:%S\",\n",
        "    \"%Y-%m-%d %H:%M\",\n",
        ")\n",
        "\n",
        "\n",
        "def parse_ts(ts: Optional[str]) -> Optional[datetime]:\n",
        "    if not ts:\n",
        "        return None\n",
        "    s = str(ts).strip()\n",
        "    if not s:\n",
        "        return None\n",
        "    for fmt in _TS_FORMATS:\n",
        "        try:\n",
        "            return datetime.strptime(s, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    try:\n",
        "        return datetime.fromisoformat(s)\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8aac70d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class POITreeIndex:\n",
        "    \"\"\"POI tree index.\n",
        "\n",
        "    Originally loaded from Sources/Files/poi_tree_with_uuids.json,\n",
        "    but also accepts poi_tree_data (dict) from Sources/metadata.pkl.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, poi_tree: object):\n",
        "        # poi_tree: dict (already loaded tree) or (str/Path) JSON path\n",
        "        self.path: str = \"\"\n",
        "\n",
        "        if isinstance(poi_tree, (str, Path)):\n",
        "            self.path = str(poi_tree)\n",
        "            with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
        "                self.tree = json.load(f)\n",
        "        elif isinstance(poi_tree, dict):\n",
        "            self.tree = poi_tree\n",
        "        else:\n",
        "            raise TypeError(\"poi_tree must be dict or path(str/Path)\")\n",
        "\n",
        "        level0 = self.tree.get(\"level_0\", {})\n",
        "        if not isinstance(level0, dict):\n",
        "            raise ValueError(\"poi_tree: expected tree['level_0'] to be a dict\")\n",
        "        self.level0_ids = set(level0.keys())\n",
        "\n",
        "        self._parent_cache: Optional[Dict[str, Dict[int, str]]] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_metadata_pkl(cls, metadata_pkl_path: object) -> \"POITreeIndex\":\n",
        "        p = Path(metadata_pkl_path)\n",
        "        with p.open(\"rb\") as f:\n",
        "            meta = pickle.load(f)\n",
        "        tree = meta.get(\"poi_tree_data\")\n",
        "        if not isinstance(tree, dict):\n",
        "            raise ValueError(\"metadata.pkl missing 'poi_tree_data' dict\")\n",
        "        return cls(tree)\n",
        "\n",
        "    def is_valid_level0(self, poi_id: str) -> bool:\n",
        "        return poi_id in self.level0_ids\n",
        "\n",
        "    def build_parent_cache(self, max_level: int = 3) -> Dict[str, Dict[int, str]]:\n",
        "        if self._parent_cache is not None:\n",
        "            return self._parent_cache\n",
        "\n",
        "        cache: Dict[str, Dict[int, str]] = {}\n",
        "        for poi_id in self.level0_ids:\n",
        "            cache[poi_id] = {}\n",
        "            current = poi_id\n",
        "            for lvl in range(max_level):\n",
        "                node = self.tree.get(f\"level_{lvl}\", {}).get(current)\n",
        "                if not isinstance(node, dict):\n",
        "                    break\n",
        "                parent = node.get(\"parent\")\n",
        "                if not parent:\n",
        "                    break\n",
        "                cache[poi_id][lvl + 1] = parent\n",
        "                current = parent\n",
        "\n",
        "        self._parent_cache = cache\n",
        "        return cache\n",
        "\n",
        "    def get_hierarchy(self, level0_poi_id: str, max_level: int = 3) -> List[Tuple[int, str]]:\n",
        "        out: List[Tuple[int, str]] = [(0, level0_poi_id)]\n",
        "        parents = self.build_parent_cache(max_level=max_level).get(level0_poi_id, {})\n",
        "        for lvl in range(1, max_level + 1):\n",
        "            pid = parents.get(lvl)\n",
        "            if pid:\n",
        "                out.append((lvl, pid))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "99a450da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# POI embedding sources (read-only)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "class POIEmbeddingSource:\n",
        "    \"\"\"Read-only interface: poi_id -> embedding vector.\"\"\"\n",
        "\n",
        "    def dim(self) -> int:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def available(self) -> bool:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def reload(self) -> bool:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def has(self, poi_id: str) -> bool:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get(self, poi_id: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Return embedding vector for poi_id, or None if unavailable/missing.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class HashPOIVectorSource(POIEmbeddingSource):\n",
        "    \"\"\"Deterministic pseudo-embedding for a POI id.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int):\n",
        "        self._dim = int(dim)\n",
        "\n",
        "    def dim(self) -> int:\n",
        "        return self._dim\n",
        "\n",
        "    def available(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def reload(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def has(self, poi_id: str) -> bool:\n",
        "        return True\n",
        "\n",
        "    def get(self, poi_id: str) -> np.ndarray:\n",
        "        import hashlib\n",
        "\n",
        "        h = hashlib.md5(poi_id.encode(\"utf-8\")).hexdigest()\n",
        "        seed = int(h[:8], 16)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        v = rng.standard_normal(self._dim).astype(np.float32)\n",
        "        v /= (np.linalg.norm(v) + 1e-12)\n",
        "        return v\n",
        "\n",
        "\n",
        "class NpyPOIEmbeddingSource(POIEmbeddingSource):\n",
        "    \"\"\"Loads POI embeddings from (poi_embeddings.npy, poi_ids.txt).\"\"\"\n",
        "\n",
        "    def __init__(self, npy_path: str, ids_path: str, expected_dim: int):\n",
        "        self.npy_path = Path(npy_path)\n",
        "        self.ids_path = Path(ids_path)\n",
        "        self.expected_dim = int(expected_dim)\n",
        "\n",
        "        self._poi_index: Dict[str, int] = {}\n",
        "        self._mat: Optional[np.ndarray] = None\n",
        "        self._loaded = False\n",
        "        self.reload()\n",
        "\n",
        "    def dim(self) -> int:\n",
        "        return self.expected_dim\n",
        "\n",
        "    def available(self) -> bool:\n",
        "        return self._loaded\n",
        "\n",
        "    def reload(self) -> bool:\n",
        "        if not (self.npy_path.exists() and self.ids_path.exists()):\n",
        "            self._loaded = False\n",
        "            self._poi_index = {}\n",
        "            self._mat = None\n",
        "            return False\n",
        "\n",
        "        mat = np.load(self.npy_path)\n",
        "        with self.ids_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            ids = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        if mat.ndim != 2:\n",
        "            raise ValueError(f\"Expected 2D npy matrix, got shape {mat.shape}\")\n",
        "        if len(ids) != mat.shape[0]:\n",
        "            raise ValueError(f\"IDs count {len(ids)} != npy rows {mat.shape[0]}\")\n",
        "        if mat.shape[1] != self.expected_dim:\n",
        "            raise ValueError(f\"POI dim mismatch: expected {self.expected_dim}, got {mat.shape[1]}\")\n",
        "\n",
        "        self._mat = mat.astype(np.float32, copy=False)\n",
        "        self._poi_index = {pid: i for i, pid in enumerate(ids)}\n",
        "        self._loaded = True\n",
        "        return True\n",
        "\n",
        "    def has(self, poi_id: str) -> bool:\n",
        "        return self._loaded and (poi_id in self._poi_index)\n",
        "\n",
        "    def get(self, poi_id: str) -> Optional[np.ndarray]:\n",
        "        if not self._loaded:\n",
        "            return None\n",
        "        idx = self._poi_index.get(poi_id)\n",
        "        if idx is None:\n",
        "            return None\n",
        "        return self._mat[idx]\n",
        "\n",
        "\n",
        "class PklPOIEmbeddingSource(POIEmbeddingSource):\n",
        "    \"\"\"Loads POI embeddings from a dict pickle: {poi_id: np.ndarray(dim,)}.\"\"\"\n",
        "\n",
        "    def __init__(self, pkl_path: str, expected_dim: int):\n",
        "        self.pkl_path = Path(pkl_path)\n",
        "        self.expected_dim = int(expected_dim)\n",
        "\n",
        "        self._d: Dict[str, np.ndarray] = {}\n",
        "        self._loaded = False\n",
        "        self.reload()\n",
        "\n",
        "    def dim(self) -> int:\n",
        "        return self.expected_dim\n",
        "\n",
        "    def available(self) -> bool:\n",
        "        return self._loaded\n",
        "\n",
        "    def reload(self) -> bool:\n",
        "        if not self.pkl_path.exists():\n",
        "            self._d = {}\n",
        "            self._loaded = False\n",
        "            return False\n",
        "\n",
        "        with self.pkl_path.open(\"rb\") as f:\n",
        "            d = pickle.load(f)\n",
        "        if not isinstance(d, dict):\n",
        "            raise ValueError(\"POI pkl must be a dict {poi_id: vec}\")\n",
        "\n",
        "        sample = next(iter(d.values()), None)\n",
        "        if sample is not None:\n",
        "            v = np.asarray(sample)\n",
        "            if v.ndim != 1 or v.shape[0] != self.expected_dim:\n",
        "                raise ValueError(\n",
        "                    f\"POI pkl dim mismatch: expected ({self.expected_dim},), got {v.shape}\"\n",
        "                )\n",
        "\n",
        "        self._d = {str(k): np.asarray(v, dtype=np.float32) for k, v in d.items()}\n",
        "        self._loaded = True\n",
        "        return True\n",
        "\n",
        "    def has(self, poi_id: str) -> bool:\n",
        "        return self._loaded and (poi_id in self._d)\n",
        "\n",
        "    def get(self, poi_id: str) -> Optional[np.ndarray]:\n",
        "        if not self._loaded:\n",
        "            return None\n",
        "        return self._d.get(poi_id)\n",
        "\n",
        "\n",
        "class LazyPOIEmbeddingSource(POIEmbeddingSource):\n",
        "    \"\"\"POI embedding source that may be unavailable initially.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        expected_dim: int,\n",
        "        *,\n",
        "        npy_path: str = \"\",\n",
        "        ids_path: str = \"\",\n",
        "        pkl_path: str = \"\",\n",
        "        prefer: str = \"pkl\",\n",
        "    ):\n",
        "        self.expected_dim = int(expected_dim)\n",
        "        self.npy_path = Path(npy_path) if npy_path else None\n",
        "        self.ids_path = Path(ids_path) if ids_path else None\n",
        "        self.pkl_path = Path(pkl_path) if pkl_path else None\n",
        "        self.prefer = prefer\n",
        "\n",
        "        self._impl: Optional[POIEmbeddingSource] = None\n",
        "        self.reload()\n",
        "\n",
        "    def dim(self) -> int:\n",
        "        return self.expected_dim\n",
        "\n",
        "    def available(self) -> bool:\n",
        "        return self._impl is not None and self._impl.available()\n",
        "\n",
        "    def reload(self) -> bool:\n",
        "        candidates: List[Tuple[str, bool]] = []\n",
        "        has_pkl = bool(self.pkl_path and self.pkl_path.exists())\n",
        "        has_npy = bool(self.npy_path and self.ids_path and self.npy_path.exists() and self.ids_path.exists())\n",
        "        candidates.append((\"pkl\", has_pkl))\n",
        "        candidates.append((\"npy\", has_npy))\n",
        "\n",
        "        order = [self.prefer, \"npy\" if self.prefer == \"pkl\" else \"pkl\"]\n",
        "        chosen: Optional[str] = None\n",
        "        for kind in order:\n",
        "            ok = dict(candidates).get(kind, False)\n",
        "            if ok:\n",
        "                chosen = kind\n",
        "                break\n",
        "\n",
        "        if chosen is None:\n",
        "            self._impl = None\n",
        "            return False\n",
        "\n",
        "        if chosen == \"pkl\":\n",
        "            self._impl = PklPOIEmbeddingSource(str(self.pkl_path), expected_dim=self.expected_dim)\n",
        "            return True\n",
        "\n",
        "        self._impl = NpyPOIEmbeddingSource(\n",
        "            str(self.npy_path), str(self.ids_path), expected_dim=self.expected_dim\n",
        "        )\n",
        "        return True\n",
        "\n",
        "    def has(self, poi_id: str) -> bool:\n",
        "        return self._impl is not None and self._impl.has(poi_id)\n",
        "\n",
        "    def get(self, poi_id: str) -> Optional[np.ndarray]:\n",
        "        if self._impl is None:\n",
        "            return None\n",
        "        return self._impl.get(poi_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ce7a4c8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Online update configs\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class UpdateRuleConfig:\n",
        "    alpha: float = 0.15\n",
        "    use_adaptive_alpha: bool = False\n",
        "    alpha_max: float = 0.35\n",
        "    alpha_c: float = 5.0\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TimeDecayConfig:\n",
        "    enabled: bool = True\n",
        "    tau_seconds: float = 7.0 * 24.0 * 3600.0\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class NegativeSamplingConfig:\n",
        "    enabled: bool = True\n",
        "    k: int = 2\n",
        "    weight: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ColdStartInitConfig:\n",
        "    enabled: bool = False\n",
        "    user_preferences_csv: str = \"\"  # expects at least user_id column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b2ee9cda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# User vector store (persistent)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "class UserVectorStore:\n",
        "    \"\"\"Persistent user vector matrix.\n",
        "\n",
        "    Stores:\n",
        "      - <prefix>.json : list of user_ids in row order\n",
        "      - <prefix>.npy  : float32 matrix [num_users, dim]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, prefix: str):\n",
        "        self.dim = int(dim)\n",
        "        self.prefix = str(prefix)\n",
        "\n",
        "        self.user_ids: List[str] = []\n",
        "        self.id_to_idx: Dict[str, int] = {}\n",
        "        self.mat: np.ndarray = np.zeros((0, self.dim), dtype=np.float32)\n",
        "\n",
        "        self.dirty = False\n",
        "        self._load_if_exists()\n",
        "\n",
        "    @property\n",
        "    def ids_path(self) -> Path:\n",
        "        return Path(self.prefix + \".json\")\n",
        "\n",
        "    @property\n",
        "    def mat_path(self) -> Path:\n",
        "        return Path(self.prefix + \".npy\")\n",
        "\n",
        "    def _load_if_exists(self) -> None:\n",
        "        if self.ids_path.exists() and self.mat_path.exists():\n",
        "            with self.ids_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                self.user_ids = json.load(f)\n",
        "            self.id_to_idx = {u: i for i, u in enumerate(self.user_ids)}\n",
        "            self.mat = np.load(self.mat_path).astype(np.float32, copy=False)\n",
        "            if self.mat.ndim != 2 or self.mat.shape[1] != self.dim:\n",
        "                raise ValueError(f\"UserVectorStore dim mismatch: expected (*,{self.dim}), got {self.mat.shape}\")\n",
        "\n",
        "    def has(self, user_id: str) -> bool:\n",
        "        return user_id in self.id_to_idx\n",
        "\n",
        "    def get(self, user_id: str) -> np.ndarray:\n",
        "        idx = self.id_to_idx.get(user_id)\n",
        "        if idx is None:\n",
        "            idx = self._add_user(user_id)\n",
        "        return self.mat[idx]\n",
        "\n",
        "    def set(self, user_id: str, vec: np.ndarray) -> None:\n",
        "        idx = self.id_to_idx.get(user_id)\n",
        "        if idx is None:\n",
        "            idx = self._add_user(user_id)\n",
        "        self.mat[idx] = vec.astype(np.float32, copy=False)\n",
        "        self.dirty = True\n",
        "\n",
        "    def _add_user(self, user_id: str) -> int:\n",
        "        idx = len(self.user_ids)\n",
        "        self.user_ids.append(user_id)\n",
        "        self.id_to_idx[user_id] = idx\n",
        "        if self.mat.size == 0:\n",
        "            self.mat = np.zeros((1, self.dim), dtype=np.float32)\n",
        "        else:\n",
        "            self.mat = np.vstack([self.mat, np.zeros((1, self.dim), dtype=np.float32)])\n",
        "        self.dirty = True\n",
        "        return idx\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        if not self.dirty:\n",
        "            return\n",
        "        self.ids_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        tmp_ids = self.ids_path.with_suffix(\".json.tmp\")\n",
        "        tmp_mat = self.mat_path.with_suffix(\".npy.tmp\")\n",
        "\n",
        "        with tmp_ids.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.user_ids, f)\n",
        "\n",
        "        with open(tmp_mat, \"wb\") as f:\n",
        "            np.save(f, self.mat.astype(np.float32, copy=False))\n",
        "\n",
        "        os.replace(tmp_ids, self.ids_path)\n",
        "        os.replace(tmp_mat, self.mat_path)\n",
        "        self.dirty = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a88a80d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Cold-start init from Sources/Files/user_preferences.csv (POI-agnostic)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "def _hash_to_unit_vec(key: str, dim: int) -> np.ndarray:\n",
        "    import hashlib\n",
        "\n",
        "    h = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
        "    seed = int(h[:8], 16)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    v = rng.standard_normal(dim).astype(np.float32)\n",
        "    v /= float(np.linalg.norm(v) + 1e-12)\n",
        "    return v\n",
        "\n",
        "\n",
        "class PreferenceInitializer:\n",
        "    def __init__(self, csv_path: str, dim: int):\n",
        "        self.dim = int(dim)\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.user_pref_vec: Dict[str, np.ndarray] = {}\n",
        "        if self.csv_path.exists():\n",
        "            self._load()\n",
        "\n",
        "    def _load(self) -> None:\n",
        "        with self.csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            cols = reader.fieldnames or []\n",
        "            if \"user_id\" not in cols:\n",
        "                raise ValueError(\"user_preferences.csv must contain a 'user_id' column\")\n",
        "            for row in reader:\n",
        "                uid = (row.get(\"user_id\") or \"\").strip()\n",
        "                if not uid:\n",
        "                    continue\n",
        "                parts = []\n",
        "                for k, v in row.items():\n",
        "                    if k == \"user_id\":\n",
        "                        continue\n",
        "                    if v is None:\n",
        "                        continue\n",
        "                    s = str(v).strip()\n",
        "                    if s:\n",
        "                        parts.append(f\"{k}={s}\")\n",
        "                key = \"|\".join(parts) if parts else \"__empty__\"\n",
        "                self.user_pref_vec[uid] = _hash_to_unit_vec(key, self.dim)\n",
        "\n",
        "    def get_init(self, user_id: str) -> Optional[np.ndarray]:\n",
        "        return self.user_pref_vec.get(user_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "21cd2117",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Real-time embedder\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "class RealTimeInteractionEmbedder:\n",
        "    \"\"\"Real-time interaction handler.\n",
        "\n",
        "    Update rule (conceptually):\n",
        "      user <- (1-a)*user + a*(w(action,value)*poi_vec)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim: int,\n",
        "        poi_index: POITreeIndex,\n",
        "        poi_source: POIEmbeddingSource,\n",
        "        user_store_prefix: str = \"data/user_vecs\",\n",
        "        log_csv_path: Optional[str] = \"data/interaction_log.csv\",\n",
        "        flush_every: int = 500,\n",
        "        validate_poi: bool = True,\n",
        "        use_hierarchy: bool = False,\n",
        "        level_weights: Sequence[float] = (1.0, 0.4, 0.2, 0.1),\n",
        "        weights: InteractionWeights = InteractionWeights(),\n",
        "        update_cfg: UpdateRuleConfig = UpdateRuleConfig(),\n",
        "        time_decay_cfg: TimeDecayConfig = TimeDecayConfig(),\n",
        "        negative_cfg: NegativeSamplingConfig = NegativeSamplingConfig(),\n",
        "        cold_start_cfg: ColdStartInitConfig = ColdStartInitConfig(),\n",
        "        normalize_each_update: bool = True,\n",
        "    ):\n",
        "        self.dim = int(dim)\n",
        "        self.poi_index = poi_index\n",
        "        self.poi_source = poi_source\n",
        "\n",
        "        self.user_store = UserVectorStore(self.dim, user_store_prefix)\n",
        "        self.log_csv_path = log_csv_path\n",
        "        self.flush_every = int(flush_every)\n",
        "        self.validate_poi = bool(validate_poi)\n",
        "\n",
        "        self.use_hierarchy = bool(use_hierarchy)\n",
        "        self.level_weights = list(level_weights)\n",
        "        if len(self.level_weights) != 4:\n",
        "            raise ValueError(\"level_weights must have 4 values for level0..3\")\n",
        "\n",
        "        self.weights = weights\n",
        "        self.update_cfg = update_cfg\n",
        "        self.time_decay_cfg = time_decay_cfg\n",
        "        self.neg_cfg = negative_cfg\n",
        "        self.normalize_each_update = bool(normalize_each_update)\n",
        "\n",
        "        self._event_count = 0\n",
        "        self._user_event_counts: Dict[str, int] = {}\n",
        "        self._user_last_ts: Dict[str, datetime] = {}\n",
        "\n",
        "        self._neg_candidates = list(self.poi_index.level0_ids)\n",
        "        self._neg_rng = np.random.default_rng(self.neg_cfg.seed)\n",
        "\n",
        "        self._stats = {\n",
        "            \"events_total\": 0,\n",
        "            \"events_ok\": 0,\n",
        "            \"events_skipped_unknown_poi\": 0,\n",
        "            \"events_skipped_no_poi_embedding\": 0,\n",
        "            \"events_skipped_missing_user_id\": 0,\n",
        "            \"events_skipped_missing_poi_id\": 0,\n",
        "        }\n",
        "\n",
        "        self.pref_init: Optional[PreferenceInitializer] = None\n",
        "        if cold_start_cfg.enabled and cold_start_cfg.user_preferences_csv:\n",
        "            self.pref_init = PreferenceInitializer(cold_start_cfg.user_preferences_csv, self.dim)\n",
        "\n",
        "    def refresh_poi_source(self) -> bool:\n",
        "        return self.poi_source.reload()\n",
        "\n",
        "    def _compute_alpha(self, user_id: str, ts: Optional[datetime]) -> float:\n",
        "        alpha = float(self.update_cfg.alpha)\n",
        "        if self.update_cfg.use_adaptive_alpha:\n",
        "            n = self._user_event_counts.get(user_id, 0)\n",
        "            scale = 1.0 + (math.log1p(n) / max(self.update_cfg.alpha_c, 1e-6))\n",
        "            alpha = min(self.update_cfg.alpha_max, alpha * scale)\n",
        "\n",
        "        if self.time_decay_cfg.enabled and ts is not None:\n",
        "            last_ts = self._user_last_ts.get(user_id)\n",
        "            if last_ts is not None:\n",
        "                dt = (ts - last_ts).total_seconds()\n",
        "                if dt > 0:\n",
        "                    decay = math.exp(-dt / max(self.time_decay_cfg.tau_seconds, 1e-6))\n",
        "                    alpha = alpha * decay\n",
        "            self._user_last_ts[user_id] = ts\n",
        "\n",
        "        return max(0.0, min(1.0, alpha))\n",
        "\n",
        "    def _get_poi_signal_vec(self, poi_id_level0: str) -> Optional[np.ndarray]:\n",
        "        if not self.use_hierarchy:\n",
        "            return self.poi_source.get(poi_id_level0)\n",
        "\n",
        "        acc = np.zeros((self.dim,), dtype=np.float32)\n",
        "        found = False\n",
        "        for lvl, pid in self.poi_index.get_hierarchy(poi_id_level0, max_level=3):\n",
        "            w = float(self.level_weights[lvl])\n",
        "            if w == 0.0:\n",
        "                continue\n",
        "            v = self.poi_source.get(pid)\n",
        "            if v is None:\n",
        "                continue\n",
        "            acc += w * v.astype(np.float32, copy=False)\n",
        "            found = True\n",
        "        if not found:\n",
        "            return None\n",
        "        n = float(np.linalg.norm(acc) + 1e-12)\n",
        "        acc = acc / n\n",
        "        return acc\n",
        "\n",
        "    def _maybe_init_user(self, user_id: str) -> None:\n",
        "        if self.user_store.has(user_id):\n",
        "            return\n",
        "        if self.pref_init is None:\n",
        "            _ = self.user_store.get(user_id)\n",
        "            return\n",
        "        init_vec = self.pref_init.get_init(user_id)\n",
        "        if init_vec is None:\n",
        "            _ = self.user_store.get(user_id)\n",
        "            return\n",
        "        self.user_store.set(user_id, init_vec)\n",
        "\n",
        "    def handle_event(self, event: InteractionEvent) -> Dict[str, object]:\n",
        "        self._stats[\"events_total\"] += 1\n",
        "\n",
        "        uid = (event.user_id or \"\").strip()\n",
        "        pid0 = (event.poi_id or \"\").strip()\n",
        "        if not uid:\n",
        "            self._stats[\"events_skipped_missing_user_id\"] += 1\n",
        "            return {\"ok\": False, \"reason\": \"missing_user_id\"}\n",
        "        if not pid0:\n",
        "            self._stats[\"events_skipped_missing_poi_id\"] += 1\n",
        "            return {\"ok\": False, \"reason\": \"missing_poi_id\"}\n",
        "\n",
        "        if self.validate_poi and not self.poi_index.is_valid_level0(pid0):\n",
        "            self._stats[\"events_skipped_unknown_poi\"] += 1\n",
        "            return {\"ok\": False, \"reason\": \"unknown_poi\", \"poi_id\": pid0}\n",
        "\n",
        "        self._maybe_init_user(uid)\n",
        "\n",
        "        ts = parse_ts(event.timestamp)\n",
        "        alpha_eff = self._compute_alpha(uid, ts)\n",
        "\n",
        "        poi_vec = self._get_poi_signal_vec(pid0)\n",
        "        if poi_vec is None:\n",
        "            self._stats[\"events_skipped_no_poi_embedding\"] += 1\n",
        "            return {\"ok\": False, \"reason\": \"no_poi_embedding\", \"poi_id\": pid0}\n",
        "\n",
        "        w = weight_for(event.interaction_type, event.value, self.weights)\n",
        "\n",
        "        u = self.user_store.get(uid)\n",
        "        u2 = (1.0 - alpha_eff) * u + alpha_eff * (w * poi_vec)\n",
        "        if self.normalize_each_update:\n",
        "            u2 = u2 / (np.linalg.norm(u2) + 1e-12)\n",
        "        self.user_store.set(uid, u2)\n",
        "\n",
        "        if self.neg_cfg.enabled and self.neg_cfg.k > 0 and self._neg_candidates:\n",
        "            neg_ids = self._neg_rng.choice(self._neg_candidates, size=self.neg_cfg.k, replace=False)\n",
        "            for neg_id in neg_ids:\n",
        "                if neg_id == pid0:\n",
        "                    continue\n",
        "                neg_vec = self.poi_source.get(neg_id)\n",
        "                if neg_vec is None:\n",
        "                    continue\n",
        "                u = self.user_store.get(uid)\n",
        "                u2 = (1.0 - alpha_eff) * u - alpha_eff * (self.neg_cfg.weight * neg_vec)\n",
        "                if self.normalize_each_update:\n",
        "                    u2 = u2 / (np.linalg.norm(u2) + 1e-12)\n",
        "                self.user_store.set(uid, u2)\n",
        "\n",
        "        self._event_count += 1\n",
        "        self._user_event_counts[uid] = self._user_event_counts.get(uid, 0) + 1\n",
        "        self._stats[\"events_ok\"] += 1\n",
        "\n",
        "        if self.log_csv_path:\n",
        "            with open(self.log_csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                csv.writer(f).writerow(\n",
        "                    [\n",
        "                        self._event_count,\n",
        "                        uid,\n",
        "                        pid0,\n",
        "                        event.interaction_type,\n",
        "                        float(event.value),\n",
        "                        event.timestamp or \"\",\n",
        "                        1,\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "        if self._event_count % max(self.flush_every, 1) == 0:\n",
        "            self.user_store.flush()\n",
        "\n",
        "        return {\"ok\": True, \"user_id\": uid, \"poi_id\": pid0}\n",
        "\n",
        "    def recommend_from_candidate_pois(\n",
        "        self, user_id: str, candidate_poi_ids: Sequence[str], k: int = 10\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "        if not self.poi_source.available():\n",
        "            raise RuntimeError(\"POI embeddings not available: cannot compute recommendations\")\n",
        "\n",
        "        u = self.user_store.get(user_id).astype(np.float32)\n",
        "        un = float(np.linalg.norm(u) + 1e-12)\n",
        "\n",
        "        scores: List[Tuple[str, float]] = []\n",
        "        for pid in candidate_poi_ids:\n",
        "            if self.validate_poi and not self.poi_index.is_valid_level0(pid):\n",
        "                continue\n",
        "            v = self._get_poi_signal_vec(pid)\n",
        "            if v is None:\n",
        "                continue\n",
        "            s = float(np.dot(u, v) / ((np.linalg.norm(v) + 1e-12) * un))\n",
        "            scores.append((pid, s))\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scores[: int(k)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bcf6e01d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded metadata from c:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\metadata.pkl\n",
            "uuid-name pairs: 4696\n",
            "sample: [('poi_0_Giant', 'Giant'), ('poi_1_NTUC_FairPrice', 'NTUC FairPrice'), ('poi_2_NTUC_FairPrice', 'NTUC FairPrice')]\n",
            "POI embeddings loaded (level_0)\n",
            "   n: 4696\n",
            "   dim: 64\n",
            "   key sample: ['poi_0_Giant', 'poi_1_NTUC_FairPrice', 'poi_2_NTUC_FairPrice']\n",
            "CSV POIs: 235\n",
            "CSV Users: 21\n",
            "\n",
            "Complete!\n",
            "   POI dim: 64\n",
            "   Total events: 567\n",
            "   OK updates : 567\n",
            "   Skipped(no POI match): 0\n",
            "   Users updated: 21\n",
            "   Saved user vecs -> Sources\\user_vecs\n",
            "   Log -> Sources\\interaction_log_out.csv\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------\n",
        "# Batch replay (offline) to rebuild user vectors\n",
        "# --------------------------------------------------\n",
        "\n",
        "\n",
        "def find_repo_root(start=None) -> Path:\n",
        "    start = Path(start or Path.cwd())\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \".git\").exists():\n",
        "            return p\n",
        "    raise RuntimeError(\"Cannot find repo root (.git not found). Run inside the repo.\")\n",
        "\n",
        "\n",
        "ROOT = find_repo_root()\n",
        "SOURCES = ROOT / \"Sources\"\n",
        "DATA_DIR = SOURCES / \"Files\"\n",
        "\n",
        "POI_PKL = SOURCES / \"poi_embeddings.pkl\"\n",
        "META_PKL = SOURCES / \"metadata.pkl\"\n",
        "POI_TREE_JSON = DATA_DIR / \"poi_tree_with_uuids.json\"  # fallback only\n",
        "INTERACTIONS_CSV = DATA_DIR / \"user_poi_interactions.csv\"\n",
        "\n",
        "# Enforce consistent embedding dimension\n",
        "EXPECTED_DIM = FINAL_EMBEDDING_DIM\n",
        "\n",
        "OUT_USER_VECS_DIR = SOURCES / \"user_vecs\"\n",
        "OUT_LOG_CSV = SOURCES / \"interaction_log_out.csv\"\n",
        "OUT_USER_IDS_JSON = OUT_USER_VECS_DIR / \"user_ids.json\"\n",
        "OUT_USER_MAT_NPY = OUT_USER_VECS_DIR / \"user_mat.npy\"\n",
        "\n",
        "assert POI_PKL.exists(), f\"Missing: {POI_PKL}\"\n",
        "assert INTERACTIONS_CSV.exists(), f\"Missing: {INTERACTIONS_CSV}\"\n",
        "\n",
        "# If metadata.pkl exists, load poi_tree + indices here\n",
        "if META_PKL.exists():\n",
        "    with META_PKL.open(\"rb\") as f:\n",
        "        _meta = pickle.load(f)\n",
        "    poi_tree = _meta.get(\"poi_tree_data\", {})\n",
        "    meta_poi_to_idx = _meta.get(\"poi_to_idx\", {})\n",
        "    meta_idx_to_poi = _meta.get(\"idx_to_poi\", {})\n",
        "    poi_index = POITreeIndex(poi_tree)\n",
        "    print(\"Loaded metadata from\", META_PKL)\n",
        "else:\n",
        "    assert POI_TREE_JSON.exists(), f\"Missing: {POI_TREE_JSON} and no metadata.pkl\"\n",
        "    with POI_TREE_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        poi_tree = json.load(f)\n",
        "    meta_poi_to_idx = {}\n",
        "    meta_idx_to_poi = {}\n",
        "    poi_index = POITreeIndex(poi_tree)\n",
        "\n",
        "\n",
        "# 1) Build token <-> name mapping from poi_tree\n",
        "lvl0 = poi_tree.get(\"level_0\", {})\n",
        "token_to_name: Dict[str, str] = {}\n",
        "name_to_token: Dict[str, str] = {}\n",
        "\n",
        "if isinstance(lvl0, dict):\n",
        "    for tok, node in lvl0.items():\n",
        "        if not isinstance(tok, str):\n",
        "            continue\n",
        "        nm = \"\"\n",
        "        if isinstance(node, dict):\n",
        "            nm = str(node.get(\"name\", \"\")).strip()\n",
        "        if nm:\n",
        "            token_to_name[tok] = nm\n",
        "            if nm not in name_to_token:\n",
        "                name_to_token[nm] = tok\n",
        "\n",
        "print(\"uuid-name pairs:\", len(token_to_name))\n",
        "print(\"sample:\", list(token_to_name.items())[:3])\n",
        "\n",
        "\n",
        "# 2) Load level_0 embeddings from PKL as (poi_id -> vec)\n",
        "\n",
        "def load_level0_poi_embeddings(pkl_path: Path) -> Tuple[Dict[str, np.ndarray], int]:\n",
        "    with open(pkl_path, \"rb\") as f:\n",
        "        obj = pickle.load(f)\n",
        "\n",
        "    lvl0 = obj[\"poi_embeddings\"][\"level_0\"]\n",
        "    mat = np.asarray(lvl0[\"embeddings\"], dtype=np.float32)  # (N,D)\n",
        "    ids = lvl0[\"poi_ids\"]  # len N\n",
        "\n",
        "    if mat.ndim != 2:\n",
        "        raise ValueError(f\"Expected 2D embeddings matrix, got shape={mat.shape}\")\n",
        "    if len(ids) != mat.shape[0]:\n",
        "        raise ValueError(f\"poi_ids len {len(ids)} != embeddings rows {mat.shape[0]}\")\n",
        "    if mat.shape[1] != EXPECTED_DIM:\n",
        "        raise ValueError(f\"POI embedding dim mismatch: expected {EXPECTED_DIM}, got {mat.shape[1]}\")\n",
        "\n",
        "    emb = {str(ids[i]): mat[i] for i in range(mat.shape[0])}\n",
        "    return emb, EXPECTED_DIM\n",
        "\n",
        "\n",
        "poi_embeddings, POI_DIM = load_level0_poi_embeddings(POI_PKL)\n",
        "\n",
        "print(\"POI embeddings loaded (level_0)\")\n",
        "print(\"   n:\", len(poi_embeddings))\n",
        "print(\"   dim:\", POI_DIM)\n",
        "print(\"   key sample:\", list(poi_embeddings.keys())[:3])\n",
        "\n",
        "\n",
        "# 3) Load interactions CSV (flexible columns)\n",
        "\n",
        "df = pd.read_csv(INTERACTIONS_CSV)\n",
        "\n",
        "\n",
        "def pick_col(cands):\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for c in cands:\n",
        "        if c in cols:\n",
        "            return cols[c]\n",
        "    return None\n",
        "\n",
        "\n",
        "USER_COL = pick_col([\"user_id\", \"user\", \"uid\"])\n",
        "POI_COL = pick_col([\"poi_id\", \"poi\", \"item_id\", \"entity_id\", \"poi_name\"])\n",
        "TYPE_COL = pick_col([\"interaction_type\", \"action\", \"type\"])\n",
        "VAL_COL = pick_col([\"value\", \"weight\", \"rating\", \"score\"])\n",
        "TS_COL = pick_col([\"timestamp\", \"time\", \"ts\", \"datetime\"])\n",
        "\n",
        "assert USER_COL and POI_COL, f\"interactions csv needs user_id & poi_id cols. got={list(df.columns)}\"\n",
        "if TYPE_COL is None:\n",
        "    df[\"_type\"] = \"visit\"\n",
        "    TYPE_COL = \"_type\"\n",
        "if VAL_COL is None:\n",
        "    df[\"_val\"] = 1.0\n",
        "    VAL_COL = \"_val\"\n",
        "if TS_COL is None:\n",
        "    df[\"_ts\"] = \"\"\n",
        "    TS_COL = \"_ts\"\n",
        "\n",
        "df[USER_COL] = df[USER_COL].astype(str).str.strip()\n",
        "df[POI_COL] = df[POI_COL].astype(str).str.strip()\n",
        "\n",
        "print(\"CSV POIs:\", df[POI_COL].nunique())\n",
        "print(\"CSV Users:\", df[USER_COL].nunique())\n",
        "\n",
        "\n",
        "# 4) Resolve: if CSV poi_id is a token, use direct; otherwise map name -> token\n",
        "emb_keys = set(poi_embeddings.keys())\n",
        "emb_key_list = list(emb_keys)\n",
        "\n",
        "\n",
        "def resolve_poi_key(raw: str) -> Optional[str]:\n",
        "    r = str(raw).strip()\n",
        "    if r in emb_keys:\n",
        "        return r\n",
        "    tok = name_to_token.get(r)\n",
        "    if tok and tok in emb_keys:\n",
        "        return tok\n",
        "    return None\n",
        "\n",
        "\n",
        "# 5) Online updates (EMA)\n",
        "ALPHA = 0.15\n",
        "NORMALIZE_EACH_UPDATE = True\n",
        "\n",
        "TIME_DECAY_ENABLED = True\n",
        "TAU_SECONDS = 7.0 * 24.0 * 3600.0  # 7-day window\n",
        "\n",
        "NEGATIVE_SAMPLES_PER_EVENT = 2\n",
        "NEGATIVE_WEIGHT = 0.1\n",
        "NEGATIVE_SEED = 42\n",
        "_neg_rng = np.random.default_rng(NEGATIVE_SEED)\n",
        "\n",
        "\n",
        "def parse_ts_local(ts: Optional[str]) -> Optional[datetime]:\n",
        "    return parse_ts(ts)\n",
        "\n",
        "\n",
        "user_vecs: Dict[str, np.ndarray] = {}\n",
        "user_last_ts: Dict[str, datetime] = {}\n",
        "\n",
        "\n",
        "def get_user_vec(uid: str) -> np.ndarray:\n",
        "    if uid in user_vecs:\n",
        "        return user_vecs[uid]\n",
        "    v = np.zeros((POI_DIM,), dtype=np.float32)\n",
        "    user_vecs[uid] = v\n",
        "    return v\n",
        "\n",
        "\n",
        "OUT_USER_VECS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if OUT_LOG_CSV.exists():\n",
        "    OUT_LOG_CSV.unlink()\n",
        "\n",
        "with OUT_LOG_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    csv.writer(f).writerow([\"i\", \"user_id\", \"poi_raw\", \"poi_id\", \"type\", \"value\", \"timestamp\", \"ok\"])\n",
        "\n",
        "ok = 0\n",
        "skipped = 0\n",
        "for i, row in df.iterrows():\n",
        "    uid = row[USER_COL]\n",
        "    raw_poi = row[POI_COL]\n",
        "    t = row[TYPE_COL]\n",
        "    val = float(row[VAL_COL]) if not pd.isna(row[VAL_COL]) else 1.0\n",
        "    ts = row[TS_COL] if TS_COL in row else \"\"\n",
        "\n",
        "    key = resolve_poi_key(raw_poi)\n",
        "    if key is None:\n",
        "        skipped += 1\n",
        "        with OUT_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow([i, uid, raw_poi, \"\", t, val, ts, 0])\n",
        "        continue\n",
        "\n",
        "    alpha_eff = ALPHA\n",
        "    if TIME_DECAY_ENABLED:\n",
        "        ts_dt = parse_ts_local(ts)\n",
        "        if ts_dt is not None:\n",
        "            last = user_last_ts.get(uid)\n",
        "            if last is not None:\n",
        "                dt = (ts_dt - last).total_seconds()\n",
        "                if dt > 0:\n",
        "                    alpha_eff *= math.exp(-dt / max(TAU_SECONDS, 1e-6))\n",
        "            user_last_ts[uid] = ts_dt\n",
        "\n",
        "    vec = poi_embeddings.get(key)\n",
        "    if vec is None:\n",
        "        skipped += 1\n",
        "        with OUT_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow([i, uid, raw_poi, key, t, val, ts, 0])\n",
        "        continue\n",
        "\n",
        "    w = weight_for(str(t), val, InteractionWeights())\n",
        "    u = get_user_vec(uid)\n",
        "    u2 = (1.0 - alpha_eff) * u + alpha_eff * (w * vec)\n",
        "    if NORMALIZE_EACH_UPDATE:\n",
        "        u2 = u2 / (np.linalg.norm(u2) + 1e-12)\n",
        "    user_vecs[uid] = u2\n",
        "\n",
        "    if NEGATIVE_SAMPLES_PER_EVENT > 0 and len(emb_key_list) > 0:\n",
        "        neg_ids = _neg_rng.choice(emb_key_list, size=NEGATIVE_SAMPLES_PER_EVENT, replace=False)\n",
        "        for neg_key in neg_ids:\n",
        "            if neg_key == key:\n",
        "                continue\n",
        "            neg_vec = poi_embeddings.get(neg_key)\n",
        "            if neg_vec is None:\n",
        "                continue\n",
        "            u = get_user_vec(uid)\n",
        "            u2 = (1.0 - alpha_eff) * u - alpha_eff * (NEGATIVE_WEIGHT * neg_vec)\n",
        "            if NORMALIZE_EACH_UPDATE:\n",
        "                u2 = u2 / (np.linalg.norm(u2) + 1e-12)\n",
        "            user_vecs[uid] = u2\n",
        "\n",
        "    with OUT_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([i, uid, raw_poi, key, t, val, ts, 1])\n",
        "\n",
        "    ok += 1\n",
        "\n",
        "# Save (per-user npy + full matrix)\n",
        "for uid, vec in user_vecs.items():\n",
        "    np.save(OUT_USER_VECS_DIR / f\"{uid}.npy\", vec.astype(np.float32))\n",
        "\n",
        "user_ids = list(user_vecs.keys())\n",
        "mat = np.stack([user_vecs[u] for u in user_ids], axis=0) if user_ids else np.zeros((0, POI_DIM), dtype=np.float32)\n",
        "\n",
        "OUT_USER_IDS_JSON.write_text(json.dumps(user_ids, ensure_ascii=False), encoding=\"utf-8\")\n",
        "np.save(OUT_USER_MAT_NPY, mat.astype(np.float32))\n",
        "\n",
        "print(\"\\nComplete!\")\n",
        "print(f\"   POI dim: {POI_DIM}\")\n",
        "print(f\"   Total events: {len(df)}\")\n",
        "print(f\"   OK updates : {ok}\")\n",
        "print(f\"   Skipped(no POI match): {skipped}\")\n",
        "print(f\"   Users updated: {len(user_vecs)}\")\n",
        "print(f\"   Saved user vecs -> {OUT_USER_VECS_DIR.relative_to(ROOT)}\")\n",
        "print(f\"   Log -> {OUT_LOG_CSV.relative_to(ROOT)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
