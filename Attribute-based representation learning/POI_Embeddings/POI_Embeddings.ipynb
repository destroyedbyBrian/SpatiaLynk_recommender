{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ab98201e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from scipy.sparse import csr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "823d483b-bc38-45d2-9b9b-725dbaa5c155",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Optional, Union, List\n",
        "\n",
        "PathLike = Union[str, Path]\n",
        "\n",
        "def find_repo_root(start: Optional[PathLike] = None) -> Path:\n",
        "    start = Path(start or Path.cwd()).resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \".git\").exists():\n",
        "            return p\n",
        "    raise RuntimeError(f\"Cannot find repo root (.git not found). start={start}\")\n",
        "\n",
        "def find_sources_dir(root: Path, required_files: List[str]) -> Path:\n",
        "    \"\"\"\n",
        "    Find the 'Sources' folder under root that contains all required_files.\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    for d in root.rglob(\"Sources\"):\n",
        "        if not d.is_dir():\n",
        "            continue\n",
        "        ok = True\n",
        "        for fn in required_files:\n",
        "            if not (d / fn).exists():\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(d)\n",
        "\n",
        "    if not candidates:\n",
        "        # Debug hint: also show Sources candidates with missing files\n",
        "        any_sources = [d for d in root.rglob(\"Sources\") if d.is_dir()]\n",
        "        hint = \"\\n\".join(str(d) for d in any_sources[:10])\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not find a valid 'Sources' directory containing required files:\\n\"\n",
        "            f\"   {required_files}\\n\\n\"\n",
        "            \"Found 'Sources' directories (showing up to 10):\\n\"\n",
        "            f\"{hint if hint else '(none found)'}\\n\\n\"\n",
        "            \"Fix: set sources_dir directly or verify where the files are.\"\n",
        "        )\n",
        "\n",
        "    # If multiple, prefer the shortest path (closest to root)\n",
        "    candidates.sort(key=lambda p: len(p.parts))\n",
        "    return candidates[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bf1f079b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class POIEmbeddings:\n",
        "    def __init__(\n",
        "        self,\n",
        "        users_file: Optional[PathLike] = None,\n",
        "        user_poi_interactions_file: Optional[PathLike] = None,\n",
        "        poi_tree_file: Optional[PathLike] = None,\n",
        "        repo_root: Optional[PathLike] = None,\n",
        "        sources_dir: Optional[PathLike] = None,\n",
        "        metadata_pkl: Optional[PathLike] = None,\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.repo_root = Path(repo_root).resolve() if repo_root else find_repo_root()\n",
        "\n",
        "        required = [\"user_preferences.csv\", \"user_poi_interactions.csv\", \"poi_tree_with_uuids.json\"]\n",
        "\n",
        "        # If sources_dir is not provided, auto-detect\n",
        "        if sources_dir:\n",
        "            self.sources_dir = Path(sources_dir).resolve()\n",
        "        else:\n",
        "            self.sources_dir = find_sources_dir(self.repo_root, required)\n",
        "\n",
        "        def resolve_input(p: Optional[PathLike], default_name: str) -> Path:\n",
        "            if p is None:\n",
        "                return (self.sources_dir / default_name).resolve()\n",
        "            p = Path(p)\n",
        "            if p.is_absolute():\n",
        "                return p.resolve()\n",
        "            # If relative path, resolve against sources_dir\n",
        "            return (self.sources_dir / p).resolve()\n",
        "\n",
        "        self.users_file = resolve_input(users_file, \"user_preferences.csv\")\n",
        "        self.user_poi_interactions_file = resolve_input(user_poi_interactions_file, \"user_poi_interactions.csv\")\n",
        "        self.poi_tree_file = resolve_input(poi_tree_file, \"poi_tree_with_uuids.json\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"[POIEmbeddings] repo_root   =\", self.repo_root)\n",
        "            print(\"[POIEmbeddings] sources_dir =\", self.sources_dir)\n",
        "            print(\"[POIEmbeddings] users_file  =\", self.users_file, \"| exists:\", self.users_file.exists())\n",
        "            print(\"[POIEmbeddings] interactions_file =\", self.user_poi_interactions_file, \"| exists:\", self.user_poi_interactions_file.exists())\n",
        "            print(\"[POIEmbeddings] poi_tree_file =\", self.poi_tree_file, \"| exists:\", self.poi_tree_file.exists())\n",
        "\n",
        "        # Existence checks\n",
        "        assert self.sources_dir.exists(), f\"Missing Sources folder: {self.sources_dir}\"\n",
        "        assert self.users_file.exists(), f\"Missing: {self.users_file}\"\n",
        "        assert self.user_poi_interactions_file.exists(), f\"Missing: {self.user_poi_interactions_file}\"\n",
        "        assert self.poi_tree_file.exists(), f\"Missing: {self.poi_tree_file}\"\n",
        "\n",
        "        # Load CSVs\n",
        "        self.users_df = pd.read_csv(self.users_file)\n",
        "        self.interactions_df = pd.read_csv(self.user_poi_interactions_file)\n",
        "\n",
        "        # Load POI tree (JSON)\n",
        "        with open(self.poi_tree_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.poi_tree = json.load(f)\n",
        "\n",
        "        # Load POI index maps from metadata.pkl for ordering\n",
        "        if metadata_pkl is None:\n",
        "            metadata_pkl = self.sources_dir / \"metadata.pkl\"\n",
        "\n",
        "        self.meta_poi_to_idx = {}\n",
        "        self.meta_idx_to_poi = {}\n",
        "        if Path(metadata_pkl).exists():\n",
        "            with Path(metadata_pkl).open(\"rb\") as f:\n",
        "                _meta = pickle.load(f)\n",
        "            # {0: {poi_id: idx}, 1: {...}, ...}\n",
        "            self.meta_poi_to_idx = _meta.get(\"poi_to_idx\", {})\n",
        "            self.meta_idx_to_poi = _meta.get(\"idx_to_poi\", {})\n",
        "            if verbose:\n",
        "                print(\"[POIEmbeddings] loaded metadata.pkl from\", metadata_pkl)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"[POIEmbeddings] metadata.pkl not found at\", metadata_pkl, \"(fallback: local order)\")\n",
        "\n",
        "        # (Optional) Validate poi_tree structure: check level_0~level_3 keys\n",
        "        for k in (f\"level_{i}\" for i in range(4)):\n",
        "            if k not in self.poi_tree:\n",
        "                raise KeyError(\n",
        "                    f\"poi_tree missing key '{k}'. \"\n",
        "                    f\"available keys (sample) = {list(self.poi_tree.keys())[:20]}\"\n",
        "                )\n",
        "\n",
        "        # (Optional) Initialize storage used in later build steps\n",
        "        self.user_embeddings = {}\n",
        "        self.poi_embeddings = {}\n",
        "        self.encoders = {}\n",
        "        \n",
        "    def build_Y_A_level(self, level: int) -> Tuple[np.ndarray, List[str], List[str]]:\n",
        "        \"\"\"\n",
        "        Build direct POI attribute matrix Y_A^l for level l\n",
        "        \n",
        "        Level-specific Explicit POI Attributes from poi_tree_with_uuids.json:\n",
        "        \n",
        "        Level 0 (Building): category, price, popularity, characteristics, spatial, textual\n",
        "        Level 1 (Street): category, num_entities, spatial, textual\n",
        "        Level 2 (District): num_level1_nodes, spatial, textual\n",
        "        Level 3 (Region): num_districts, spatial, textual\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"Building Y_A^{level}: Direct POI Attribute Matrix (Level {level})\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        \n",
        "        level_names = {0: 'Building', 1: 'Street', 2: 'District', 3: 'Region'}\n",
        "        print(f\"Granularity: {level_names.get(level, 'Unknown')}\")\n",
        "        \n",
        "        level_key = f'level_{level}'\n",
        "        pois_at_level = self.poi_tree[level_key]\n",
        "\n",
        "        # Sort POI IDs by metadata.pkl idx order (fallback to default)\n",
        "        if hasattr(self, \"meta_idx_to_poi\") and level in self.meta_idx_to_poi:\n",
        "            idx2poi_level = self.meta_idx_to_poi[level]\n",
        "            poi_ids = [idx2poi_level[i] for i in sorted(idx2poi_level.keys()) if idx2poi_level[i] in pois_at_level]\n",
        "        else:\n",
        "            poi_ids = list(pois_at_level.keys())\n",
        "\n",
        "        n_pois = len(poi_ids)\n",
        "        \n",
        "        print(f\"Number of nodes at level {level}: {n_pois}\")\n",
        "        \n",
        "        features_list = []\n",
        "        feature_names = []\n",
        "        \n",
        "        # Store encoders/scalers for later use\n",
        "        self.encoders[level] = {}\n",
        "        \n",
        "        # COMMON FEATURES (All Levels): spatial, textual\n",
        "        \n",
        "        # 1. Spatial Features (lat, lon)\n",
        "        spatial_features = self._extract_spatial_features(pois_at_level, poi_ids)\n",
        "        spatial_scaler = StandardScaler()\n",
        "        spatial_normalized = spatial_scaler.fit_transform(spatial_features)\n",
        "        \n",
        "        self.encoders[level]['spatial_scaler'] = spatial_scaler\n",
        "        features_list.append(spatial_normalized)\n",
        "        feature_names.extend(['spatial_lat_norm', 'spatial_lon_norm'])\n",
        "        print(f\"  [1] Spatial features: 2 dimensions (lat, lon normalized)\")\n",
        "        \n",
        "        # LEVEL-SPECIFIC FEATURES\n",
        "        \n",
        "        if level == 0:\n",
        "            self._build_level0_features(pois_at_level, poi_ids, features_list, feature_names)\n",
        "        elif level == 1:\n",
        "            self._build_level1_features(pois_at_level, poi_ids, features_list, feature_names)\n",
        "        elif level == 2:\n",
        "            self._build_level2_features(pois_at_level, poi_ids, features_list, feature_names)\n",
        "        elif level == 3:\n",
        "            self._build_level3_features(pois_at_level, poi_ids, features_list, feature_names)\n",
        "        \n",
        "        # TEXTUAL FEATURES (All Levels) - TF-IDF\n",
        "        text_features, text_feature_names = self._extract_textual_features(\n",
        "            pois_at_level, poi_ids, level\n",
        "        )\n",
        "        features_list.append(text_features)\n",
        "        feature_names.extend(text_feature_names)\n",
        "        \n",
        "        # Concatenate all features\n",
        "        Y_A_level = np.hstack(features_list)\n",
        "        \n",
        "        return Y_A_level, poi_ids, feature_names\n",
        "\n",
        "    def _extract_spatial_features(self, pois_at_level: Dict, poi_ids: List[str]) -> np.ndarray:\n",
        "        \"\"\"Extract spatial (lat, lon) features from POI data\"\"\"\n",
        "        spatial_features = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            spatial = poi_data['spatial']\n",
        "            \n",
        "            if isinstance(spatial, str):\n",
        "                spatial = eval(spatial)\n",
        "            elif isinstance(spatial, list):\n",
        "                spatial = tuple(spatial)\n",
        "            \n",
        "            spatial_features.append([spatial[0], spatial[1]])\n",
        "        \n",
        "        return np.array(spatial_features, dtype=np.float32)\n",
        "\n",
        "    def _build_level0_features(self, pois_at_level: Dict, poi_ids: List[str],\n",
        "                               features_list: List, feature_names: List):\n",
        "        \"\"\"Build Level 0 (Building) specific features\"\"\"\n",
        "        level = 0\n",
        "        \n",
        "        # Category (one-hot)\n",
        "        categories = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            cat = poi_data.get('data', {}).get('category', 'unknown')\n",
        "            categories.append(str(cat).lower().strip())\n",
        "        \n",
        "        category_encoder = LabelEncoder()\n",
        "        category_encoded = category_encoder.fit_transform(categories)\n",
        "        n_categories = len(category_encoder.classes_)\n",
        "        category_onehot = np.eye(n_categories, dtype=np.float32)[category_encoded]\n",
        "        \n",
        "        self.encoders[level]['category_encoder'] = category_encoder\n",
        "        features_list.append(category_onehot)\n",
        "        feature_names.extend([f'category_{cls}' for cls in category_encoder.classes_])\n",
        "        print(f\"  [2] Category features: {n_categories} dimensions (one-hot)\")\n",
        "        \n",
        "        # Price (normalized)\n",
        "        prices = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            price_str = poi_data.get('data', {}).get('price', None)\n",
        "            avg_price = self._parse_price(price_str)\n",
        "            prices.append(avg_price)\n",
        "        \n",
        "        prices = np.array(prices, dtype=np.float32).reshape(-1, 1)\n",
        "        price_scaler = StandardScaler()\n",
        "        prices_normalized = price_scaler.fit_transform(prices)\n",
        "        \n",
        "        self.encoders[level]['price_scaler'] = price_scaler\n",
        "        features_list.append(prices_normalized)\n",
        "        feature_names.append('price_norm')\n",
        "        print(f\"  [3] Price feature: 1 dimension (normalized)\")\n",
        "        print(f\"      Range: [{prices.min():.2f}, {prices.max():.2f}], mean: {prices.mean():.2f}\")\n",
        "        \n",
        "        # Popularity (normalized)\n",
        "        popularities = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            pop_val = poi_data.get('data', {}).get('popularity', 3.0)\n",
        "            try:\n",
        "                popularity = float(pop_val)\n",
        "            except (ValueError, TypeError):\n",
        "                popularity = 3.0\n",
        "            popularities.append(popularity)\n",
        "        \n",
        "        popularities = np.array(popularities, dtype=np.float32).reshape(-1, 1)\n",
        "        popularity_scaler = StandardScaler()\n",
        "        popularities_normalized = popularity_scaler.fit_transform(popularities)\n",
        "        \n",
        "        self.encoders[level]['popularity_scaler'] = popularity_scaler\n",
        "        features_list.append(popularities_normalized)\n",
        "        feature_names.append('popularity_norm')\n",
        "        print(f\"  [4] Popularity feature: 1 dimension (normalized)\")\n",
        "        print(f\"      Range: [{popularities.min():.2f}, {popularities.max():.2f}], mean: {popularities.mean():.2f}\")\n",
        "        \n",
        "        # Characteristics (multi-hot)\n",
        "        characteristics_list = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            char_str = poi_data.get('data', {}).get('characteristic', \n",
        "                    poi_data.get('data', {}).get('characteristics', ''))\n",
        "            tags = self._parse_characteristics(char_str)\n",
        "            characteristics_list.append(tags)\n",
        "        \n",
        "        mlb_chars = MultiLabelBinarizer()\n",
        "        chars_multihot = mlb_chars.fit_transform(characteristics_list).astype(np.float32)\n",
        "        \n",
        "        self.encoders[level]['characteristics_encoder'] = mlb_chars\n",
        "        features_list.append(chars_multihot)\n",
        "        feature_names.extend([f'char_{cls}' for cls in mlb_chars.classes_])\n",
        "        print(f\"  [5] Characteristics features: {len(mlb_chars.classes_)} dimensions (multi-hot)\")\n",
        "\n",
        "    def _build_level1_features(self, pois_at_level: Dict, poi_ids: List[str],\n",
        "                               features_list: List, feature_names: List):\n",
        "        \"\"\"Build Level 1 (Street) specific features\"\"\"\n",
        "        level = 1\n",
        "        \n",
        "        # Category (one-hot)\n",
        "        categories = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            cat = poi_data.get('data', {}).get('category', 'mixed')\n",
        "            categories.append(str(cat).lower().strip())\n",
        "        \n",
        "        category_encoder = LabelEncoder()\n",
        "        category_encoded = category_encoder.fit_transform(categories)\n",
        "        n_categories = len(category_encoder.classes_)\n",
        "        category_onehot = np.eye(n_categories, dtype=np.float32)[category_encoded]\n",
        "        \n",
        "        self.encoders[level]['category_encoder'] = category_encoder\n",
        "        features_list.append(category_onehot)\n",
        "        feature_names.extend([f'category_{cls}' for cls in category_encoder.classes_])\n",
        "        print(f\"  [2] Category features: {n_categories} dimensions (one-hot)\")\n",
        "        \n",
        "        # Number of entities\n",
        "        num_entities = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            count = poi_data.get('data', {}).get('num_entities', 1)\n",
        "            try:\n",
        "                count = int(count)\n",
        "            except (ValueError, TypeError):\n",
        "                count = 1\n",
        "            num_entities.append(count)\n",
        "        \n",
        "        num_entities = np.array(num_entities, dtype=np.float32).reshape(-1, 1)\n",
        "        entities_scaler = StandardScaler()\n",
        "        entities_normalized = entities_scaler.fit_transform(num_entities)\n",
        "        \n",
        "        self.encoders[level]['num_entities_scaler'] = entities_scaler\n",
        "        features_list.append(entities_normalized)\n",
        "        feature_names.append('num_entities_norm')\n",
        "        print(f\"  [3] Num entities feature: 1 dimension (normalized)\")\n",
        "        print(f\"      Range: [{num_entities.min():.0f}, {num_entities.max():.0f}], mean: {num_entities.mean():.1f}\")\n",
        "\n",
        "    def _build_level2_features(self, pois_at_level: Dict, poi_ids: List[str],\n",
        "                               features_list: List, feature_names: List):\n",
        "        \"\"\"Build Level 2 (District) specific features\"\"\"\n",
        "        level = 2\n",
        "        \n",
        "        # Number of Level 1 nodes\n",
        "        num_streets = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            count = poi_data.get('data', {}).get('num_level1_nodes', \n",
        "                    poi_data.get('data', {}).get('num_streets', 1))\n",
        "            try:\n",
        "                count = int(count)\n",
        "            except (ValueError, TypeError):\n",
        "                count = 1\n",
        "            num_streets.append(count)\n",
        "        \n",
        "        num_streets = np.array(num_streets, dtype=np.float32).reshape(-1, 1)\n",
        "        streets_scaler = StandardScaler()\n",
        "        streets_normalized = streets_scaler.fit_transform(num_streets)\n",
        "        \n",
        "        self.encoders[level]['num_level1_nodes_scaler'] = streets_scaler\n",
        "        features_list.append(streets_normalized)\n",
        "        feature_names.append('num_level1_nodes_norm')\n",
        "        print(f\"  [2] Num streets feature: 1 dimension (normalized)\")\n",
        "        print(f\"      Range: [{num_streets.min():.0f}, {num_streets.max():.0f}], mean: {num_streets.mean():.1f}\")\n",
        "\n",
        "    def _build_level3_features(self, pois_at_level: Dict, poi_ids: List[str],\n",
        "                               features_list: List, feature_names: List):\n",
        "        \"\"\"Build Level 3 (Region) specific features\"\"\"\n",
        "        level = 3\n",
        "        \n",
        "        # Number of districts\n",
        "        num_districts = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            count = poi_data.get('data', {}).get('num_districts', \n",
        "                    poi_data.get('data', {}).get('num_level2_nodes', 1))\n",
        "            try:\n",
        "                count = int(count)\n",
        "            except (ValueError, TypeError):\n",
        "                count = 1\n",
        "            num_districts.append(count)\n",
        "        \n",
        "        num_districts = np.array(num_districts, dtype=np.float32).reshape(-1, 1)\n",
        "        districts_scaler = StandardScaler()\n",
        "        districts_normalized = districts_scaler.fit_transform(num_districts)\n",
        "        \n",
        "        self.encoders[level]['num_districts_scaler'] = districts_scaler\n",
        "        features_list.append(districts_normalized)\n",
        "        feature_names.append('num_districts_norm')\n",
        "        print(f\"  [2] Num districts feature: 1 dimension (normalized)\")\n",
        "        print(f\"      Range: [{num_districts.min():.0f}, {num_districts.max():.0f}], mean: {num_districts.mean():.1f}\")\n",
        "\n",
        "    def _extract_textual_features(self, pois_at_level: Dict, poi_ids: List[str],\n",
        "                                  level: int) -> Tuple[np.ndarray, List[str]]:\n",
        "        \"\"\"Extract TF-IDF textual features\"\"\"\n",
        "        texts = []\n",
        "        for poi_id in poi_ids:\n",
        "            poi_data = pois_at_level[poi_id]\n",
        "            text = poi_data.get('textual', '')\n",
        "            if text is None:\n",
        "                text = ''\n",
        "            texts.append(str(text).lower())\n",
        "        \n",
        "        max_text_features = {0: 100, 1: 75, 2: 50, 3: 30}.get(level, 50)\n",
        "        \n",
        "        tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=max_text_features,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            text_features = tfidf_vectorizer.fit_transform(texts).toarray().astype(np.float32)\n",
        "            feature_names = [f'text_{word}' for word in tfidf_vectorizer.get_feature_names_out()]\n",
        "            actual_features = text_features.shape[1]\n",
        "        except ValueError:\n",
        "            print(f\"      Warning: TF-IDF vocabulary empty, using zero features\")\n",
        "            text_features = np.zeros((len(poi_ids), 1), dtype=np.float32)\n",
        "            feature_names = ['text_empty']\n",
        "            actual_features = 1\n",
        "            tfidf_vectorizer = None\n",
        "        \n",
        "        self.encoders[level]['tfidf_vectorizer'] = tfidf_vectorizer\n",
        "        print(f\"  [T] Textual features: {actual_features} dimensions (TF-IDF)\")\n",
        "        \n",
        "        return text_features, feature_names\n",
        "\n",
        "    def _parse_price(self, price_str) -> float:\n",
        "        \"\"\"Parse price string to float value\"\"\"\n",
        "        if price_str is None or price_str == '' or pd.isna(price_str):\n",
        "            return 25.0\n",
        "        \n",
        "        try:\n",
        "            price_str = str(price_str).strip()\n",
        "            \n",
        "            if '-' in price_str:\n",
        "                parts = price_str.split('-')\n",
        "                if len(parts) == 2:\n",
        "                    low = float(parts[0].strip())\n",
        "                    high = float(parts[1].strip())\n",
        "                    return (low + high) / 2\n",
        "            \n",
        "            return float(price_str)\n",
        "        \n",
        "        except (ValueError, TypeError):\n",
        "            return 25.0\n",
        "\n",
        "    def _parse_characteristics(self, char_str) -> List[str]:\n",
        "        \"\"\"Parse characteristics string to list of tags\"\"\"\n",
        "        if char_str is None or char_str == '' or pd.isna(char_str):\n",
        "            return []\n",
        "        \n",
        "        char_str = str(char_str)\n",
        "        \n",
        "        if ',' in char_str:\n",
        "            parts = char_str.split(',')\n",
        "        elif ';' in char_str:\n",
        "            parts = char_str.split(';')\n",
        "        else:\n",
        "            parts = [char_str]\n",
        "        \n",
        "        tags = []\n",
        "        for part in parts:\n",
        "            tag = part.strip().replace('#', '').lower()\n",
        "            if tag and len(tag) > 1:\n",
        "                tags.append(tag)\n",
        "        \n",
        "        return tags\n",
        "    \n",
        "    def build_Y_T_level(self, level: int, embedding_dim: int = 32) -> Tuple[np.ndarray, List[str]]:\n",
        "        \"\"\"Build derived POI attribute matrix\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Building Y_T^{level}: Derived POI Attribute Matrix (Level {level})\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        level_names = {0: 'Building', 1: 'Street', 2: 'District', 3: 'Region'}\n",
        "        print(f\"Granularity: {level_names.get(level, 'Unknown')}\")\n",
        "        \n",
        "        level_key = f'level_{level}'\n",
        "        pois_at_level = self.poi_tree[level_key]\n",
        "\n",
        "        # Sort POI IDs by metadata.pkl idx order (fallback to default)\n",
        "        if hasattr(self, \"meta_idx_to_poi\") and level in self.meta_idx_to_poi:\n",
        "            idx2poi_level = self.meta_idx_to_poi[level]\n",
        "            poi_ids = [idx2poi_level[i] for i in sorted(idx2poi_level.keys()) if idx2poi_level[i] in pois_at_level]\n",
        "        else:\n",
        "            poi_ids = list(pois_at_level.keys())\n",
        "        \n",
        "        n_pois = len(poi_ids)\n",
        "        n_users = len(self.users_df)\n",
        "        \n",
        "        print(f\"Number of POIs at level {level}: {n_pois}\")\n",
        "        print(f\"Number of users: {n_users}\")\n",
        "        \n",
        "        # Create index mappings\n",
        "        poi_to_idx = {pid: idx for idx, pid in enumerate(poi_ids)}\n",
        "        user_id_col = 'uuid' if 'uuid' in self.users_df.columns else 'uudi'\n",
        "        user_to_idx = {uid: idx for idx, uid in enumerate(self.users_df[user_id_col])}\n",
        "        \n",
        "        # Step 1: Build User Preference Features\n",
        "        print(f\"\\n  [Step 1] Building user preference features...\")\n",
        "        user_pref_features, user_pref_names = self._build_user_preference_features()\n",
        "        print(f\"    User preference matrix shape: {user_pref_features.shape}\")\n",
        "        \n",
        "        # Step 2: Build POI-User Interaction Matrix\n",
        "        print(f\"\\n  [Step 2] Building POI-User interaction matrix...\")\n",
        "        interaction_matrix, interaction_stats = self._build_poi_user_interaction_matrix(\n",
        "            poi_ids, poi_to_idx, user_to_idx, level\n",
        "        )\n",
        "        print(f\"    Interaction matrix shape: {interaction_matrix.shape}\")\n",
        "        print(f\"    Density: {interaction_stats['density']:.2f}%\")\n",
        "        print(f\"    Total interactions: {interaction_stats['total_interactions']}\")\n",
        "        \n",
        "        # Step 3: Derive POI features\n",
        "        print(f\"\\n  [Step 3] Deriving POI features from user preferences...\")\n",
        "        derived_features = []\n",
        "        derived_names = []\n",
        "        \n",
        "        poi_user_pref_agg = self._aggregate_user_preferences_to_pois(\n",
        "            interaction_matrix, user_pref_features, user_pref_names\n",
        "        )\n",
        "        derived_features.append(poi_user_pref_agg['features'])\n",
        "        derived_names.extend(poi_user_pref_agg['names'])\n",
        "        print(f\"    Aggregated user preferences: {poi_user_pref_agg['features'].shape[1]} dims\")\n",
        "        \n",
        "        diversity_features = self._compute_user_diversity_features(\n",
        "            interaction_matrix, user_pref_features\n",
        "        )\n",
        "        derived_features.append(diversity_features['features'])\n",
        "        derived_names.extend(diversity_features['names'])\n",
        "        print(f\"    User diversity features: {diversity_features['features'].shape[1]} dims\")\n",
        "        \n",
        "        pattern_features = self._compute_interaction_pattern_features(\n",
        "            poi_ids, poi_to_idx, level\n",
        "        )\n",
        "        derived_features.append(pattern_features['features'])\n",
        "        derived_names.extend(pattern_features['names'])\n",
        "        print(f\"    Interaction pattern features: {pattern_features['features'].shape[1]} dims\")\n",
        "        \n",
        "        # Step 4: Matrix Factorization\n",
        "        print(f\"\\n  [Step 4] Computing latent embeddings via NMF...\")\n",
        "        latent_embeddings = self._compute_latent_embeddings(\n",
        "            interaction_matrix, embedding_dim\n",
        "        )\n",
        "        derived_features.append(latent_embeddings['features'])\n",
        "        derived_names.extend(latent_embeddings['names'])\n",
        "        print(f\"    Latent embeddings: {latent_embeddings['features'].shape[1]} dims\")\n",
        "        \n",
        "        # Step 5: Concatenate all features\n",
        "        Y_T_level = np.hstack(derived_features)\n",
        "        \n",
        "        final_scaler = StandardScaler()\n",
        "        Y_T_level = final_scaler.fit_transform(Y_T_level)\n",
        "        self.encoders[level]['Y_T_scaler'] = final_scaler\n",
        "\n",
        "        return Y_T_level, poi_ids\n",
        "\n",
        "    def _build_user_preference_features(self) -> Tuple[np.ndarray, List[str]]:\n",
        "        \"\"\"Build user preference features\"\"\"\n",
        "        features_list = []\n",
        "        feature_names = []\n",
        "        \n",
        "        user_id_col = 'uuid' if 'uuid' in self.users_df.columns else 'uudi'\n",
        "        \n",
        "        # 1. Interests (multi-hot)\n",
        "        interests_list = []\n",
        "        for _, row in self.users_df.iterrows():\n",
        "            interests_str = row.get('interests', '')\n",
        "            if pd.isna(interests_str) or interests_str == '':\n",
        "                interests = []\n",
        "            else:\n",
        "                interests = [i.strip().lower() for i in str(interests_str).split(';')]\n",
        "            interests_list.append(interests)\n",
        "        \n",
        "        mlb_interests = MultiLabelBinarizer()\n",
        "        interests_encoded = mlb_interests.fit_transform(interests_list).astype(np.float32)\n",
        "        \n",
        "        self.encoders['user_interests_encoder'] = mlb_interests\n",
        "        features_list.append(interests_encoded)\n",
        "        feature_names.extend([f'interest_{cls}' for cls in mlb_interests.classes_])\n",
        "        \n",
        "        # 2. Age group (one-hot)\n",
        "        age_groups = self.users_df['age_group'].fillna('unknown').astype(str).str.lower().tolist()\n",
        "        age_encoder = LabelEncoder()\n",
        "        age_encoded = age_encoder.fit_transform(age_groups)\n",
        "        age_onehot = np.eye(len(age_encoder.classes_), dtype=np.float32)[age_encoded]\n",
        "        \n",
        "        self.encoders['user_age_encoder'] = age_encoder\n",
        "        features_list.append(age_onehot)\n",
        "        feature_names.extend([f'age_{cls}' for cls in age_encoder.classes_])\n",
        "        \n",
        "        # 3. Price sensitivity (one-hot)\n",
        "        price_sens = self.users_df['price_sensitivity'].fillna('medium').astype(str).str.lower().tolist()\n",
        "        price_encoder = LabelEncoder()\n",
        "        price_encoded = price_encoder.fit_transform(price_sens)\n",
        "        price_onehot = np.eye(len(price_encoder.classes_), dtype=np.float32)[price_encoded]\n",
        "        \n",
        "        self.encoders['user_price_encoder'] = price_encoder\n",
        "        features_list.append(price_onehot)\n",
        "        feature_names.extend([f'price_sens_{cls}' for cls in price_encoder.classes_])\n",
        "        \n",
        "        user_features = np.hstack(features_list)\n",
        "        \n",
        "        return user_features, feature_names\n",
        "\n",
        "    def _build_poi_user_interaction_matrix(\n",
        "        self, \n",
        "        poi_ids: List[str], \n",
        "        poi_to_idx: Dict[str, int],\n",
        "        user_to_idx: Dict[str, int],\n",
        "        level: int\n",
        "    ) -> Tuple[csr_matrix, Dict]:\n",
        "        \"\"\"Build POI-User interaction matrix\"\"\"\n",
        "        n_pois = len(poi_ids)\n",
        "        n_users = len(user_to_idx)\n",
        "        \n",
        "        poi_user_scores = {}\n",
        "        \n",
        "        interaction_weights = {\n",
        "            'visit': 1.0,\n",
        "            'rating': 0.8,\n",
        "            'search': 0.3,\n",
        "            'click': 0.2,\n",
        "            'bookmark': 0.5\n",
        "        }\n",
        "        \n",
        "        total_interactions = 0\n",
        "        \n",
        "        for _, row in self.interactions_df.iterrows():\n",
        "            user_id = row['user_id']\n",
        "            poi_id = row['poi_id']\n",
        "            \n",
        "            if level > 0:\n",
        "                mapped_poi_id = self._get_parent_at_level(poi_id, target_level=level)\n",
        "            else:\n",
        "                mapped_poi_id = poi_id\n",
        "            \n",
        "            if mapped_poi_id not in poi_to_idx or user_id not in user_to_idx:\n",
        "                continue\n",
        "            \n",
        "            key = (mapped_poi_id, user_id)\n",
        "            interaction_type = row.get('interaction_type', 'visit')\n",
        "            \n",
        "            base_weight = interaction_weights.get(interaction_type, 0.5)\n",
        "            \n",
        "            if interaction_type == 'rating':\n",
        "                rating_value = row.get('value', 3)\n",
        "                score = base_weight * (rating_value / 5.0)\n",
        "            else:\n",
        "                score = base_weight * row.get('value', 1)\n",
        "            \n",
        "            poi_user_scores[key] = poi_user_scores.get(key, 0) + score\n",
        "            total_interactions += 1\n",
        "        \n",
        "        row_indices = []\n",
        "        col_indices = []\n",
        "        values = []\n",
        "        \n",
        "        for (poi_id, user_id), score in poi_user_scores.items():\n",
        "            row_indices.append(poi_to_idx[poi_id])\n",
        "            col_indices.append(user_to_idx[user_id])\n",
        "            values.append(score)\n",
        "        \n",
        "        interaction_matrix = csr_matrix(\n",
        "            (values, (row_indices, col_indices)),\n",
        "            shape=(n_pois, n_users),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        density = interaction_matrix.nnz / (n_pois * n_users) * 100 if n_pois * n_users > 0 else 0\n",
        "        \n",
        "        stats = {\n",
        "            'density': density,\n",
        "            'total_interactions': total_interactions,\n",
        "            'nnz': interaction_matrix.nnz,\n",
        "            'unique_poi_user_pairs': len(poi_user_scores)\n",
        "        }\n",
        "        \n",
        "        return interaction_matrix, stats\n",
        "\n",
        "    def _aggregate_user_preferences_to_pois(\n",
        "        self,\n",
        "        interaction_matrix: csr_matrix,\n",
        "        user_pref_features: np.ndarray,\n",
        "        user_pref_names: List[str]\n",
        "    ) -> Dict:\n",
        "        \"\"\"Aggregate user preferences to POIs\"\"\"\n",
        "        weighted_sum = interaction_matrix.dot(user_pref_features)\n",
        "        \n",
        "        weight_sums = np.array(interaction_matrix.sum(axis=1)).flatten()\n",
        "        weight_sums[weight_sums == 0] = 1.0\n",
        "        \n",
        "        aggregated_features = weighted_sum / weight_sums.reshape(-1, 1)\n",
        "        \n",
        "        no_interaction_mask = np.array(interaction_matrix.sum(axis=1)).flatten() == 0\n",
        "        if no_interaction_mask.any():\n",
        "            global_avg = user_pref_features.mean(axis=0)\n",
        "            aggregated_features[no_interaction_mask] = global_avg\n",
        "        \n",
        "        aggregated_names = [f'agg_user_{name}' for name in user_pref_names]\n",
        "        \n",
        "        return {\n",
        "            'features': aggregated_features.astype(np.float32),\n",
        "            'names': aggregated_names\n",
        "        }\n",
        "\n",
        "    def _compute_user_diversity_features(\n",
        "        self,\n",
        "        interaction_matrix: csr_matrix,\n",
        "        user_pref_features: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"Compute user diversity features\"\"\"\n",
        "        n_pois = interaction_matrix.shape[0]\n",
        "        \n",
        "        diversity_features = []\n",
        "        \n",
        "        # 1. User count\n",
        "        user_counts = np.array(interaction_matrix.getnnz(axis=1)).reshape(-1, 1).astype(np.float32)\n",
        "        diversity_features.append(user_counts)\n",
        "        \n",
        "        # 2. Interaction strength variance\n",
        "        interaction_variance = []\n",
        "        for i in range(n_pois):\n",
        "            row = interaction_matrix.getrow(i).toarray().flatten()\n",
        "            nonzero_values = row[row > 0]\n",
        "            if len(nonzero_values) > 1:\n",
        "                variance = np.var(nonzero_values)\n",
        "            else:\n",
        "                variance = 0.0\n",
        "            interaction_variance.append(variance)\n",
        "        interaction_variance = np.array(interaction_variance, dtype=np.float32).reshape(-1, 1)\n",
        "        diversity_features.append(interaction_variance)\n",
        "        \n",
        "        # 3. User preference diversity\n",
        "        pref_diversity = []\n",
        "        for i in range(n_pois):\n",
        "            row = interaction_matrix.getrow(i)\n",
        "            user_indices = row.indices\n",
        "            \n",
        "            if len(user_indices) > 1:\n",
        "                poi_user_prefs = user_pref_features[user_indices]\n",
        "                if poi_user_prefs.shape[0] > 1:\n",
        "                    norms = np.linalg.norm(poi_user_prefs, axis=1, keepdims=True)\n",
        "                    norms[norms == 0] = 1.0\n",
        "                    normalized_prefs = poi_user_prefs / norms\n",
        "                    similarity_matrix = normalized_prefs @ normalized_prefs.T\n",
        "                    n = similarity_matrix.shape[0]\n",
        "                    mean_similarity = (similarity_matrix.sum() - n) / (n * (n - 1)) if n > 1 else 1.0\n",
        "                    diversity = 1.0 - mean_similarity\n",
        "                else:\n",
        "                    diversity = 0.0\n",
        "            else:\n",
        "                diversity = 0.0\n",
        "            \n",
        "            pref_diversity.append(diversity)\n",
        "        \n",
        "        pref_diversity = np.array(pref_diversity, dtype=np.float32).reshape(-1, 1)\n",
        "        diversity_features.append(pref_diversity)\n",
        "        \n",
        "        all_diversity = np.hstack(diversity_features)\n",
        "        \n",
        "        return {\n",
        "            'features': all_diversity,\n",
        "            'names': ['user_count', 'interaction_variance', 'user_pref_diversity']\n",
        "        }\n",
        "\n",
        "    def _compute_interaction_pattern_features(\n",
        "        self,\n",
        "        poi_ids: List[str],\n",
        "        poi_to_idx: Dict[str, int],\n",
        "        level: int\n",
        "    ) -> Dict:\n",
        "        \"\"\"Compute interaction pattern features\"\"\"\n",
        "        n_pois = len(poi_ids)\n",
        "        \n",
        "        poi_stats = {pid: {\n",
        "            'visits': 0,\n",
        "            'ratings': [],\n",
        "            'searches': 0,\n",
        "            'total': 0,\n",
        "            'user_visits': {}\n",
        "        } for pid in poi_ids}\n",
        "        \n",
        "        for _, row in self.interactions_df.iterrows():\n",
        "            poi_id = row['poi_id']\n",
        "            user_id = row['user_id']\n",
        "            \n",
        "            if level > 0:\n",
        "                poi_id = self._get_parent_at_level(poi_id, target_level=level)\n",
        "            \n",
        "            if poi_id not in poi_stats:\n",
        "                continue\n",
        "            \n",
        "            stats = poi_stats[poi_id]\n",
        "            stats['total'] += 1\n",
        "            \n",
        "            interaction_type = row.get('interaction_type', 'visit')\n",
        "            \n",
        "            if interaction_type == 'visit':\n",
        "                stats['visits'] += 1\n",
        "                stats['user_visits'][user_id] = stats['user_visits'].get(user_id, 0) + 1\n",
        "            elif interaction_type == 'rating':\n",
        "                stats['ratings'].append(row.get('value', 3))\n",
        "            elif interaction_type == 'search':\n",
        "                stats['searches'] += 1\n",
        "        \n",
        "        visit_ratios = []\n",
        "        avg_ratings = []\n",
        "        search_to_visit_ratios = []\n",
        "        repeat_visitor_ratios = []\n",
        "        \n",
        "        for poi_id in poi_ids:\n",
        "            stats = poi_stats[poi_id]\n",
        "            \n",
        "            visit_ratio = stats['visits'] / stats['total'] if stats['total'] > 0 else 0.5\n",
        "            visit_ratios.append(visit_ratio)\n",
        "            \n",
        "            avg_rating = np.mean(stats['ratings']) if stats['ratings'] else 3.0\n",
        "            avg_ratings.append(avg_rating)\n",
        "            \n",
        "            if stats['searches'] > 0:\n",
        "                s2v_ratio = min(stats['visits'] / stats['searches'], 2.0)\n",
        "            else:\n",
        "                s2v_ratio = 1.0\n",
        "            search_to_visit_ratios.append(s2v_ratio)\n",
        "            \n",
        "            if stats['user_visits']:\n",
        "                repeat_count = sum(1 for v in stats['user_visits'].values() if v > 1)\n",
        "                repeat_ratio = repeat_count / len(stats['user_visits'])\n",
        "            else:\n",
        "                repeat_ratio = 0.0\n",
        "            repeat_visitor_ratios.append(repeat_ratio)\n",
        "        \n",
        "        pattern_features = np.column_stack([\n",
        "            visit_ratios,\n",
        "            avg_ratings,\n",
        "            search_to_visit_ratios,\n",
        "            repeat_visitor_ratios\n",
        "        ]).astype(np.float32)\n",
        "        \n",
        "        return {\n",
        "            'features': pattern_features,\n",
        "            'names': ['visit_ratio', 'avg_rating', 'search_to_visit_ratio', 'repeat_visitor_ratio']\n",
        "        }\n",
        "\n",
        "    def _compute_latent_embeddings(\n",
        "        self,\n",
        "        interaction_matrix: csr_matrix,\n",
        "        embedding_dim: int\n",
        "    ) -> Dict:\n",
        "        \"\"\"Compute latent embeddings via NMF\"\"\"\n",
        "        if interaction_matrix.nnz == 0:\n",
        "            n_pois = interaction_matrix.shape[0]\n",
        "            return {\n",
        "                'features': np.zeros((n_pois, embedding_dim), dtype=np.float32),\n",
        "                'names': [f'latent_{i}' for i in range(embedding_dim)],\n",
        "                'reconstruction_error': 0.0\n",
        "            }\n",
        "        \n",
        "        interaction_dense = interaction_matrix.toarray()\n",
        "        interaction_dense = np.maximum(interaction_dense, 0)\n",
        "        \n",
        "        actual_dim = min(embedding_dim, min(interaction_dense.shape) - 1)\n",
        "        actual_dim = max(actual_dim, 1)\n",
        "        \n",
        "        nmf = NMF(\n",
        "            n_components=actual_dim,\n",
        "            init='nndsvda',\n",
        "            random_state=42,\n",
        "            max_iter=300,\n",
        "            l1_ratio=0.5,\n",
        "            alpha_W=0.1,\n",
        "            alpha_H=0.1\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            latent_features = nmf.fit_transform(interaction_dense)\n",
        "            reconstruction_error = nmf.reconstruction_err_\n",
        "        except Exception as e:\n",
        "            print(f\"    Warning: NMF failed ({e}), using SVD fallback\")\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "            svd = TruncatedSVD(n_components=actual_dim, random_state=42)\n",
        "            latent_features = svd.fit_transform(interaction_matrix)\n",
        "            latent_features = np.maximum(latent_features, 0)\n",
        "            reconstruction_error = 0.0\n",
        "        \n",
        "        if actual_dim < embedding_dim:\n",
        "            padding = np.zeros((latent_features.shape[0], embedding_dim - actual_dim), dtype=np.float32)\n",
        "            latent_features = np.hstack([latent_features, padding])\n",
        "        \n",
        "        return {\n",
        "            'features': latent_features.astype(np.float32),\n",
        "            'names': [f'latent_{i}' for i in range(embedding_dim)],\n",
        "            'reconstruction_error': reconstruction_error\n",
        "        }\n",
        "\n",
        "    def _get_parent_at_level(self, poi_id: str, target_level: int) -> str:\n",
        "        \"\"\"Get parent node at target level\"\"\"\n",
        "        if target_level == 0:\n",
        "            return poi_id\n",
        "        \n",
        "        current_level = 0\n",
        "        current_id = poi_id\n",
        "        \n",
        "        while current_level < target_level:\n",
        "            level_key = f'level_{current_level}'\n",
        "            \n",
        "            if level_key not in self.poi_tree:\n",
        "                break\n",
        "            \n",
        "            if current_id not in self.poi_tree[level_key]:\n",
        "                break\n",
        "            \n",
        "            parent = self.poi_tree[level_key][current_id].get('parent')\n",
        "            \n",
        "            if parent:\n",
        "                current_id = parent\n",
        "                current_level += 1\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        return current_id\n",
        "\n",
        "    def build_poi_embeddings(self, levels: List[int] = [0, 1, 2, 3]):\n",
        "        \"\"\"Build POI embeddings for all levels\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Building Complete POI Embeddings (All Levels)\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        level_names = {0: 'Building', 1: 'Street', 2: 'District', 3: 'Region'}\n",
        "        \n",
        "        for level in levels:\n",
        "            print(f\"\\n{'#' * 60}\")\n",
        "            print(f\"### Processing Level {level}: {level_names.get(level, 'Unknown')} ###\")\n",
        "            print(f\"{'#' * 60}\")\n",
        "            \n",
        "            Y_A_l, poi_ids_A, Y_A_feature_names = self.build_Y_A_level(level)\n",
        "            Y_T_l, poi_ids_T = self.build_Y_T_level(level, embedding_dim=32)\n",
        "            \n",
        "            if poi_ids_A != poi_ids_T:\n",
        "                print(f\"  Warning: POI ID mismatch between Y_A and Y_T\")\n",
        "                poi_ids = poi_ids_A\n",
        "            else:\n",
        "                poi_ids = poi_ids_A\n",
        "            \n",
        "            Y_T_feature_names = self._get_Y_T_feature_names(Y_T_l.shape[1])\n",
        "            \n",
        "            Y_l = np.hstack([Y_A_l, Y_T_l])\n",
        "            \n",
        "            all_feature_names = Y_A_feature_names + Y_T_feature_names\n",
        "            \n",
        "            print(f\"\\n{'=' * 60}\")\n",
        "            print(f\"Final POI Embedding Summary - Level {level}\")\n",
        "            print(f\"{'=' * 60}\")\n",
        "            print(f\"  Number of POIs: {len(poi_ids)}\")\n",
        "            print(f\"  Y_A^{level} dimensions: {Y_A_l.shape[1]}\")\n",
        "            print(f\"  Y_T^{level} dimensions: {Y_T_l.shape[1]}\")\n",
        "            print(f\"  Total embedding dimensions: {Y_l.shape[1]}\")\n",
        "            \n",
        "            self.poi_embeddings[f'level_{level}'] = {\n",
        "                'embeddings': Y_l,\n",
        "                'poi_ids': poi_ids,\n",
        "                'Y_A': Y_A_l,\n",
        "                'Y_T': Y_T_l,\n",
        "                'Y_A_feature_names': Y_A_feature_names,\n",
        "                'Y_T_feature_names': Y_T_feature_names,\n",
        "                'all_feature_names': all_feature_names,\n",
        "                'n_explicit_features': Y_A_l.shape[1],\n",
        "                'n_derived_features': Y_T_l.shape[1],\n",
        "                'level_name': level_names.get(level, 'Unknown')\n",
        "            }\n",
        "\n",
        "    def _get_Y_T_feature_names(self, n_features: int) -> List[str]:\n",
        "        \"\"\"Generate Y_T feature names\"\"\"\n",
        "        feature_names = []\n",
        "        \n",
        "        if 'user_interests_encoder' in self.encoders:\n",
        "            interest_classes = self.encoders['user_interests_encoder'].classes_\n",
        "            feature_names.extend([f'agg_user_interest_{cls}' for cls in interest_classes])\n",
        "        \n",
        "        if 'user_age_encoder' in self.encoders:\n",
        "            age_classes = self.encoders['user_age_encoder'].classes_\n",
        "            feature_names.extend([f'agg_user_age_{cls}' for cls in age_classes])\n",
        "        \n",
        "        if 'user_price_encoder' in self.encoders:\n",
        "            price_classes = self.encoders['user_price_encoder'].classes_\n",
        "            feature_names.extend([f'agg_user_price_sens_{cls}' for cls in price_classes])\n",
        "        \n",
        "        feature_names.extend(['user_count', 'interaction_variance', 'user_pref_diversity'])\n",
        "        \n",
        "        feature_names.extend(['visit_ratio', 'avg_rating', 'search_to_visit_ratio', 'repeat_visitor_ratio'])\n",
        "        \n",
        "        current_count = len(feature_names)\n",
        "        latent_count = n_features - current_count\n",
        "        if latent_count > 0:\n",
        "            feature_names.extend([f'latent_{i}' for i in range(latent_count)])\n",
        "        \n",
        "        return feature_names[:n_features]\n",
        "        \n",
        "    def save_embeddings(self, output_file: str = 'poi_embeddings.pkl'):\n",
        "        \"\"\"Save embeddings\"\"\"\n",
        "        # If output_file is relative, resolve against self.sources_dir\n",
        "        out_path = Path(output_file)\n",
        "        if not out_path.is_absolute():\n",
        "            out_path = self.sources_dir / out_path\n",
        "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"\\nSaving embeddings to: {out_path}\")\n",
        "        \n",
        "        embedding_dims = {}\n",
        "        for level_key, level_data in self.poi_embeddings.items():\n",
        "            embedding_dims[level_key] = {\n",
        "                'total': level_data['embeddings'].shape[1],\n",
        "                'explicit': level_data['n_explicit_features'],\n",
        "                'derived': level_data['n_derived_features']\n",
        "            }\n",
        "        \n",
        "        poi_id_to_idx = {}\n",
        "        for level_key, level_data in self.poi_embeddings.items():\n",
        "            poi_id_to_idx[level_key] = {\n",
        "                pid: idx for idx, pid in enumerate(level_data['poi_ids'])\n",
        "            }\n",
        "        \n",
        "        feature_names_map = {}\n",
        "        for level_key, level_data in self.poi_embeddings.items():\n",
        "            feature_names_map[level_key] = {\n",
        "                'all': level_data['all_feature_names'],\n",
        "                'explicit': level_data['Y_A_feature_names'],\n",
        "                'derived': level_data['Y_T_feature_names']\n",
        "            }\n",
        "        \n",
        "        save_data = {\n",
        "            'poi_embeddings': self.poi_embeddings,\n",
        "            'encoders': self.encoders,\n",
        "            'poi_tree': self.poi_tree,\n",
        "            'metadata': {\n",
        "                'levels': list(self.poi_embeddings.keys()),\n",
        "                'n_users': len(self.users_df),\n",
        "                'n_interactions': len(self.interactions_df),\n",
        "                'created_at': pd.Timestamp.now().isoformat(),\n",
        "                'embedding_dimensions': embedding_dims\n",
        "            },\n",
        "            'poi_id_to_idx': poi_id_to_idx,\n",
        "            'feature_names': feature_names_map\n",
        "        }\n",
        "        \n",
        "        with out_path.open('wb') as f:\n",
        "            pickle.dump(save_data, f)\n",
        "        \n",
        "        file_size = os.path.getsize(out_path) / (1024 * 1024)\n",
        "        print(f\"  File size: {file_size:.2f} MB\")\n",
        "        print(f\"  Levels saved: {list(self.poi_embeddings.keys())}\")\n",
        "        print(\"  Save complete!\")\n",
        "\n",
        "    def load_embeddings(self, input_file: str = 'poi_embeddings.pkl'):\n",
        "        \"\"\"Load embeddings\"\"\"\n",
        "        in_path = Path(input_file)\n",
        "        if not in_path.is_absolute():\n",
        "            in_path = self.sources_dir / in_path\n",
        "        print(f\"\\nLoading embeddings from: {in_path}\")\n",
        "        \n",
        "        with in_path.open('rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        \n",
        "        self.poi_embeddings = data['poi_embeddings']\n",
        "        self.encoders = data['encoders']\n",
        "        self.poi_tree = data['poi_tree']\n",
        "        \n",
        "        print(f\"  Loaded {len(self.poi_embeddings)} levels\")\n",
        "        print(f\"  Loaded {len(self.encoders)} encoders\")\n",
        "        print(f\"  Created at: {data['metadata']['created_at']}\")\n",
        "        \n",
        "        return data\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "08a0c745",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_preferences_file: C:\\Users\\syoon\\SpatiaLynk_recommender\\Attribute-based representation learning\\POI_Embeddings\\Sources\\user_preferences.csv False\n",
            "user_poi_interactions_file: C:\\Users\\syoon\\SpatiaLynk_recommender\\Attribute-based representation learning\\POI_Embeddings\\Sources\\user_poi_interactions.csv False\n",
            "poi_tree_file: C:\\Users\\syoon\\SpatiaLynk_recommender\\Attribute-based representation learning\\POI_Embeddings\\Sources\\poi_tree_with_uuids.json False\n",
            "[POIEmbeddings] repo_root   = C:\\Users\\syoon\\SpatiaLynk_recommender\n",
            "[POIEmbeddings] sources_dir = C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\n",
            "[POIEmbeddings] users_file  = C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\user_preferences.csv | exists: True\n",
            "[POIEmbeddings] interactions_file = C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\user_poi_interactions.csv | exists: True\n",
            "[POIEmbeddings] poi_tree_file = C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\poi_tree_with_uuids.json | exists: True\n",
            "[POIEmbeddings] loaded metadata.pkl from C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\metadata.pkl\n",
            "\n",
            "============================================================\n",
            "Building Complete POI Embeddings (All Levels)\n",
            "============================================================\n",
            "\n",
            "############################################################\n",
            "### Processing Level 0: Building ###\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Building Y_A^0: Direct POI Attribute Matrix (Level 0)\n",
            "============================================================\n",
            "Granularity: Building\n",
            "Number of nodes at level 0: 4696\n",
            "  [1] Spatial features: 2 dimensions (lat, lon normalized)\n",
            "  [2] Category features: 23 dimensions (one-hot)\n",
            "  [3] Price feature: 1 dimension (normalized)\n",
            "      Range: [4.20, 57.15], mean: 29.99\n",
            "  [4] Popularity feature: 1 dimension (normalized)\n",
            "      Range: [1.00, 5.00], mean: 2.99\n",
            "  [5] Characteristics features: 22 dimensions (multi-hot)\n",
            "  [T] Textual features: 100 dimensions (TF-IDF)\n",
            "\n",
            "============================================================\n",
            "Building Y_T^0: Derived POI Attribute Matrix (Level 0)\n",
            "============================================================\n",
            "Granularity: Building\n",
            "Number of POIs at level 0: 4696\n",
            "Number of users: 21\n",
            "\n",
            "  [Step 1] Building user preference features...\n",
            "    User preference matrix shape: (21, 33)\n",
            "\n",
            "  [Step 2] Building POI-User interaction matrix...\n",
            "    Interaction matrix shape: (4696, 21)\n",
            "    Density: 0.26%\n",
            "    Total interactions: 567\n",
            "\n",
            "  [Step 3] Deriving POI features from user preferences...\n",
            "    Aggregated user preferences: 33 dims\n",
            "    User diversity features: 3 dims\n",
            "    Interaction pattern features: 4 dims\n",
            "\n",
            "  [Step 4] Computing latent embeddings via NMF...\n",
            "    Latent embeddings: 32 dims\n",
            "\n",
            "============================================================\n",
            "Final POI Embedding Summary - Level 0\n",
            "============================================================\n",
            "  Number of POIs: 4696\n",
            "  Y_A^0 dimensions: 149\n",
            "  Y_T^0 dimensions: 72\n",
            "  Total embedding dimensions: 221\n",
            "\n",
            "############################################################\n",
            "### Processing Level 1: Street ###\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Building Y_A^1: Direct POI Attribute Matrix (Level 1)\n",
            "============================================================\n",
            "Granularity: Street\n",
            "Number of nodes at level 1: 1355\n",
            "  [1] Spatial features: 2 dimensions (lat, lon normalized)\n",
            "  [2] Category features: 21 dimensions (one-hot)\n",
            "  [3] Num entities feature: 1 dimension (normalized)\n",
            "      Range: [0, 561], mean: 3.5\n",
            "  [T] Textual features: 75 dimensions (TF-IDF)\n",
            "\n",
            "============================================================\n",
            "Building Y_T^1: Derived POI Attribute Matrix (Level 1)\n",
            "============================================================\n",
            "Granularity: Street\n",
            "Number of POIs at level 1: 1355\n",
            "Number of users: 21\n",
            "\n",
            "  [Step 1] Building user preference features...\n",
            "    User preference matrix shape: (21, 33)\n",
            "\n",
            "  [Step 2] Building POI-User interaction matrix...\n",
            "    Interaction matrix shape: (1355, 21)\n",
            "    Density: 0.81%\n",
            "    Total interactions: 567\n",
            "\n",
            "  [Step 3] Deriving POI features from user preferences...\n",
            "    Aggregated user preferences: 33 dims\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.8.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.17.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\syoon\\spatialynk_recommender\\.venv\\lib\\site-packages (from scikit-learn) (2.4.1)\n",
            "Collecting joblib>=1.3.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.8.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
            "Using cached scipy-1.17.0-cp313-cp313-win_amd64.whl (36.3 MB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   ---------- ----------------------------- 1/4 [scipy]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [joblib]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ------------------------------ --------- 3/4 [scikit-learn]\n",
            "   ---------------------------------------- 4/4 [scikit-learn]\n",
            "\n",
            "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
            "    User diversity features: 3 dims\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Interaction pattern features: 4 dims\n",
            "\n",
            "  [Step 4] Computing latent embeddings via NMF...\n",
            "    Latent embeddings: 32 dims\n",
            "\n",
            "============================================================\n",
            "Final POI Embedding Summary - Level 1\n",
            "============================================================\n",
            "  Number of POIs: 1355\n",
            "  Y_A^1 dimensions: 99\n",
            "  Y_T^1 dimensions: 72\n",
            "  Total embedding dimensions: 171\n",
            "\n",
            "############################################################\n",
            "### Processing Level 2: District ###\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Building Y_A^2: Direct POI Attribute Matrix (Level 2)\n",
            "============================================================\n",
            "Granularity: District\n",
            "Number of nodes at level 2: 44\n",
            "  [1] Spatial features: 2 dimensions (lat, lon normalized)\n",
            "  [2] Num streets feature: 1 dimension (normalized)\n",
            "      Range: [1, 125], mean: 30.8\n",
            "  [T] Textual features: 50 dimensions (TF-IDF)\n",
            "\n",
            "============================================================\n",
            "Building Y_T^2: Derived POI Attribute Matrix (Level 2)\n",
            "============================================================\n",
            "Granularity: District\n",
            "Number of POIs at level 2: 44\n",
            "Number of users: 21\n",
            "\n",
            "  [Step 1] Building user preference features...\n",
            "    User preference matrix shape: (21, 33)\n",
            "\n",
            "  [Step 2] Building POI-User interaction matrix...\n",
            "    Interaction matrix shape: (44, 21)\n",
            "    Density: 19.48%\n",
            "    Total interactions: 567\n",
            "\n",
            "  [Step 3] Deriving POI features from user preferences...\n",
            "    Aggregated user preferences: 33 dims\n",
            "    User diversity features: 3 dims\n",
            "    Interaction pattern features: 4 dims\n",
            "\n",
            "  [Step 4] Computing latent embeddings via NMF...\n",
            "    Latent embeddings: 32 dims\n",
            "\n",
            "============================================================\n",
            "Final POI Embedding Summary - Level 2\n",
            "============================================================\n",
            "  Number of POIs: 44\n",
            "  Y_A^2 dimensions: 53\n",
            "  Y_T^2 dimensions: 72\n",
            "  Total embedding dimensions: 125\n",
            "\n",
            "############################################################\n",
            "### Processing Level 3: Region ###\n",
            "############################################################\n",
            "\n",
            "============================================================\n",
            "Building Y_A^3: Direct POI Attribute Matrix (Level 3)\n",
            "============================================================\n",
            "Granularity: Region\n",
            "Number of nodes at level 3: 5\n",
            "  [1] Spatial features: 2 dimensions (lat, lon normalized)\n",
            "  [2] Num districts feature: 1 dimension (normalized)\n",
            "      Range: [5, 20], mean: 8.8\n",
            "  [T] Textual features: 30 dimensions (TF-IDF)\n",
            "\n",
            "============================================================\n",
            "Building Y_T^3: Derived POI Attribute Matrix (Level 3)\n",
            "============================================================\n",
            "Granularity: Region\n",
            "Number of POIs at level 3: 5\n",
            "Number of users: 21\n",
            "\n",
            "  [Step 1] Building user preference features...\n",
            "    User preference matrix shape: (21, 33)\n",
            "\n",
            "  [Step 2] Building POI-User interaction matrix...\n",
            "    Interaction matrix shape: (5, 21)\n",
            "    Density: 67.62%\n",
            "    Total interactions: 567\n",
            "\n",
            "  [Step 3] Deriving POI features from user preferences...\n",
            "    Aggregated user preferences: 33 dims\n",
            "    User diversity features: 3 dims\n",
            "    Interaction pattern features: 4 dims\n",
            "\n",
            "  [Step 4] Computing latent embeddings via NMF...\n",
            "    Latent embeddings: 32 dims\n",
            "\n",
            "============================================================\n",
            "Final POI Embedding Summary - Level 3\n",
            "============================================================\n",
            "  Number of POIs: 5\n",
            "  Y_A^3 dimensions: 33\n",
            "  Y_T^3 dimensions: 72\n",
            "  Total embedding dimensions: 105\n",
            "\n",
            "Saving embeddings to: C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\Sources\\poi_embeddings.pkl\n",
            "  File size: 13.14 MB\n",
            "  Levels saved: ['level_0', 'level_1', 'level_2', 'level_3']\n",
            "  Save complete!\n",
            "\n",
            "Loading embeddings from: C:\\Users\\syoon\\SpatiaLynk_recommender\\Sources\\Sources\\poi_embeddings.pkl\n",
            "  Loaded 4 levels\n",
            "  Loaded 7 encoders\n",
            "  Created at: 2026-01-29T00:04:51.081061\n",
            "\n",
            "Embedding dimensions per level:\n",
            "  level_0: 221 total (149 explicit + 72 derived)\n",
            "  level_1: 171 total (99 explicit + 72 derived)\n",
            "  level_2: 125 total (53 explicit + 72 derived)\n",
            "  level_3: 105 total (33 explicit + 72 derived)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    ROOT = Path.cwd()\n",
        "    SOURCES = ROOT / \"Sources\"\n",
        "    \n",
        "    user_preferences_file = (SOURCES / \"user_preferences.csv\").resolve()\n",
        "    user_poi_interactions_file = (SOURCES / \"user_poi_interactions.csv\").resolve()\n",
        "    poi_tree_file = (SOURCES / \"poi_tree_with_uuids.json\").resolve()\n",
        "    \n",
        "    print(\"user_preferences_file:\", user_preferences_file, user_preferences_file.exists())\n",
        "    print(\"user_poi_interactions_file:\", user_poi_interactions_file, user_poi_interactions_file.exists())\n",
        "    print(\"poi_tree_file:\", poi_tree_file, poi_tree_file.exists())\n",
        "\n",
        "    # Initialize embedding generator\n",
        "    learner = POIEmbeddings()\n",
        "\n",
        "    # Build embeddings for all levels\n",
        "    learner.build_poi_embeddings(levels=[0, 1, 2, 3])\n",
        "\n",
        "    output_file = \"Sources/poi_embeddings.pkl\"\n",
        "\n",
        "    # Save embeddings\n",
        "    learner.save_embeddings(output_file)\n",
        "    \n",
        "    loaded_data = learner.load_embeddings(output_file)\n",
        "    \n",
        "    print(\"\\nEmbedding dimensions per level:\")\n",
        "    for level_key, dims in loaded_data['metadata']['embedding_dimensions'].items():\n",
        "        print(f\"  {level_key}: {dims['total']} total ({dims['explicit']} explicit + {dims['derived']} derived)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b669ef33-4537-460f-ab4e-94f325e8390c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
