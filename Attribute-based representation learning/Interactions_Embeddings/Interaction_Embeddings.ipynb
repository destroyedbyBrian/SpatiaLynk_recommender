{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e39714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcafbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionGenerator:\n",
    "    \"\"\"\n",
    "    Generate training interactions at all hierarchy levels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "\t\t\t\tinteractions_file: str,\n",
    "\t\t\t\tpoi_tree_file: str,\n",
    "\t\t\t\tmetadata_file: str):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Initializing Interaction Generator\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load interactions\n",
    "        print(f\"\\nLoading interactions from: {interactions_file}\")\n",
    "        self.interactions_df = pd.read_csv(interactions_file)\n",
    "        print(f\"  Loaded {len(self.interactions_df)} raw interactions\")\n",
    "        \n",
    "        # Load POI tree\n",
    "        print(f\"\\nLoading POI tree from: {poi_tree_file}\")\n",
    "        with open(poi_tree_file, 'r', encoding='utf-8') as f:\n",
    "            self.poi_tree = json.load(f)\n",
    "        \n",
    "        # Load metadata for ID mappings\n",
    "        print(f\"\\nLoading metadata from: {metadata_file}\")\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        \n",
    "        self.mappings = self.metadata['mappings']\n",
    "        \n",
    "        # Build parent lookup cache\n",
    "        print(\"\\nBuilding parent lookup cache...\")\n",
    "        self.parent_cache = self._build_parent_cache()\n",
    "        \n",
    "        print(\"\\nInitialization complete!\")\n",
    "    \n",
    "    def _build_parent_cache(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"\n",
    "        Build a cache mapping POI ID -> {level: parent_id_at_level}\n",
    "        \n",
    "        For each level-0 POI, precompute its parent at each higher level.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary: poi_id -> {1: parent_l1, 2: parent_l2, 3: parent_l3}\n",
    "        \"\"\"\n",
    "        parent_cache = {}\n",
    "        \n",
    "        # Get all level 0 POIs\n",
    "        level_0_pois = self.poi_tree.get('level_0', {})\n",
    "        \n",
    "        for poi_id, poi_data in level_0_pois.items():\n",
    "            parent_cache[poi_id] = {}\n",
    "            \n",
    "            current_id = poi_id\n",
    "            current_level = 0\n",
    "            \n",
    "            # Traverse up the tree\n",
    "            while current_level < 3:\n",
    "                level_key = f'level_{current_level}'\n",
    "                \n",
    "                if level_key not in self.poi_tree:\n",
    "                    break\n",
    "                \n",
    "                if current_id not in self.poi_tree[level_key]:\n",
    "                    break\n",
    "                \n",
    "                parent_id = self.poi_tree[level_key][current_id].get('parent')\n",
    "                \n",
    "                if parent_id:\n",
    "                    parent_cache[poi_id][current_level + 1] = parent_id\n",
    "                    current_id = parent_id\n",
    "                    current_level += 1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # Statistics\n",
    "        pois_with_full_hierarchy = sum(\n",
    "            1 for cache in parent_cache.values() \n",
    "            if len(cache) == 3\n",
    "        )\n",
    "        print(f\"  Cached parents for {len(parent_cache)} level-0 POIs\")\n",
    "        print(f\"  POIs with full hierarchy (up to level 3): {pois_with_full_hierarchy}\")\n",
    "        \n",
    "        return parent_cache\n",
    "    \n",
    "    def _get_parent_at_level(self, poi_id: str, target_level: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get parent of poi_id at target_level using cache\n",
    "        \n",
    "        Args:\n",
    "            poi_id: Level 0 POI ID\n",
    "            target_level: Target level (1, 2, or 3)\n",
    "        \n",
    "        Returns:\n",
    "            Parent POI ID at target level, or None if not found\n",
    "        \"\"\"\n",
    "        if target_level == 0:\n",
    "            return poi_id\n",
    "        \n",
    "        if poi_id in self.parent_cache:\n",
    "            return self.parent_cache[poi_id].get(target_level)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _propagate_interactions_to_level(self, \n",
    "\t\t\t\t\t\t\t\t\t\tlevel_0_interactions: List[Tuple],\n",
    "\t\t\t\t\t\t\t\t\t\ttarget_level: int) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Propagate level-0 interactions to a higher level\n",
    "        \n",
    "        Args:\n",
    "            level_0_interactions: List of (user_id, poi_id, timestamp, interaction_type, value)\n",
    "            target_level: Target level (1, 2, or 3)\n",
    "        \n",
    "        Returns:\n",
    "            List of propagated interactions at target level\n",
    "        \"\"\"\n",
    "        propagated = []\n",
    "        \n",
    "        for user_id, poi_id, timestamp, interaction_type, value in level_0_interactions:\n",
    "            parent_id = self._get_parent_at_level(poi_id, target_level)\n",
    "            \n",
    "            if parent_id:\n",
    "                propagated.append((user_id, parent_id, timestamp, interaction_type, value))\n",
    "        \n",
    "        return propagated\n",
    "    \n",
    "    def generate_interactions(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate interaction data for all levels\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing interactions at all levels\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Generating Interactions for All Levels\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        print(\"\\n[Step 1] Sorting interactions by timestamp...\")\n",
    "        self.interactions_df['timestamp'] = pd.to_datetime(self.interactions_df['timestamp'])\n",
    "        self.interactions_df = self.interactions_df.sort_values('timestamp')\n",
    "        \n",
    "        # Extract level 0 interactions\n",
    "        print(\"\\n[Step 2] Extracting level-0 interactions...\")\n",
    "        level_0_interactions = []\n",
    "        \n",
    "        for _, row in self.interactions_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            poi_id = row['poi_id']\n",
    "            timestamp = row['timestamp']\n",
    "            interaction_type = row.get('interaction_type', 'visit')\n",
    "            value = row.get('value', 1)\n",
    "            \n",
    "            level_0_interactions.append((\n",
    "                user_id, poi_id, timestamp, interaction_type, value\n",
    "            ))\n",
    "        \n",
    "        print(f\"  Extracted {len(level_0_interactions)} level-0 interactions\")\n",
    "        \n",
    "        # Generate interactions for each level\n",
    "        interactions_by_level = {}\n",
    "        \n",
    "        for level in range(4):\n",
    "            print(f\"\\n[Step 3.{level}] Processing Level {level}...\")\n",
    "            \n",
    "            if level == 0:\n",
    "                level_interactions = level_0_interactions\n",
    "            else:\n",
    "                level_interactions = self._propagate_interactions_to_level(\n",
    "                    level_0_interactions, level\n",
    "                )\n",
    "            \n",
    "            # Process and aggregate interactions\n",
    "            processed = self._process_level_interactions(level_interactions, level)\n",
    "            interactions_by_level[f'level_{level}'] = processed\n",
    "            \n",
    "            print(f\"  Level {level}: {processed['stats']['total_interactions']} interactions\")\n",
    "            print(f\"  Level {level}: {processed['stats']['unique_users']} unique users\")\n",
    "            print(f\"  Level {level}: {processed['stats']['unique_pois']} unique POIs\")\n",
    "        \n",
    "        return interactions_by_level\n",
    "    \n",
    "    def _process_level_interactions(self, \n",
    "                                    interactions: List[Tuple],\n",
    "                                    level: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Process interactions for a specific level\n",
    "        \n",
    "        Creates:\n",
    "        - Sparse interaction matrix\n",
    "        - Edge lists\n",
    "        - Aggregated scores\n",
    "        - Train/val/test splits\n",
    "        \n",
    "        Args:\n",
    "            interactions: List of (user_id, poi_id, timestamp, type, value)\n",
    "            level: Current level\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with processed interaction data\n",
    "        \"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        \n",
    "        # Get mappings\n",
    "        user_to_idx = self.mappings['user']['id_to_idx']\n",
    "        poi_to_idx = self.mappings['poi'][level_key]['id_to_idx']\n",
    "        \n",
    "        n_users = self.mappings['user']['count']\n",
    "        n_pois = self.mappings['poi'][level_key]['count']\n",
    "        \n",
    "        # Aggregate interactions by (user, poi) pair\n",
    "        pair_data = defaultdict(lambda: {\n",
    "            'visits': 0,\n",
    "            'ratings': [],\n",
    "            'searches': 0,\n",
    "            'total_value': 0,\n",
    "            'timestamps': [],\n",
    "            'interaction_types': []\n",
    "        })\n",
    "        \n",
    "        skipped_users = set()\n",
    "        skipped_pois = set()\n",
    "        \n",
    "        for user_id, poi_id, timestamp, interaction_type, value in interactions:\n",
    "            # Check if user and POI exist in mappings\n",
    "            if user_id not in user_to_idx:\n",
    "                skipped_users.add(user_id)\n",
    "                continue\n",
    "            if poi_id not in poi_to_idx:\n",
    "                skipped_pois.add(poi_id)\n",
    "                continue\n",
    "            \n",
    "            key = (user_id, poi_id)\n",
    "            data = pair_data[key]\n",
    "            \n",
    "            data['timestamps'].append(timestamp)\n",
    "            data['interaction_types'].append(interaction_type)\n",
    "            \n",
    "            if interaction_type == 'visit':\n",
    "                data['visits'] += 1\n",
    "                data['total_value'] += 1.0\n",
    "            elif interaction_type == 'rating':\n",
    "                data['ratings'].append(value)\n",
    "                data['total_value'] += value / 5.0\n",
    "            elif interaction_type == 'search':\n",
    "                data['searches'] += 1\n",
    "                data['total_value'] += 0.3\n",
    "            else:\n",
    "                data['total_value'] += 0.5\n",
    "        \n",
    "        if skipped_users:\n",
    "            print(f\"    Warning: Skipped {len(skipped_users)} unknown users\")\n",
    "        if skipped_pois:\n",
    "            print(f\"    Warning: Skipped {len(skipped_pois)} unknown POIs\")\n",
    "        \n",
    "        # Build various representations\n",
    "        \n",
    "        # 1. Edge list (for GNN)\n",
    "        edge_user_ids = []\n",
    "        edge_poi_ids = []\n",
    "        edge_user_indices = []\n",
    "        edge_poi_indices = []\n",
    "        edge_weights = []\n",
    "        edge_timestamps = []\n",
    "        \n",
    "        # 2. Sparse matrix (for matrix factorization)\n",
    "        interaction_matrix = lil_matrix((n_users, n_pois), dtype=np.float32)\n",
    "        \n",
    "        # 3. Binary interaction matrix\n",
    "        binary_matrix = lil_matrix((n_users, n_pois), dtype=np.float32)\n",
    "        \n",
    "        # 4. User -> POI mapping\n",
    "        user_to_pois = defaultdict(list)\n",
    "        \n",
    "        # 5. POI -> User mapping\n",
    "        poi_to_users = defaultdict(list)\n",
    "        \n",
    "        for (user_id, poi_id), data in pair_data.items():\n",
    "            user_idx = user_to_idx[user_id]\n",
    "            poi_idx = poi_to_idx[poi_id]\n",
    "            \n",
    "            # Compute aggregated score\n",
    "            score = data['total_value']\n",
    "            if data['ratings']:\n",
    "                score += np.mean(data['ratings']) / 5.0\n",
    "            \n",
    "            # Edge list\n",
    "            edge_user_ids.append(user_id)\n",
    "            edge_poi_ids.append(poi_id)\n",
    "            edge_user_indices.append(user_idx)\n",
    "            edge_poi_indices.append(poi_idx)\n",
    "            edge_weights.append(score)\n",
    "            edge_timestamps.append(max(data['timestamps']))  # Latest timestamp\n",
    "            \n",
    "            # Matrices\n",
    "            interaction_matrix[user_idx, poi_idx] = score\n",
    "            binary_matrix[user_idx, poi_idx] = 1.0\n",
    "            \n",
    "            # Mappings\n",
    "            user_to_pois[user_idx].append({\n",
    "                'poi_idx': poi_idx,\n",
    "                'poi_id': poi_id,\n",
    "                'score': score,\n",
    "                'visits': data['visits'],\n",
    "                'avg_rating': np.mean(data['ratings']) if data['ratings'] else None\n",
    "            })\n",
    "            \n",
    "            poi_to_users[poi_idx].append({\n",
    "                'user_idx': user_idx,\n",
    "                'user_id': user_id,\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "        # Convert to CSR for efficiency\n",
    "        interaction_matrix = interaction_matrix.tocsr()\n",
    "        binary_matrix = binary_matrix.tocsr()\n",
    "        \n",
    "        # Create train/val/test splits based on timestamp\n",
    "        edge_data = list(zip(\n",
    "            edge_user_indices, \n",
    "            edge_poi_indices, \n",
    "            edge_weights,\n",
    "            edge_timestamps\n",
    "        ))\n",
    "        edge_data.sort(key=lambda x: x[3])  # Sort by timestamp\n",
    "        \n",
    "        n_edges = len(edge_data)\n",
    "        train_end = int(n_edges * 0.7)\n",
    "        val_end = int(n_edges * 0.85)\n",
    "        \n",
    "        train_edges = edge_data[:train_end]\n",
    "        val_edges = edge_data[train_end:val_end]\n",
    "        test_edges = edge_data[val_end:]\n",
    "        \n",
    "        # Statistics\n",
    "        stats = {\n",
    "            'total_interactions': len(pair_data),\n",
    "            'unique_users': len(set(edge_user_indices)),\n",
    "            'unique_pois': len(set(edge_poi_indices)),\n",
    "            'total_visits': sum(d['visits'] for d in pair_data.values()),\n",
    "            'total_ratings': sum(len(d['ratings']) for d in pair_data.values()),\n",
    "            'total_searches': sum(d['searches'] for d in pair_data.values()),\n",
    "            'avg_interactions_per_user': len(pair_data) / max(len(set(edge_user_indices)), 1),\n",
    "            'avg_interactions_per_poi': len(pair_data) / max(len(set(edge_poi_indices)), 1),\n",
    "            'matrix_density': interaction_matrix.nnz / (n_users * n_pois) * 100,\n",
    "            'train_size': len(train_edges),\n",
    "            'val_size': len(val_edges),\n",
    "            'test_size': len(test_edges)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            # Edge lists\n",
    "            'edges': {\n",
    "                'user_ids': edge_user_ids,\n",
    "                'poi_ids': edge_poi_ids,\n",
    "                'user_indices': np.array(edge_user_indices, dtype=np.int64),\n",
    "                'poi_indices': np.array(edge_poi_indices, dtype=np.int64),\n",
    "                'weights': np.array(edge_weights, dtype=np.float32),\n",
    "                'timestamps': edge_timestamps\n",
    "            },\n",
    "            \n",
    "            # Sparse matrices\n",
    "            'matrices': {\n",
    "                'interaction': interaction_matrix,\n",
    "                'binary': binary_matrix\n",
    "            },\n",
    "            \n",
    "            # Mappings\n",
    "            'user_to_pois': dict(user_to_pois),\n",
    "            'poi_to_users': dict(poi_to_users),\n",
    "            \n",
    "            # Train/Val/Test splits\n",
    "            'splits': {\n",
    "                'train': {\n",
    "                    'user_indices': np.array([e[0] for e in train_edges], dtype=np.int64),\n",
    "                    'poi_indices': np.array([e[1] for e in train_edges], dtype=np.int64),\n",
    "                    'weights': np.array([e[2] for e in train_edges], dtype=np.float32)\n",
    "                },\n",
    "                'val': {\n",
    "                    'user_indices': np.array([e[0] for e in val_edges], dtype=np.int64),\n",
    "                    'poi_indices': np.array([e[1] for e in val_edges], dtype=np.int64),\n",
    "                    'weights': np.array([e[2] for e in val_edges], dtype=np.float32)\n",
    "                },\n",
    "                'test': {\n",
    "                    'user_indices': np.array([e[0] for e in test_edges], dtype=np.int64),\n",
    "                    'poi_indices': np.array([e[1] for e in test_edges], dtype=np.int64),\n",
    "                    'weights': np.array([e[2] for e in test_edges], dtype=np.float32)\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # Statistics\n",
    "            'stats': stats,\n",
    "            \n",
    "            # Dimensions\n",
    "            'n_users': n_users,\n",
    "            'n_pois': n_pois\n",
    "        }\n",
    "    \n",
    "    def generate_negative_samples(self, \n",
    "\t\t\t\t\t\t\t\tinteractions_by_level: Dict,\n",
    "\t\t\t\t\t\t\t\tneg_ratio: int = 4) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate negative samples for training\n",
    "        \n",
    "        For each positive (user, poi) pair, sample neg_ratio negative POIs\n",
    "        that the user has NOT interacted with.\n",
    "        \n",
    "        Args:\n",
    "            interactions_by_level: Output from generate_interactions()\n",
    "            neg_ratio: Number of negative samples per positive\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with negative samples for each level\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Generating Negative Samples (ratio={neg_ratio})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        negative_samples = {}\n",
    "        \n",
    "        for level in range(4):\n",
    "            level_key = f'level_{level}'\n",
    "            level_data = interactions_by_level[level_key]\n",
    "            \n",
    "            print(f\"\\n  Processing Level {level}...\")\n",
    "            \n",
    "            n_users = level_data['n_users']\n",
    "            n_pois = level_data['n_pois']\n",
    "            \n",
    "            # Get positive interactions per user\n",
    "            user_positive_pois = defaultdict(set)\n",
    "            for user_idx, poi_idx in zip(\n",
    "                level_data['edges']['user_indices'],\n",
    "                level_data['edges']['poi_indices']\n",
    "            ):\n",
    "                user_positive_pois[user_idx].add(poi_idx)\n",
    "            \n",
    "            # Generate negative samples for each split\n",
    "            level_negatives = {}\n",
    "            \n",
    "            for split_name in ['train', 'val', 'test']:\n",
    "                split_data = level_data['splits'][split_name]\n",
    "                user_indices = split_data['user_indices']\n",
    "                poi_indices = split_data['poi_indices']\n",
    "                \n",
    "                neg_user_indices = []\n",
    "                neg_poi_indices = []\n",
    "                \n",
    "                all_pois = set(range(n_pois))\n",
    "                \n",
    "                for user_idx, pos_poi_idx in zip(user_indices, poi_indices):\n",
    "                    # Get POIs user hasn't interacted with\n",
    "                    positive_pois = user_positive_pois[user_idx]\n",
    "                    negative_pois = list(all_pois - positive_pois)\n",
    "                    \n",
    "                    if len(negative_pois) >= neg_ratio:\n",
    "                        sampled_negs = np.random.choice(\n",
    "                            negative_pois, \n",
    "                            size=neg_ratio, \n",
    "                            replace=False\n",
    "                        )\n",
    "                    else:\n",
    "                        # If not enough negatives, sample with replacement\n",
    "                        sampled_negs = np.random.choice(\n",
    "                            negative_pois if negative_pois else list(all_pois),\n",
    "                            size=neg_ratio,\n",
    "                            replace=True\n",
    "                        )\n",
    "                    \n",
    "                    for neg_poi_idx in sampled_negs:\n",
    "                        neg_user_indices.append(user_idx)\n",
    "                        neg_poi_indices.append(neg_poi_idx)\n",
    "                \n",
    "                level_negatives[split_name] = {\n",
    "                    'user_indices': np.array(neg_user_indices, dtype=np.int64),\n",
    "                    'poi_indices': np.array(neg_poi_indices, dtype=np.int64)\n",
    "                }\n",
    "                \n",
    "                print(f\"    {split_name}: {len(neg_user_indices)} negative samples\")\n",
    "            \n",
    "            negative_samples[level_key] = level_negatives\n",
    "        \n",
    "        return negative_samples\n",
    "    \n",
    "    def save_interactions(self, \n",
    "\t\t\t\t\t\tinteractions_by_level: Dict,\n",
    "\t\t\t\t\t\tnegative_samples: Dict,\n",
    "\t\t\t\t\t\toutput_file: str = 'interactions.pkl'):\n",
    "        \"\"\"\n",
    "        Save all interaction data to pickle file\n",
    "        \n",
    "        Args:\n",
    "            interactions_by_level: Interaction data for all levels\n",
    "            negative_samples: Negative samples for all levels\n",
    "            output_file: Output file path\n",
    "        \"\"\"\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Saving interactions to: {output_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        save_data = {\n",
    "            # Main interaction data\n",
    "            'interactions': interactions_by_level,\n",
    "            \n",
    "            # Negative samples\n",
    "            'negative_samples': negative_samples,\n",
    "            \n",
    "            # Quick access to dimensions\n",
    "            'dimensions': {\n",
    "                f'level_{level}': {\n",
    "                    'n_users': interactions_by_level[f'level_{level}']['n_users'],\n",
    "                    'n_pois': interactions_by_level[f'level_{level}']['n_pois'],\n",
    "                    'n_interactions': interactions_by_level[f'level_{level}']['stats']['total_interactions']\n",
    "                }\n",
    "                for level in range(4)\n",
    "            },\n",
    "            \n",
    "            # Metadata\n",
    "            'info': {\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'negative_ratio': 4,  # Default ratio used\n",
    "                'split_ratios': {'train': 0.7, 'val': 0.15, 'test': 0.15},\n",
    "                'levels': ['level_0', 'level_1', 'level_2', 'level_3'],\n",
    "                'level_names': {\n",
    "                    'level_0': 'Building',\n",
    "                    'level_1': 'Street',\n",
    "                    'level_2': 'District',\n",
    "                    'level_3': 'Region'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        # Print summary\n",
    "        import os\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        print(f\"\\n  File size: {file_size:.2f} MB\")\n",
    "        \n",
    "        print(\"\\n  Interactions per level:\")\n",
    "        for level in range(4):\n",
    "            level_key = f'level_{level}'\n",
    "            stats = interactions_by_level[level_key]['stats']\n",
    "            print(f\"    Level {level}:\")\n",
    "            print(f\"      - Total interactions: {stats['total_interactions']}\")\n",
    "            print(f\"      - Train/Val/Test: {stats['train_size']}/{stats['val_size']}/{stats['test_size']}\")\n",
    "            print(f\"      - Matrix density: {stats['matrix_density']:.4f}%\")\n",
    "        \n",
    "        print(\"\\n  Save complete!\")\n",
    "\n",
    "\n",
    "def generate_interactions_pkl(interactions_file: str,\n",
    "\t\t\t\t\t\t\tpoi_tree_file: str,\n",
    "\t\t\t\t\t\t\tmetadata_file: str,\n",
    "\t\t\t\t\t\t\toutput_file: str = 'interactions.pkl',\n",
    "\t\t\t\t\t\t\tneg_ratio: int = 4):\n",
    "    \"\"\"\n",
    "    Main function to generate interactions.pkl\n",
    "    \n",
    "    Args:\n",
    "        interactions_file: Path to user_poi_interactions.csv\n",
    "        poi_tree_file: Path to poi_tree_with_uuids.json\n",
    "        metadata_file: Path to metadata.pkl\n",
    "        output_file: Output file path\n",
    "        neg_ratio: Negative sampling ratio\n",
    "    \"\"\"\n",
    "    # Initialize generator\n",
    "    generator = InteractionGenerator(\n",
    "        interactions_file=interactions_file,\n",
    "        poi_tree_file=poi_tree_file,\n",
    "        metadata_file=metadata_file\n",
    "    )\n",
    "    \n",
    "    # Generate interactions\n",
    "    interactions_by_level = generator.generate_interactions()\n",
    "    \n",
    "    # Generate negative samples\n",
    "    negative_samples = generator.generate_negative_samples(\n",
    "        interactions_by_level, \n",
    "        neg_ratio=neg_ratio\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    generator.save_interactions(\n",
    "        interactions_by_level,\n",
    "        negative_samples,\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    return interactions_by_level, negative_samples\n",
    "\n",
    "\n",
    "# Utility class for loading and using interactions\n",
    "class InteractionLoader:\n",
    "    \"\"\"\n",
    "    Utility class for loading and accessing interaction data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interactions_file: str = 'interactions.pkl'):\n",
    "        \"\"\"\n",
    "        Load interactions from pickle file\n",
    "        \n",
    "        Args:\n",
    "            interactions_file: Path to interactions.pkl\n",
    "        \"\"\"\n",
    "        with open(interactions_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        \n",
    "        self.interactions = self.data['interactions']\n",
    "        self.negative_samples = self.data['negative_samples']\n",
    "        self.dimensions = self.data['dimensions']\n",
    "        self.info = self.data['info']\n",
    "    \n",
    "    def get_train_data(self, level: int) -> Dict:\n",
    "        \"\"\"Get training data for a specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return {\n",
    "            'positive': self.interactions[level_key]['splits']['train'],\n",
    "            'negative': self.negative_samples[level_key]['train']\n",
    "        }\n",
    "    \n",
    "    def get_val_data(self, level: int) -> Dict:\n",
    "        \"\"\"Get validation data for a specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return {\n",
    "            'positive': self.interactions[level_key]['splits']['val'],\n",
    "            'negative': self.negative_samples[level_key]['val']\n",
    "        }\n",
    "    \n",
    "    def get_test_data(self, level: int) -> Dict:\n",
    "        \"\"\"Get test data for a specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return {\n",
    "            'positive': self.interactions[level_key]['splits']['test'],\n",
    "            'negative': self.negative_samples[level_key]['test']\n",
    "        }\n",
    "    \n",
    "    def get_interaction_matrix(self, level: int, binary: bool = False) -> csr_matrix:\n",
    "        \"\"\"Get interaction matrix for a specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        matrix_key = 'binary' if binary else 'interaction'\n",
    "        return self.interactions[level_key]['matrices'][matrix_key]\n",
    "    \n",
    "    def get_user_history(self, user_idx: int, level: int) -> List[Dict]:\n",
    "        \"\"\"Get interaction history for a user at specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return self.interactions[level_key]['user_to_pois'].get(user_idx, [])\n",
    "    \n",
    "    def get_poi_users(self, poi_idx: int, level: int) -> List[Dict]:\n",
    "        \"\"\"Get users who interacted with a POI at specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return self.interactions[level_key]['poi_to_users'].get(poi_idx, [])\n",
    "    \n",
    "    def get_stats(self, level: int) -> Dict:\n",
    "        \"\"\"Get statistics for a specific level\"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        return self.interactions[level_key]['stats']\n",
    "    \n",
    "    def get_all_edges(self, level: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get all edges (user_indices, poi_indices, weights) for a level\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (user_indices, poi_indices, weights)\n",
    "        \"\"\"\n",
    "        level_key = f'level_{level}'\n",
    "        edges = self.interactions[level_key]['edges']\n",
    "        return (\n",
    "            edges['user_indices'],\n",
    "            edges['poi_indices'],\n",
    "            edges['weights']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf449439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTERACTION GENERATION PIPELINE\n",
      "============================================================\n",
      "\n",
      "Input files:\n",
      "  - Interactions: ../../Sources/Files/user_poi_interactions.csv\n",
      "  - POI Tree: ../../Sources/Files/poi_tree_with_uuids.json\n",
      "  - Metadata: ../../Sources/Embeddings/metadata.pkl\n",
      "\n",
      "Output file: interactions.pkl\n",
      "============================================================\n",
      "Initializing Interaction Generator\n",
      "============================================================\n",
      "\n",
      "Loading interactions from: ../../Sources/Files/user_poi_interactions.csv\n",
      "  Loaded 567 raw interactions\n",
      "\n",
      "Loading POI tree from: ../../Sources/Files/poi_tree_with_uuids.json\n",
      "\n",
      "Loading metadata from: ../../Sources/Embeddings/metadata.pkl\n",
      "\n",
      "Building parent lookup cache...\n",
      "  Cached parents for 4696 level-0 POIs\n",
      "  POIs with full hierarchy (up to level 3): 4696\n",
      "\n",
      "Initialization complete!\n",
      "\n",
      "============================================================\n",
      "Generating Interactions for All Levels\n",
      "============================================================\n",
      "\n",
      "[Step 1] Sorting interactions by timestamp...\n",
      "\n",
      "[Step 2] Extracting level-0 interactions...\n",
      "  Extracted 567 level-0 interactions\n",
      "\n",
      "[Step 3.0] Processing Level 0...\n",
      "  Level 0: 257 interactions\n",
      "  Level 0: 21 unique users\n",
      "  Level 0: 235 unique POIs\n",
      "\n",
      "[Step 3.1] Processing Level 1...\n",
      "  Level 1: 231 interactions\n",
      "  Level 1: 21 unique users\n",
      "  Level 1: 142 unique POIs\n",
      "\n",
      "[Step 3.2] Processing Level 2...\n",
      "  Level 2: 180 interactions\n",
      "  Level 2: 21 unique users\n",
      "  Level 2: 31 unique POIs\n",
      "\n",
      "[Step 3.3] Processing Level 3...\n",
      "  Level 3: 71 interactions\n",
      "  Level 3: 21 unique users\n",
      "  Level 3: 5 unique POIs\n",
      "\n",
      "============================================================\n",
      "Generating Negative Samples (ratio=4)\n",
      "============================================================\n",
      "\n",
      "  Processing Level 0...\n",
      "    train: 716 negative samples\n",
      "    val: 156 negative samples\n",
      "    test: 156 negative samples\n",
      "\n",
      "  Processing Level 1...\n",
      "    train: 644 negative samples\n",
      "    val: 140 negative samples\n",
      "    test: 140 negative samples\n",
      "\n",
      "  Processing Level 2...\n",
      "    train: 500 negative samples\n",
      "    val: 112 negative samples\n",
      "    test: 108 negative samples\n",
      "\n",
      "  Processing Level 3...\n",
      "    train: 196 negative samples\n",
      "    val: 44 negative samples\n",
      "    test: 44 negative samples\n",
      "\n",
      "============================================================\n",
      "Saving interactions to: interactions.pkl\n",
      "============================================================\n",
      "\n",
      "  File size: 0.20 MB\n",
      "\n",
      "  Interactions per level:\n",
      "    Level 0:\n",
      "      - Total interactions: 257\n",
      "      - Train/Val/Test: 179/39/39\n",
      "      - Matrix density: 0.2606%\n",
      "    Level 1:\n",
      "      - Total interactions: 231\n",
      "      - Train/Val/Test: 161/35/35\n",
      "      - Matrix density: 0.8118%\n",
      "    Level 2:\n",
      "      - Total interactions: 180\n",
      "      - Train/Val/Test: 125/28/27\n",
      "      - Matrix density: 19.4805%\n",
      "    Level 3:\n",
      "      - Total interactions: 71\n",
      "      - Train/Val/Test: 49/11/11\n",
      "      - Matrix density: 67.6190%\n",
      "\n",
      "  Save complete!\n",
      "\n",
      "============================================================\n",
      "DEMO: Using InteractionLoader\n",
      "============================================================\n",
      "\n",
      "Level 0 Statistics:\n",
      "  total_interactions: 257\n",
      "  unique_users: 21\n",
      "  unique_pois: 235\n",
      "  total_visits: 264\n",
      "  total_ratings: 209\n",
      "  total_searches: 94\n",
      "  avg_interactions_per_user: 12.238095238095237\n",
      "  avg_interactions_per_poi: 1.0936170212765957\n",
      "  matrix_density: 0.26060679808550336\n",
      "  train_size: 179\n",
      "  val_size: 39\n",
      "  test_size: 39\n",
      "\n",
      "Training data shape (Level 0):\n",
      "  Positive samples: 179\n",
      "  Negative samples: 716\n",
      "\n",
      "Interaction matrix info (Level 0):\n",
      "  Shape: (21, 4696)\n",
      "  Non-zero entries: 257\n",
      "  Density: 0.2606%\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_poi_interactions_file = \"../../Sources/Files/user_poi_interactions.csv\"\n",
    "    poi_tree_file = \"../../Sources/Files/poi_tree_with_uuids.json\"\n",
    "    metadata_file = \"../../Sources/Embeddings/metadata.pkl\"\n",
    "    output_file = \"interactions.pkl\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"INTERACTION GENERATION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nInput files:\")\n",
    "    print(f\"  - Interactions: {user_poi_interactions_file}\")\n",
    "    print(f\"  - POI Tree: {poi_tree_file}\")\n",
    "    print(f\"  - Metadata: {metadata_file}\")\n",
    "    print(f\"\\nOutput file: {output_file}\")\n",
    "    \n",
    "    # Generate interactions\n",
    "    interactions_by_level, negative_samples = generate_interactions_pkl(\n",
    "        interactions_file=user_poi_interactions_file,\n",
    "        poi_tree_file=poi_tree_file,\n",
    "        metadata_file=metadata_file,\n",
    "        output_file=output_file,\n",
    "        neg_ratio=4\n",
    "    )\n",
    "    \n",
    "    # Demo: Using the InteractionLoader\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMO: Using InteractionLoader\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    loader = InteractionLoader(output_file)\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\nLevel 0 Statistics:\")\n",
    "    stats = loader.get_stats(level=0)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nTraining data shape (Level 0):\")\n",
    "    train_data = loader.get_train_data(level=0)\n",
    "    print(f\"  Positive samples: {len(train_data['positive']['user_indices'])}\")\n",
    "    print(f\"  Negative samples: {len(train_data['negative']['user_indices'])}\")\n",
    "    \n",
    "    print(\"\\nInteraction matrix info (Level 0):\")\n",
    "    matrix = loader.get_interaction_matrix(level=0)\n",
    "    print(f\"  Shape: {matrix.shape}\")\n",
    "    print(f\"  Non-zero entries: {matrix.nnz}\")\n",
    "    print(f\"  Density: {matrix.nnz / (matrix.shape[0] * matrix.shape[1]) * 100:.4f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
